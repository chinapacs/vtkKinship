% This file was created with JabRef 2.6.
% Encoding: UTF8

@INPROCEEDINGS{OLA03,
  author = {S\'{\i}lvia Delgado Olabarriaga and Marcel Breeuwer and Wiro J. Niessen},
  title = {Minimum Cost Path Algorithm for Coronary Artery Central Axis Tracking
	in CT Images},
  booktitle = {MICCAI (2)},
  year = {2003},
  pages = {687-694},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/miccai/2003-2},
  ee = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=2879{\&}spage=687}
}

@INPROCEEDINGS{ABE03,
  author = {Abe, Toru and Matsuzawa, Yuki},
  title = {Active Contour Model Using A Priori Knowledge of Region Shape.},
  booktitle = {ICIP},
  year = {2003},
  address = {Barcelona},
  publisher = {IEEE},
  abstract = {To improve the region extraction accuracy in active contour model
	(ACM), we propose a novel method for employing the a priori knowledge
	of a target region shape. The proposed method is based on an idea
	that a user-drawn initial contour for ACM can include the shape features
	of a target region. In this method, the sequence of shape features
	(e.g., corner, line, and curve) is obtained from an initial contour,
	and this sequence is used as the a priori knowledge of a target region.
	During region extraction procedure, by finding the optimal assignment
	of the shape features to contour control points and determining the
	optimal positions for these points, ACM energy is minimized and the
	knowledge is reflected on an extracted region shape. Using an initial
	contour not only as an initial value for the energy minimization
	but also as the a priori knowledge of a target region shape, the
	proposed method can employ the a priori knowledge effectively without
	time-consuming preparation work.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{ALL03.1,
  author = {Alliez, Pierre and Cohen-Steiner, David and Devillers, Olivier and
	Levy, Bruno and Desbrun, Mathieu},
  title = {Anisotropic polygonal remeshing},
  journal = {ACM Transactions on Graphics},
  year = {2003},
  volume = { 22},
  pages = {485 - 493},
  number = { 3},
  note = {Surface remeshing;Anisotropic sampling;Polygon meshes;Lines of curvatures;Tensor
	fields;},
  abstract = {In this paper, we propose a novel polygonal remeshing technique that
	exploits a key aspect of surfaces: the intrinsic anisotropy of natural
	or man-made geometry. In particular, we use curvature directions
	to drive the remeshing process, mimicking the lines that artists
	themselves would use when creating 3D models from scratch. After
	extracting and smoothing the curvature tensor field of an input genus-0
	surface patch, lines of minimum and maximum curvatures are used to
	determine appropriate edges for the remeshed version in anisotropic
	regions, while spherical regions are simply point-sampled since there
	is no natural direction of symmetry locally. As a result our technique
	generates polygon meshes mainly composed of quads in anisotropic
	regions, and of triangles in spherical regions. Our approach provides
	the flexibility to produce meshes ranging from isotropic to anisotropic,
	from coarse to dense, and from uniform to curvature adapted. &copy;
	2003 ACM.},
  address = {San Diego, CA, United States},
  copyright = {Compilation and indexing terms, Copyright 2006 Elsevier Inc. All rights
	reserved},
  issn = {0730-0301},
  key = {Computer graphics},
  keywords = {Anisotropy;Approximation theory;Tensors;Computational geometry;Three
	dimensional;Models;},
  language = {English},
  owner = {euHeart},
  timestamp = {2009.02.24},
  url = {http://dx.doi.org/10.1145/882262.882296}
}

@INPROCEEDINGS{ALL02,
  author = {Alliez, Pierre and Meyer, Mark and Desbrun, Mathieu},
  title = {Interactive geometry remeshing},
  booktitle = {ACM Transactions on Graphics; Proceedings of ACM SIGGRAPH 2002, Jul
	23-26 2002},
  year = {2002},
  volume = {21},
  series = {ACM Transactions on Graphics},
  pages = {347-354},
  address = {United States},
  publisher = {Association for Computing Machinery},
  note = {TY - CONF U1 - 03097372547 L2 - http://dx.doi.org/10.1145/566654.566588
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Interactive geometry remeshing Halftoning},
  abstract = {We present a novel technique, both flexible and efficient, for interactive
	remeshing of irregular geometry. First, the original (arbitrary genus)
	mesh is substituted by a series of 2D maps in parameter space. Using
	these maps, our algorithm is then able to take advantage of established
	signal processing and halftoning tools that offer real-time interaction
	and intricate control. The user can easily combine these maps to
	create a control map - a map which controls the sampling density
	over the surface patch. This map is then sampled at interactive rates
	allowing the user to easily design a tailored resampling. Once this
	sampling is complete, a Delaunay triangulation and fast optimization
	are performed to perfect the final mesh. As a result, our remeshing
	technique is extremely versatile and general, being able to produce
	arbitrarily complex meshes with a variety of properties including:
	uniformity, regularity, semi-regularity, curvature sensitive resampling,
	and feature preservation. We provide a high level of control over
	the sampling distribution allowing the user to interactively custom
	design the mesh based on their requirements thereby increasing their
	productivity in creating a wide variety of meshes.},
  keywords = {Computational geometry Three dimensional computer graphics Two dimensional
	Algorithms Image processing Optimization Computer vision Image quality
	Interactive computer graphics},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{ALL03.2,
  author = {Alliez, P. and de Verdire, E.C. and Devillers, O. and Isenburg, M.},
  title = {Isotropic surface remeshing},
  booktitle = {Proc. Shape Modeling International},
  year = {2003},
  pages = {49--58},
  doi = {10.1109/SMI.2003.1199601},
  keywords = {computational geometry, mesh generation, solid modelling, surface
	fitting, conformal parameter space, constrained Delaunay triangulation,
	error diffusion generalization, feature edge, flexible design, image
	halftoning, isotropic surface remeshing, low-pass filtering, mesh
	resampling, mesh triangle, optimal cutting, parameterization, polygonal
	schema, smoother gradation, surface sampling, triangulated surface
	mesh, user-specified density function, weighted centroidal Voronoi
	tessellation},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{AMI98,
  author = {Amini, Amir A. and Chen, Yasheng and Curwen, Rupert W. and Mani,
	Vaidy and Sun, Jean},
  title = {Coupled B-snake grids and constrained thin-plate splines for analysis
	of 2-D tissue deformations from tagged MRI},
  journal = {IEEE Transactions on Medical Imaging},
  year = {1998},
  volume = {17},
  pages = {344-356},
  number = {3},
  note = {TY - JOUR U1 - 98094391204 L2 - http://dx.doi.org/10.1109/42.712124
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Image warps},
  abstract = {Magnetic resonance imaging (MRI) is unique in its ability to noninvasively
	and selectively alter tissue magnetization and create tagged patterns
	within a deforming body such as the heart muscle. The resulting patterns
	define a time-varying curvilinear coordinate system on the tissue,
	which we track with coupled B-snake grids. B-spline bases provide
	local control of shape, compact representation, and parametric continuity.
	Efficient spline warps are proposed which warp an area in the plane
	such that two embedded snake grids obtained from two tagged frames
	are brought into registration, interpolating a dense displacement
	vector field. The reconstructed vector field adheres to the known
	displacement information at the intersections, forces corresponding
	snakes to be warped into one another, and for all other points in
	the plane, where no information is available, a C<sup>1</sup> continuous
	vector field is interpolated. The implementation proposed in this
	paper improves on our previous variational-based implementation and
	generalizes warp methods to include biologically relevant contiguous
	open curves, in addition to standard landmark points. The methods
	are validated with a cardiac motion simulator, in addition to in-vivo
	tagging data sets.},
  keywords = {Magnetic resonance imaging Image reconstruction Interpolation Vectors
	Muscle Medical imaging},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{AMI88,
  author = {Amini, Amir A. and Tehranni, Saeid and Weymouth, Terry E.},
  title = {Using Dynamic Programming for Minimizing the Energy of Active Contours
	in the Presence of Hard Constraints},
  booktitle = {Second International Conference on Computer Vision},
  year = {1988},
  pages = {95-99},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{AMI90,
  author = {Amini, Amir A. and Weymouth, Terry E. and Jain, Ramesh C.},
  title = {Using Dynamic Programming for Solving Variational Problems in Vision},
  journal = {Transactions on Pattern Analysis and Machine Intelligence.},
  year = {1990},
  volume = {12},
  pages = {855-867},
  number = {9},
  abstract = {Early image understanding seeks to derive analytic representations
	from image intensities. The authors present steps towards this goal
	by considering the inference of surfaces from three-dimensional images.
	Only smooth surfaces are considered and the focus is on the coupled
	problems of inferring the trace points (the points through which
	the surface passes) and estimating the associated differential structure
	given by the principal curvature and direction fields over the estimated
	smooth surfaces. Computation of these fields is based on determining
	an atlas of local charts or parameterizations at estimated surface
	points. Algorithm robustness and the stability of results are essential
	for analyzing real images; to this end, the authors present a functional
	minimization algorithm utilizing overlapping local charts to refine
	surface points and curvature estimates, and develop an implementation
	as an iterative constraint satisfaction procedure based on local
	surface smoothness properties. Examples of the recovery of local
	structure are presented for synthetic images degraded by noise and
	for clinical magnetic resonance images.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@PHDTHESIS{ANW01,
  author = {Anwander, Alfred},
  title = {Segmentation d'images couleur par un op\'erateur gradient vectoriel
	multi\'echelle et contour actif : Application \`a la quantification
	des phases min\'eralogiques du clinker de ciment.},
  school = {INSA Lyon},
  year = {2001},
  type = {Thèse de doctorat},
  keywords = {images couleurs, contours actifs, multiechelle.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{ASP02,
  author = {N. Aspert and D. Santa-Cruz and T. Ebrahimi},
  title = {MESH: Measuring Errors between Surfaces using the Hausdorff Distance},
  booktitle = {IEEE International Conference on Multimedia and Expo},
  year = {2002},
  volume = {I},
  pages = {705 -- 708},
  note = {\texttt{http://mesh.epfl.ch}},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{AYA03,
  author = {Ayache, N.},
  title = {Epidaure: A research project in medical image analysis, simulation,
	and robotics at INRIA},
  journal = {Medical Imaging, IEEE Transactions on},
  year = {2003},
  volume = {22},
  pages = {1185-1201},
  number = {10},
  note = {TY - JOUR},
  abstract = {Epidaure is the name of a research project launched in 1989 at INRIA
	Rocquencourt, close to Paris, France. The research directions of
	the project were progressively defined around the following topics:
	volumetric image segmentation, three-dimensional (3-D) shape modeling,
	image registration, motion analysis, morphometry, and surgery simulation.
	The author describes and illustrates some of the contributions of
	the Epidaure team on these different topics.},
  keywords = {brain cardiology edge detection image registration image segmentation
	medical image processing medical robotics reviews surgery Epidaure
	INRIA cardiac images image registration medical image analysis morphometry
	motion analysis research project robotics simulation surgery simulation
	three-dimensional shape modeling volumetric image segmentation},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{BAJXX.1,
  author = {Bajaj, Chandrajit L. and Bernardini, Fausto and Xu, Guoliang},
  title = {Reconstructing Surfaces and Surfaces on Surfaces from Scattered 3D
	Data.},
  abstract = {We present an efficient and uniform approach for the automatic reconstruction
	of surfaces of CAD (computer aided design) models and scalar fields
	defined on them, from an unorganized collection of scanned point
	data. Example applications in the manufacturing domain are the rapid
	computer model reconstruction of any existing physical part from
	some three dimensional (3D) points scan of the part’s surface. Color,
	texture or some scalar material property of the physical part, defines
	natural scalar fields over the surface of the CAD model. Manufacturers
	who wish to use existing computer aided design and manufacturing
	software (CAD/CAM) need to have computer models of these parts. Using
	these algorithms, existing parts and prototypes can be automatically
	reconstructed into computer models from 3D scans.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{BAJXX.2,
  author = {Bajaj, C. L. and Bernardini, F. and Xu, G.},
  title = {Reconstructing Surfaces and Functions on Surfaces from Unorganized
	Three-Dimensional Data.},
  abstract = {Creating a computer model from an existing part is a common problem
	in Reverse Engineering. The part might be scanned with a device like
	the laser range scanner, or points might be measured on its surface
	with a mechanical probe. Sometimes, not only the spatial location
	of points, but also some associated physical property can be measured.
	The problem of automatically reconstructing from this data a topologically
	consistent and geometrically accurate model of the object and of
	the sampled scalar field is the subject of this paper.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{BAJ95,
  author = {Bajaj, Chandrajit L. and Bernardini, Fausto and Xu, Guoliang},
  title = {Automatic Reconstruction of Surfaces and Scalar Fields from 3D Scans},
  booktitle = {International Conference on Computer Graphics and Interactive Techniques},
  year = {1995},
  pages = {109-118},
  publisher = {ACM Press},
  abstract = {We present an efficient and uniform approach for the automatic reconstruction
	of surfaces of CAD (computer aided design)models and scalar fields
	defined on them, from an unorganized collection of scanned point
	data. A possible application is the rapid computer model reconstruction
	of an existing part or prototype from a three dimensional (3D) points
	scan of its surface. Color, texture or some scalar material property
	of the physical part, define natural scalar fields over the surface
	of the CAD model. },
  keywords = {Geometricmodeling, shape recovery, range data analysis, algebraic
	surfaces, triangulations, alpha-shapes.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{BAJ96,
  author = {Bajaj, Chandrajit L. and Coyle, Edward J. and Lin, Kwun-Nan},
  title = {Arbitrary topology shape reconstruction from planar cross sections},
  journal = {Graphical Models and Image Processing},
  year = {1996},
  volume = {58},
  pages = {524-543},
  number = {6},
  note = {TY - JOUR U1 - 97013485391 L2 - http://dx.doi.org/10.1006/gmip.1996.0044
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Arbitrary topology shape reconstruction Planar
	cross sections},
  abstract = {In computed tomography, magnetic resonance imaging and ultrasound
	imaging, reconstruction of the 3D object from the 2D scalar-valued
	slices obtained by the imaging system is difficult because of the
	large spacings between the 2D slices. The aliasing that results from
	this undersampling in the direction orthogonal to the slices leads
	to two problems, known as the correspondence problem and the tiling
	problem. A third problem, known as the branching problem, arises
	because of the structure of the objects being imaged in these applications.
	Existing reconstruction algorithms typically address only one or
	two of these problems. In this paper, we approach all three of these
	problems simultaneously. This is accomplished by imposing a set of
	three constraints on the reconstructed surface and then deriving
	precise correspondence and tiling rules from these constraints. The
	constraints ensure that the regions tiled by these rules obey physical
	constructs and have a natural appearance. Regions which cannot be
	tiled by these rules without breaking one or more constraints are
	tiled with their medial axis (edge Voronoi diagram). Our implementation
	of the above approach generates triangles of 3D isosurfaces from
	input which is either a set of contour data or a volume of image
	slices. Results obtained with synthetic and actual medical data are
	presented. There are still specific cases in which our new approach
	can generate distorted results, but these cases are much less likely
	to occur than those which cause distortions in other tiling approaches.},
  keywords = {Magnetic resonance imaging Topology Medical imaging Algorithms Computerized
	tomography Problem solving Signal distortion Ultrasonic imaging Surfaces
	Image reconstruction},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{BAJ03,
  author = {Chandrajit L. Bajaj and Guoliang Xu},
  title = {Anisotropic diffusion of surfaces and functions on surfaces},
  journal = {ACM Trans. Graph.},
  year = {2003},
  volume = {22},
  pages = {4--32},
  number = {1},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/588272.588276},
  issn = {0730-0301},
  owner = {euHeart},
  publisher = {ACM Press},
  timestamp = {2009.02.24}
}

@ARTICLE{BAN02,
  author = {Bandyopadhyay, Sanghamitra},
  title = {Contour extraction using genetic algorithms},
  journal = {IETE Journal of Research},
  year = {2002},
  volume = {48},
  pages = {369-376},
  number = {5 SPEC},
  note = {TY - JOUR U1 - 03067355019 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc. U2 - Contour extraction
	Active contour},
  abstract = {A fast and efficient contour extraction algorithm that exploits the
	enhanced searching capability of genetic algorithm while minimising
	the energy of an active contour is proposed in this article. The
	energy is computed based on a chamfer image, in which pixel values
	relate to their closeness to surrounding edges. Given an initial
	approximation of the contour of interest, its energy (both internal
	and external) is computed. Subsequently, genetic algorithm is used
	to minimize the energy by appropriately moving the contour towards
	the one of interest. Comparison of the performance of the proposed
	algorithm with snake, a traditional contour extraction technique
	based on the active contour model, demonstrates the superiority of
	the former for situations where the spline drawn from initial control
	points do not closely approximate the contour of interest.},
  keywords = {Feature extraction Edge detection Optimization Approximation theory
	Genetic algorithms},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{BAR05,
  author = {D. C. Barber and D. R. Hose},
  title = {Automatic segmentation of medical images using image registration:
	diagnostic and simulation applications.},
  journal = {J Med Eng Technol},
  year = {2005},
  volume = {29},
  pages = {53--63},
  number = {2},
  abstract = {Automatic identification of the boundaries of significant structure
	(segmentation) within a medical image is an are of ongoing research.
	Various approaches have been proposed but only two methods have achieved
	widespread use: manual delineation of boundaries and segmentation
	using intensity values. In this paper we describe an approach based
	on image registration. A reference image is prepared and segmented,
	by hand or otherwise. A patient image is registered to the reference
	image and the mapping then applied to ther reference segmentation
	to map it back to the patient image. In general a high-resolution
	nonlinear mapping is required to achieve accurate segmentation. This
	paper describes an algorithm that can efficiently generate such mappings,
	and outlines the uses of this tool in two relevant applications.
	An important feature of the approach described in this paper is that
	the algorithm is independent of the segmentation problem being addresses.
	All knowledge about the problem at hand is contained in files of
	reference data. A secondary benefit is that the continuous three-dimensional
	mapping generated is well suited to the generation of patient-specific
	numerical models (e.g. finite element meshes) from the library models.
	Smoothness constraints in the morphing algorithm tend to maintain
	the geometric quality of the reference mesh.},
  doi = {10.1080/03091900412331289889},
  institution = {Medical Physics, Royal Hallamshire Hospital, University of Sheffield,
	Glossop Road, Sheffield, S10 2JF, UK. d.c.barber@sheffield.ac.uk},
  keywords = {Algorithms; Artificial Intelligence; Computer Simulation; Humans;
	Image Enhancement, methods; Image Interpretation, Computer-Assisted,
	methods; Imaging, Three-Dimensional, methods; Information Storage
	and Retrieval, methods; Models, Biological; Pattern Recognition,
	Automated, methods; Reproducibility of Results; Sensitivity and Specificity;
	Subtraction Technique},
  owner = {euHeart},
  pii = {X3616043L745Q754},
  pmid = {15804853},
  timestamp = {2008.09.22},
  url = {http://dx.doi.org/10.1080/03091900412331289889}
}

@ARTICLE{BAR07,
  author = {D. C. Barber and E. Oubel and A. F. Frangi and D. R. Hose},
  title = {Efficient computational fluid dynamics mesh generation by image registration.},
  journal = {Med Image Anal},
  year = {2007},
  volume = {11},
  pages = {648--662},
  number = {6},
  month = {Dec},
  abstract = {Most implementations of computational fluid dynamics (CFD) solutions
	require a discretisation or meshing of the solution domain. The production
	from a medical image of a computationally efficient mesh representing
	the structures of interest can be time consuming and labour-intensive,
	and remains a major bottleneck in the clinical application of CFD.
	This paper presents a method for deriving a patient-specific mesh
	from a medical image. The method uses volumetric registration of
	a pseudo-image, produced from an idealised template mesh, with the
	medical image. The registration algorithm used is robust and computationally
	efficient. The accuracy of the new algorithm is measured in terms
	of the distance between a registered surface and a known surface,
	for image data derived from casts of the lumen of two different vessels.
	The true surface is identified by laser profiling. The average distance
	between the surface points measured by the laser profiler and the
	surface of the mapped mesh is better than 0.2 mm. For the images
	analysed, the new algorithm is shown to be 2-3 times more accurate
	than a standard published algorithm based on maximising normalised
	mutual information. Computation times are approximately 18 times
	faster for the new algorithm than the standard algorithm. Examples
	of the use of the algorithm on two clinical examples are also given.
	The registration methodology lends itself immediately to the construction
	of dynamic mesh models in which vessel wall motion is obtained directly
	using registration.},
  doi = {10.1016/j.media.2007.06.011},
  institution = {Department of Medical Physics, University of Sheffield, Royal Hallamshire
	Hospital, Sheffield, UK. D.Barber@sheffield.ac.uk},
  keywords = {Algorithms; Animals; Aorta, physiology; Blood Flow Velocity, physiology;
	Carotid Arteries, physiology; Cattle; Computational Biology, methods;
	Hemorheology, methods; Humans; Image Enhancement, methods; Imaging,
	Three-Dimensional, methods; Magnetic Resonance Imaging},
  owner = {euHeart},
  pii = {S1361-8415(07)00065-5},
  pmid = {17702641},
  timestamp = {2008.09.22},
  url = {http://dx.doi.org/10.1016/j.media.2007.06.011}
}

@ARTICLE{BAR98,
  author = {Bardinet, Eric and Cohen, Laurent D. and Ayache, Nicholas},
  title = {A Parametric Deformable Model to Fit Unstructured 3D Data},
  journal = {Computer Vision and Image Understanding.},
  year = {1998},
  volume = {71},
  pages = {39-54},
  number = {1},
  abstract = {In many computer vision and image understanding problems, it is important
	to find a smooth surface that fits a set of given unstructured 3D
	data. Although approaches based on general deformable models give
	satisfactory results, in particular a local description of the surface,
	they involve large linear systems to solve when dealing with high
	resolution 3D images. The advantage of parametric deformable templates
	like superquadrics is their small number of parameters to describe
	a shape. However, the set of shapes described by superquadrics is
	too limited to approximate precisely complex surfaces. This is why
	hybrid models have been introduced to refine the initial approximation.
	This article introduces a deformable superquadric model based on
	a superquadric fit followed by a freeform deformation (FFD) to fit
	unstructured 3D points. At the expense of a reasonable number of
	additional parameters, free-form deformations provide a much closer
	fit and a volumetric deformation field.We first present the mathematical
	and algorithmic details of the method. Then, since we are mainly
	concerned with applications for medical images, we present a medical
	application consisting in the reconstruction of the left ventricle
	of the heart from a number of various 3D cardiac images. The extension
	of the method to track anatomical structures in spatio-temporal images
	(4D data) is presented in a companion article [9].},
  keywords = {Parametric models, Deformable models, Superquadrics, 3D Splines, Free-form
	deformations, Regularization, Surface Reconstruction.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{BAR04,
  author = {Julia F Barrett and Nicholas Keat},
  title = {Artifacts in CT: recognition and avoidance.},
  journal = {Radiographics},
  year = {2004},
  volume = {24},
  pages = {1679--1691},
  number = {6},
  abstract = {Artifacts can seriously degrade the quality of computed tomographic
	(CT) images, sometimes to the point of making them diagnostically
	unusable. To optimize image quality, it is necessary to understand
	why artifacts occur and how they can be prevented or suppressed.
	CT artifacts originate from a range of sources. Physics-based artifacts
	result from the physical processes involved in the acquisition of
	CT data. Patient-based artifacts are caused by such factors as patient
	movement or the presence of metallic materials in or on the patient.
	Scanner-based artifacts result from imperfections in scanner function.
	Helical and multisection technique artifacts are produced by the
	image reconstruction process. Design features incorporated into modern
	CT scanners minimize some types of artifacts, and some can be partially
	corrected by the scanner software. However, in many instances, careful
	patient positioning and optimum selection of scanning parameters
	are the most important factors in avoiding CT artifacts.},
  doi = {10.1148/rg.246045065},
  institution = {Imaging Performance Assessment of CT Scanners, St George's Hospital,
	Blackshaw Rd, London SW17 0QT, England. julia@beamed.wanadoo.co.uk},
  keywords = {Artifacts; Humans; Tomography, X-Ray Computed, methods},
  owner = {euHeart},
  pii = {24/6/1679},
  pmid = {15537976},
  timestamp = {2009.03.10},
  url = {http://dx.doi.org/10.1148/rg.246045065}
}

@ARTICLE{BEE09,
  author = {Philipp Beerbaum and Samir Sarikouch and Kai-Thorsten Laser and Gerald
	Greil and Wolfgang Burchert and Hermann Körperich},
  title = {Coronary anomalies assessed by whole-heart isotropic 3D magnetic
	resonance imaging for cardiac morphology in congenital heart disease.},
  journal = {J Magn Reson Imaging},
  year = {2009},
  volume = {29},
  pages = {320--327},
  number = {2},
  month = {Feb},
  abstract = {PURPOSE: To determine the value of whole-heart three-dimensional magnetic
	resonance imaging (MRI) for coronary artery imaging in children/adolescents
	with congenital heart disease (CHD). MATERIALS AND METHODS: Forty
	children/adolescents (median age: 14 years, range 2.6-25.8) with
	CHD underwent free-breathing navigator-gated isotropic three-dimensional
	steady-state free-precession (3D-SSFP) MRI for cardiac morphology.
	Two observers independently evaluated visibility of origin, course,
	vessel lengths, image quality (IQ), and contrast between coronary
	lumen and myocardium. A subgroup was compared with cardiac catheter.
	RESULTS: The total scan time was 6.3 +/- 3.2 minutes (mean +/- SD,
	at mean heart rate 76 +/- 15/min). The mean vessel length for right
	coronary artery (RCA) by observer 1 was 97 +/- 43 mm (observer 2:
	94 +/- 37 mm), for left main and anterior descending artery (LM/LAD)
	91 +/- 40 mm (observer 2: 90 +/- 40 mm), and for left circumflex
	artery (LCX) 64 +/- 28mm (observer 2: 66 +/- 28 mm). The mean vessel
	contrast was 0.34 +/- 0.05 (range: 0.23-0.45; maximum = 1, minimum
	= 0). On a 4-level score (1 = nondiagnostic, 4 = excellent), mean
	IQ scores ranged between 2.3-2.9 (+/-0.8-1.0). Both observers agreed
	on the presence/proximal course of RCA in 40/40, LM/LAD in 38/40,
	and LCX in 36/40 patients. There was complete agreement with invasive
	coronary angiography available in 12/40 patients (six anomalies).
	CONCLUSION: Isotropic whole-heart 3D-MRI for cardiac morphology allows
	reliable discrimination between normal and abnormal coronary anatomy
	in children/adolescents with CHD.},
  doi = {10.1002/jmri.21655},
  institution = { St Thomas' Hospital, London, UK.},
  keywords = {Adolescent; Analysis of Variance; Child; Child, Preschool; Coronary
	Angiography; Female; Heart Defects, Congenital, diagnosis; Humans;
	Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional;
	Magnetic Resonance Imaging, methods; Male; Prospective Studies; Young
	Adult},
  language = {eng},
  medline-pst = {ppublish},
  owner = {euHeart},
  pmid = {19161183},
  timestamp = {2010.01.08},
  url = {http://dx.doi.org/10.1002/jmri.21655}
}

@INPROCEEDINGS{BEL03,
  author = {Belaroussi, Boubakeur and Odet, Christophe and Benoit-Cattin, Hugues},
  title = {Application of scalable discrepancy measures for computer vision
	image segmentation tasks},
  booktitle = {International Conference on Quality Control by Artificial Vision,
	QCAV'03},
  year = {2003},
  pages = {56-62},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{BEN09,
  author = {Benmansour, Fethallah and Cohen, Laurent D.},
  title = {Fast Object Segmentation by Growing Minimal Paths from a Single Point
	on 2D or 3D Images},
  journal = {J. Math. Imaging Vis.},
  year = {2009},
  volume = {33},
  pages = {209--221},
  number = {2},
  abstract = {In this paper, we present a new method for segmenting closed contours
	and surfaces. Our work builds on a variant of the minimal path approach.
	First, an initial point on the desired contour is chosen by the user.
	Next, new keypoints are detected automatically using a front propagation
	approach. We assume that the desired object has a closed boundary.
	This a-priori knowledge on the topology is used to devise a relevant
	criterion for stopping the keypoint detection and front propagation.
	The final domain visited by the front will yield a band surrounding
	the object of interest. Linking pairs of neighboring keypoints with
	minimal paths allows us to extract a closed contour from a 2D image.
	This approach can also be used for finding an open curve giving extra
	information as stopping criteria. Detection of a variety of objects
	on real images is demonstrated. Using a similar idea, we can extract
	networks of minimal paths from a 3D image called Geodesic Meshing.
	The proposed method is applied to 3D data with promising results.},
  address = {Norwell, MA, USA},
  doi = {http://dx.doi.org/10.1007/s10851-008-0131-0},
  issn = {0924-9907},
  publisher = {Kluwer Academic Publishers}
}

@ARTICLE{BEN07,
  author = {H. E. Bennink and H. C. van Assen and G. J. Streekstra and R. ter
	Wee and J. A E Spaan and B. M. ter Haar Romeny},
  title = {A novel 3D multi-scale lineness filter for vessel detection.},
  journal = {Med Image Comput Comput Assist Interv Int Conf Med Image Comput Comput
	Assist Interv},
  year = {2007},
  volume = {10},
  pages = {436--443},
  number = {Pt 2},
  abstract = {The branching pattern and geometry of coronary microvessels are of
	high interest to understand and model the blood flow distribution
	and the processes of contrast invasion, ischemic changes and repair
	in the heart in detail. Analysis is performed on high resolution,
	3D volumes of the arterial microvasculature of entire goat hearts,
	which are acquired with an imaging cryomicrotome. Multi-scale vessel
	detection is an important step required for a detailed quantitative
	analysis of the coronary microvasculature. Based on visual inspection,
	the derived lineness filter shows promising results on real data
	and digital phantoms, on the way towards accurate computerized reconstructions
	of entire coronary trees. The novel lineness filter exploits the
	local first and second order multi-scale derivatives in order to
	give an intensity-independent response to line centers and to suppress
	unwanted responses to steep edges.},
  institution = {Biomedical Image Analysis, Faculty of Biomedical Engineering, Eindhoven
	University of Technology, The Netherlands.},
  keywords = {Algorithms; Artificial Intelligence; Coronary Vessels, anatomy /&/
	histology; Humans; Image Enhancement, methods; Image Interpretation,
	Computer-Assisted, methods; Imaging, Three-Dimensional, methods;
	Microcirculation, anatomy /&/ histology; Microscopy, methods; Models,
	Biological; Models, Statistical; Pattern Recognition, Automated,
	methods; Reproducibility of Results; Sensitivity and Specificity},
  language = {eng},
  medline-pst = {ppublish},
  owner = {euHeart},
  pmid = {18044598},
  timestamp = {2009.10.14}
}

@INPROCEEDINGS{BEN98,
  author = {Benoit-Cattin, H. and Planat, A.C. and Joachimsmann, P.R. and Baskurt,
	A. and Clarysse, P. and Magnin, I.E.},
  title = {On the coding of active quadtree mesh},
  booktitle = {Proceedings of the 1998 International Conference on Image Processing,
	ICIP. Part 2 (of 3), Oct 4-7 1998},
  year = {1998},
  volume = {2},
  series = {IEEE International Conference on Image Processing},
  pages = {895-898},
  address = {Chicago, IL, USA},
  publisher = {IEEE Comp Soc, Los Alamitos, CA, USA},
  note = {TY - CONF U1 - 98124508292 L2 - http://dx.doi.org/10.1109/ICIP.1998.723700
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Quadtree mesh Global image code},
  abstract = {In this paper, we consider the coding of a quadrangular mesh structure
	generated by a multiresolution deformable quadtree algorithm. Such
	a structure has been found efficient for region based video coding
	[1], but few works [2] deal with the coding of the mesh structure.
	Here, we propose to use the rules of the mesh generation to significantly
	reduce the number of bits required for the coding of the nodes position
	defining the mesh structure. This reduction induces an improvement
	of either the image quality or either the global bit rate.},
  keywords = {Algorithms Image quality Image segmentation Image reconstruction Image
	coding},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{BER00,
  author = {S. L. Bernard and J. R. Ewen and C. H. Barlow and J. J. Kelly and
	S. McKinney and D. A. Frazer and R. W. Glenny},
  title = {High spatial resolution measurements of organ blood flow in small
	laboratory animals.},
  journal = {Am J Physiol Heart Circ Physiol},
  year = {2000},
  volume = {279},
  pages = {H2043--H2052},
  number = {5},
  month = {Nov},
  abstract = {With the use of a newly developed Imaging Cryomicrotome to determine
	the spatial location of fluorescent microspheres in organs, we validate
	and report our processing algorithms for measuring regional blood
	flow in small laboratory animals. Microspheres (15-microm diameter)
	of four different fluorescent colors and one radioactive label were
	simultaneously injected into the left ventricle of a pig. The heart
	and kidneys were dissected, and the numbers of fluorescent and radioactive
	microspheres were determined in 10 randomly selected pieces. All
	microsphere counts fell well within the 95\% expected confidence
	limits as determined from the radioactive counts. Fluorescent microspheres
	(15-microm diameter) of four different colors were also injected
	into the tail vein of a rat and the left ventricle of a rabbit. After
	correction for Poisson noise, correlation coefficients between the
	colors were 0.99 +/- 0.02 (means +/- SD) for the rabbit heart and
	0.99 +/- 0.02 for the rat lung. Mathematical dissection algorithms,
	statistics to analyze the spatial data, and methods to visualize
	blood flow distributions in small animal organs are presented.},
  institution = {Division of Pulmonary and Critical Care Medicine, University of Washington,
	Seattle 98195, USA. sbernard@u.washington.edu},
  keywords = {Animals; Blood Flow Velocity; Fluorescent Dyes, pharmacokinetics;
	Kidney, blood supply/cytology; Lung, blood supply/cytology; Microspheres;
	Models, Cardiovascular; Myocardium, metabolism; Poisson Distribution;
	Rabbits; Rats; Regional Blood Flow, physiology; Swine; Tissue Distribution},
  owner = {euHeart},
  pmid = {11045936},
  timestamp = {2008.09.12}
}

@ARTICLE{BER99,
  author = {Bernardini, Fausto and Mittleman, Joshua and Rushmeier, Holly and
	Silva, Claudio and Taubin, Gabriel},
  title = {Ball-Pivoting algorithm for surface reconstruction},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  year = {1999},
  volume = { 5},
  pages = {349 - 359},
  number = { 4},
  note = {Shape reconstruction;Point cloud;Range image;Ball pivoting algorithm;},
  abstract = {The Ball-Pivoting Algorithm (BPA) computes a triangle mesh interpolating
	a given point cloud. Typically, the points are surface samples acquired
	with multiple range scans of an object. The principle of the BPA
	is very simple: Three points form a triangle if a ball of a user-specified
	radius &rho; touches them without containing any other point. Starting
	with a seed triangle, the ball pivots around an edge (i.e., it revolves
	around the edge while keeping in contact with the edge's endpoints)
	until it touches another point, forming another triangle. The process
	continues until all reachable edges have been tried, and then starts
	from another seed triangle, until all points have been considered.
	The process can then be repeated with a ball of larger radius to
	handle uneven sampling densities. We applied the BPA to datasets
	of millions of points representing actual scans of complex 3D objects.
	The relatively small amount of memory required by the BPA, its time
	efficiency, and the quality of the results obtained compare favorably
	with existing techniques.},
  copyright = {Compilation and indexing terms, Copyright 2006 Elsevier Inc. All rights
	reserved},
  issn = {1077-2626},
  key = {Three dimensional computer graphics},
  keywords = {Image reconstruction;Algorithms;Interpolation;Scanning;Computational
	geometry;Mathematical models;Computational complexity;Personal computers;},
  language = {English},
  owner = {euHeart},
  timestamp = {2009.02.24},
  url = {http://dx.doi.org/10.1109/2945.817351}
}

@ARTICLE{BER04,
  author = {Bertram, Martin and Duchaineau, Mark A. and Hamann, Bernd and Joy,
	Kenneth I.},
  title = {Generalized B-Spline Subdivision-Surface Wavelets for Geometry Compression},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  year = {2004},
  volume = {10},
  pages = {326-338},
  number = {3},
  note = {TY - JOUR U1 - 04208160422 L2 - http://dx.doi.org/10.1109/TVCG.2004.1272731
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Arbitrary-topology meshes Biorthogonal wavelets
	Geometry compression Multiresolution methods Subdivision surfaces},
  abstract = {We present a new construction of lifted biorthogonal wavelets on surfaces
	of arbitrary two-manifold topology for compression and multiresolution
	representation. Our method combines three approaches: subdivision
	surfaces of arbitrary topology, B-spline wavelets, and the lifting
	scheme for biorthogonal wavelet construction. The simple building
	blocks of our wavelet transform are local lifting operations performed
	on polygonal meshes with subdivision hierarchy. Starting with a coarse,
	irregular polyhedral base mesh, our transform creates a subdivision
	hierarchy of meshes converging to a smooth limit surface. At every
	subdivision level, geometric detail can be expanded from wavelet
	coefficients and added to the surface. We present wavelet constructions
	for bilinear, bicubic, and biquintic B-Spline subdivision. While
	the bilinear and bicubic constructions perform well in numerical
	experiments, the biquintic construction turns out to be unstable.
	For lossless compression, our transform can be computed in integer
	arithmetic, mapping integer coordinates of control points to integer
	wavelet coefficients. Our approach provides a highly efficient and
	progressive representation for complex geometries of arbitrary topology.},
  keywords = {Computer aided design Surface properties Topology Mathematical transformations
	Data reduction Hierarchical systems Geometry Vectors Computational
	methods Mathematical models Image compression},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{BHA09,
  author = {Himanshu Bhat and Sven Zuehlsdorff and Xiaoming Bi and Debiao Li},
  title = {Whole-heart contrast-enhanced coronary magnetic resonance angiography
	using gradient echo interleaved EPI.},
  journal = {Magn Reson Med},
  year = {2009},
  volume = {61},
  pages = {1388--1395},
  number = {6},
  month = {Jun},
  abstract = {Whole-heart coronary MR angiography (MRA) is a promising method for
	detecting coronary artery disease. However, the imaging time is relatively
	long (on the order of 10-15 min). Such a long imaging time may result
	in patient discomfort and compromise the robustness of whole-heart
	coronary MRA due to increased respiratory and cardiac motion artifacts.
	The goal of this study was to optimize a gradient echo interleaved
	echo planar imaging (GRE-EPI) acquisition scheme for reducing the
	imaging time of contrast-enhanced whole-heart coronary MRA. Numerical
	simulations and phantom studies were used to optimize the GRE-EPI
	sequence parameters. Healthy volunteers were scanned with both the
	proposed GRE-EPI sequence and a 3D TrueFISP sequence for comparison
	purposes. Slow infusion (0.5 cc/sec) of Gd-DTPA was used to enhance
	the signal-to-noise ratio (SNR) of the GRE-EPI acquisition. Whole-heart
	images with the GRE-EPI technique were acquired with a true resolution
	of 1.0 x 1.1 x 2.0 mm(3) in an average scan time of 4.7 +/- 0.7 min
	with an average navigator efficiency of 44 +/- 6\%. The GRE-EPI acquisition
	showed excellent delineation of all the major coronary arteries with
	scan time reduced by a factor of 2 compared with the TrueFISP acquisition.},
  doi = {10.1002/mrm.21963},
  institution = {Departments of Radiology and Biomedical Engineering, Northwestern
	University, Chicago, Illinois, USA.},
  keywords = {Adult; Algorithms; Contrast Media; Echo-Planar Imaging, methods; Gadolinium
	DTPA, diagnostic use; Heart, anatomy /&/ histology; Humans; Image
	Enhancement, methods; Image Interpretation, Computer-Assisted, methods;
	Magnetic Resonance Angiography, methods; Male; Reproducibility of
	Results; Sensitivity and Specificity},
  owner = {euHeart},
  pmid = {19319898},
  timestamp = {2009.08.21},
  url = {http://dx.doi.org/10.1002/mrm.21963}
}

@ARTICLE{BI05,
  author = {Xiaoming Bi and Debiao Li},
  title = {Coronary arteries at 3.0 T: Contrast-enhanced magnetization-prepared
	three-dimensional breathhold MR angiography.},
  journal = {J Magn Reson Imaging},
  year = {2005},
  volume = {21},
  pages = {133--139},
  number = {2},
  month = {Feb},
  abstract = {PURPOSE: To evaluate the efficacy of contrast-enhanced coronary magnetic
	resonance angiography (MRA) at 3.0 T. MATERIALS AND METHODS: Nine
	healthy human volunteers were studied on a 3.0-T whole-body MR system.
	A three-dimensional, breathhold, magnetization-prepared, segmented,
	gradient-echo sequence was used, with injection of 20 mL gadopentetate
	dimeglumine for each three-dimensional slab. Imaging parameters were
	optimized based on computer simulations. Signal-to-noise ratio (SNR),
	contrast-to-noise ratio (CNR), depicted coronary artery length, lumen
	diameter, and imaging sharpness with contrast agent were evaluated.
	SNR and CNR were compared to the results from a previous 1.5-T study.
	RESULTS: A 53\% increment in SNR and a 305\% enhancement in CNR were
	measured with contrast. Vessel length and sharpness depicted were
	higher and the lumen diameter was lower (all P values < 0.05) in
	postcontrast images. Compared to previous results from 1.5-T, the
	SNR, CNR, and vessel sharpness were enhanced at 3.0 T with higher
	spatial resolution. CONCLUSION: Contrast-enhanced, three-dimensional,
	coronary MRA at 3.0 T is a promising technique for diagnosing coronary
	artery diseases. Patient studies are necessary to evaluate its clinical
	utility.},
  doi = {10.1002/jmri.20250},
  institution = {Department of Radiology, Northwestern University, Chicago, Illinois,
	USA.},
  keywords = {Adult; Blood; Contrast Media; Coronary Vessels, anatomy /&/ histology;
	Female; Gadolinium DTPA, diagnostic use; Humans; Image Enhancement,
	methods; Image Processing, Computer-Assisted, methods; Imaging, Three-Dimensional,
	methods; Magnetic Resonance Angiography, methods; Male; Middle Aged;
	Respiration; Signal Processing, Computer-Assisted},
  owner = {euHeart},
  pmid = {15666400},
  timestamp = {2009.02.23},
  url = {http://dx.doi.org/10.1002/jmri.20250}
}

@PHDTHESIS{BIT98,
  author = {Bittar, Eric},
  title = {Mod\`eles D\'eformables Surfaciques, Implicites et Volumiques, pour
	l'Imagerie M\'edicale},
  school = {Grenoble I},
  year = {1998},
  type = {Informatique},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{BIT06,
  author = {Bitter, I. and Van Uitert, R. and Wolf, I. and Tzatha, E. and Gharib,A.
	M. and Summers,R. and Meinzer, H.-P. and Pettigrew, R.},
  title = {Virtual Contrast for Coronary Vessels Based on Level Set Generated
	Subvoxel Accurate Centerlines},
  journal = {International Journal of Biomedical Imaging},
  year = {2006},
  volume = {2006},
  pages = {Article ID 94025},
  doi = {doi:10.1155/IJBI/2006/94025},
  owner = {euHeart},
  timestamp = {2009.10.28}
}

@ARTICLE{BOL07,
  author = {Bolbos, R. and Benoit-Cattin, H. and Langlois, J.B. and Chomel, A.
	and Chereul, E. and Odet, C. and Pastoureau, P. and Janier, M. and
	Beuf, O.},
  title = {Knee cartilage thickness measurements using MRI: a 4 1/2-month longitudinal
	study in the meniscectomized guinea pig model of OA},
  journal = {Osteoarthritis and Cartilage},
  year = {2007},
  pages = {(in-press)},
  keywords = {st2i, meth_syst_IRM_optique},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{BOL03,
  author = {Boldak, C. and Toumoulin, C. and Coatrieux, J. -L.},
  title = {3D segmentation and characterization of lower limb vessels in multi-slice
	computed tomography},
  booktitle = {Proc. 25th Annual International Conference of the IEEE Engineering
	in Medicine and Biology Society},
  year = {2003},
  volume = {1},
  pages = {580--583 Vol.1},
  abstract = {This paper describes a fast 3D extraction method for the lower limb
	vessels in multi-slice computed tomography. It represents an extension
	of our previous work and makes uses of a two-step process: the first
	one performs a vessel central axis tracking. It makes use of a semi-automatic
	3D geometrical moment-based method to localize the center of the
	vessel and its local direction. It allows at the same time a first
	roughly delineation of the calcification areas and a diameter estimation
	at any point of the vessel. A refinement is then performed based
	on a level set approach to improve the delineation accuracy of the
	contour and calcifications. The vascular lumen is then calculated
	in the calcified area to evaluate the severity of the vessel narrowing.},
  doi = {10.1109/IEMBS.2003.1279811},
  issn = {1094-687X},
  keywords = {blood vessels, computerised tomography, image segmentation, medical
	image processing, patient diagnosis, 3D extraction method, 3D segmentation,
	calcification areas, delineation, diameter estimation, lower limb
	vessels, multislice computed tomography, semiautomatic 3D geometrical
	moment-based method, vascular lumen, vessel central axis tracking,
	vessel narrowing},
  owner = {euHeart},
  timestamp = {2008.09.10}
}

@ARTICLE{BOS04,
  author = {Tobias Boskamp and Daniel Rinck and Florian Link and Bernd Kümmerlen
	and Georg Stamm and Peter Mildenberger},
  title = {New vessel analysis tool for morphometric quantification and visualization
	of vessels in CT and MR imaging data sets.},
  journal = {Radiographics},
  year = {2004},
  volume = {24},
  pages = {287--297},
  number = {1},
  abstract = {Image processing algorithms and a prototypical research software tool
	have been developed for visualization and quantitative analysis of
	vessels in data sets from computed tomography and magnetic resonance
	imaging. The software is based on a sequence of processing steps,
	which are as follows: (a) vessel segmentation based on a region growing
	algorithm, (b) interactive "premasking" to optionally exclude interfering
	structures close to the vessels of interest, (c) distance transform-based
	skeletonization, (d) multiplanar reformation orthogonal to the vessel
	path, (e) identification of the lumen boundary on the orthogonal
	cross-section images, and (f) morphometric measurements. The development
	of the algorithmic components and the application user interface
	has been carried out in close cooperation with clinical users to
	achieve a high degree of usability and flexible support of work flow.
	The software has been successfully applied to the intracranial arteries,
	carotid arteries, and abdominal and thoracic aorta, as well as the
	renal, coronary, and peripheral arteries.},
  doi = {10.1148/rg.241035073},
  institution = {MeVis Center for Medical Diagnostic Systems and Visualization, Universitätsallee
	29, 28359 Bremen, Germany. tboskamp@mevis.de},
  keywords = {Algorithms; Angiography, methods; Blood Vessels, pathology; Humans;
	Image Processing, Computer-Assisted, instrumentation/methods; Magnetic
	Resonance Imaging, instrumentation; Software Design; Tomography,
	X-Ray Computed, instrumentation; User-Computer Interface},
  owner = {euHeart},
  pii = {24/1/287},
  pmid = {14730052},
  timestamp = {2009.03.27},
  url = {http://dx.doi.org/10.1148/rg.241035073}
}

@INPROCEEDINGS{BOT06,
  author = {Mario Botsch and Mark Pauly and Christian Rossl and Stephan Bischoff
	and Leif Kobbelt},
  title = {Geometric modeling based on triangle meshes},
  booktitle = {SIGGRAPH '06: ACM SIGGRAPH 2006 Courses},
  year = {2006},
  pages = {1},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  doi = {http://doi.acm.org/10.1145/1185657.1185839},
  isbn = {1-59593-364-6},
  location = {Boston, Massachusetts},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{BOU08,
  author = {Bouraoui, B. and Ronse, C. and Baruthio, J. and Passat, N. and Germain,
	P. },
  title = {Fully automatic 3D segmentation of coronary arteries based on mathematical
	morphology},
  booktitle = {Proc. 5th IEEE International Symposium on Biomedical Imaging: From
	Nano to Macro ISBI 2008},
  year = {2008},
  pages = {1059--1062},
  abstract = {In this paper we propose a fully automatic algorithm for coronary
	artery extraction from X-ray data (3D-CT scan, 64 detectors) based
	on the mathematical morphology techniques and guided by anatomical
	knowledge. Growing and thresholding methods, in their most general
	form, are not sufficient to extract only the whole coronary arteries,
	because of the properties of these images. Finding appropriate methods
	is known to be a challenging problem because of the data imperfections
	such as noise, heterogeneous intensity and contrasts of similar tissues.
	We deal with these challenges by employing discrete geometric tools
	to fit on the arteries form independently from any perturbation of
	the data.},
  doi = {10.1109/ISBI.2008.4541182},
  keywords = {blood vessels, computerised tomography, feature extraction, medical
	image processing, 3D-CT scan, automatic 3D segmentation, coronary
	arteries, coronary artery extraction, fully automatic algorithm,
	mathematical morphology, anatomical knowledge, coronary arteries,
	hit-or-miss transform, region-growing, segmentation},
  owner = {euHeart},
  timestamp = {2008.09.12}
}

@ARTICLE{BOU06,
  author = {A. Bousse and C. Boldak and C. Toumoulin and G. Yang and S. Laguitton
	and D. Boulmier},
  title = {Coronary extraction and characterization in multi-detector computed
	tomography},
  journal = {ITBM-RBM},
  year = {2006},
  volume = {27},
  pages = {217 - 226},
  number = {5-6},
  doi = {DOI: 10.1016/j.rbmret.2007.01.001},
  issn = {1297-9562},
  keywords = {MDCTA},
  owner = {euHeart},
  timestamp = {2010.05.17},
  url = {http://www.sciencedirect.com/science/article/B7GHY-4N3P9N9-2/2/05a031ac22e3c964bc1be3fe769336ab}
}

@INPROCEEDINGS{BRI04,
  author = {Brieva, J. and Galvez, M. and Toumoulin, C.},
  title = {Coronary extraction and stenosis quantification in X-Ray angiographic
	imaging},
  booktitle = {EMBC},
  year = {2004},
  volume = {26 III},
  pages = {1714-1717},
  address = {San Francisco, CA, United States},
  publisher = {IEEE},
  abstract = {This paper describes a method to quantify stenosis in X-Ray coronary
	angiography. Vascular edge extraction is first performed based on
	a deformable spline algorithm. It makes use of directional S-Gabor
	filters to build an external energy field that is then used in a
	snake optimisation scheme. A string matching technique is then applied
	to match the contour points and obtain a Trace between the matched
	points. This Trace allows then computing the vessel diameter and
	deriving quantitive stenosis measurements. Experimental results are
	presented on simulated data and real images.},
  keywords = {Imaging techniques Edge detection Feature extraction Image reconstruction
	Optimization Convergence of numerical methods Iterative methods Interpolation
	Dynamic programming Mathematical models Algorithms Computer simulation
	Angiocardiography},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{BRI00,
  author = {Brigger, Patrick and Hoeg, Jeff and Unser, Michael},
  title = {B-spline snakes: a flexible tool for parametric contour detection},
  journal = {IEEE Transactions on Image Processing},
  year = {2000},
  volume = {9},
  pages = {1484-1496},
  number = {9},
  note = {TY - JOUR U1 - 00095341205 L2 - http://dx.doi.org/10.1109/83.862624
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - B-spline snakes Parametric contour detection
	Noisy image},
  abstract = {We present a novel formulation for B-spline snakes that can be used
	as a tool for fast and intuitive contour outlining. We start with
	a theoretical argument in favor of splines in the traditional formulation
	by showing that the optimal, curvature-constrained snake is a cubic
	spline, irrespective of the form of the external energy field. Unfortunately,
	such regularized snakes suffer from slow convergence speed because
	of a large number of control points, as well as from difficulties
	in determining the weight factors associated to the internal energies
	of the curve. We therefore propose an alternative formulation in
	which the intrinsic scale of the spline model is adjusted a priori;
	this leads to a reduction of the number of parameters to be optimized
	and eliminates the need for internal energies (i.e., the regularization
	term). In other words, we are now controlling the elasticity of the
	spline implicitly and rather intuitively by varying the spacing between
	the spline knots. The theory is embedded into a multiresolution formulation
	demonstrating improved stability in noisy image environments. Validation
	results are presented, comparing the traditional snake using internal
	energies and the proposed approach without internal energies, showing
	the similar performance of the latter. Several biomedical examples
	of application are included to illustrate the versatility of the
	method.},
  keywords = {Edge detection Least squares approximations Curve fitting Algorithms
	Mathematical models Fourier transforms Optimization Spurious signal
	noise Feature extraction Image enhancement Medical applications Image
	analysis},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{BRI98,
  author = {Brigger, Patrick and Unser, Michael},
  title = {Multi-scale B-spline snakes for general contour detection},
  booktitle = {Wavelet Applications in Signal and Image Processing VI, Jul 22-23
	1998},
  year = {1998},
  volume = {3458},
  series = {Proceedings of SPIE},
  pages = {92-102},
  address = {San Diego, CA, United States},
  note = {TY - CONF U1 - 02407121712 L2 - http://dx.doi.org/10.1117/12.328127
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Contour detection Multi-scale snakes},
  abstract = {Traditional snakes suffer from slow convergence speed (many control
	points) and difficult to adjust weighting factors for internal energy
	terms. We propose an alternative formulation using cubic B-splines,
	where the knot spacing is variable and controlled by the user. A
	larger knot spacing allows to reduce the number of parameters, which
	increases optimization speeds. It also eliminates the need for internal
	energies, which improves user interactivity. The optimization procedure
	is embedded into a multi-resolution image representation, where the
	number of snake points is adapted to the image grid spacing by correctly
	adjusting the spline knot spacing. Hence, the proposed method provides
	a multi-scale approach in both the image and parametric contour domain.
	Our technique provides fast optimization of the initial snake curve
	and leads to more stable algorithms in noisy imaging environments.
	Several biomedical examples of applications are included to illustrate
	the versatility of the method.},
  keywords = {Contour measurement User interfaces Biomedical engineering Optimization
	Image analysis},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{CAI007,
  author = {Wenli Cai},
  title = {3D planar reformation of vascular central axis surface with biconvex
	slab.},
  journal = {Comput Med Imaging Graph},
  year = {2007},
  volume = {31},
  pages = {570--576},
  number = {7},
  month = {Oct},
  abstract = {Curved multi-planar reformation (curved MPR) is one of the commonly
	used vascular visualization methods in clinics. It re-samples and
	visualizes the vascular central axis surface (VCAS), which is a curved
	surface passing through the vascular central axis (VCA) or vessel
	centerline. The rotation of the VCAS along the VCA generates a set
	of 2D images. In this paper, we introduce a 3D curved MPR method,
	VCAS planar reformation (VPR) by a convex hull, called a biconvex
	slab. The entire vessel is enclosed within a biconvex slab and rendered
	in one image by volume rendering, such as MIP or X-ray. The method
	is applied to computed tomographic angiography (CTA) data sets. The
	resulting image is clear and free from obstruction by bones and other
	adjacent organs.},
  doi = {10.1016/j.compmedimag.2007.06.007},
  institution = {Department of Radiology, Massachusetts General Hospital/Harvard Medical
	School, 25 New Chardon Street 400C, Boston, MA 02114, USA. Cai.Wenli@mgh.harvard.edu},
  keywords = {Algorithms; Coronary Angiography, methods; Humans; Image Processing,
	Computer-Assisted, methods; Imaging, Three-Dimensional; Tomography,
	X-Ray Computed, methods; United States},
  language = {eng},
  medline-pst = {ppublish},
  owner = {dje},
  pii = {S0895-6111(07)00086-9},
  pmid = {17706399},
  timestamp = {2009.12.11},
  url = {http://dx.doi.org/10.1016/j.compmedimag.2007.06.007}
}

@TECHREPORT{CAI94,
  author = {Cai, Xing and Langtangen, Hans P.},
  title = {A B-Spline Package in C++},
  institution = {SINTEF Informatics},
  year = {1994},
  number = {STF33 A94048},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{Mig06,
  author = {Carreira-Perpi\~n\'an, Miguel \'A.},
  title = {Fast nonparametric clustering with Gaussian blurring mean-shift},
  booktitle = {ICML '06: Proceedings of the 23rd international conference on Machine
	learning},
  year = {2006},
  pages = {153--160},
  address = {New York, NY, USA},
  publisher = {ACM},
  abstract = {We revisit Gaussian blurring mean-shift (GBMS), a procedure that iteratively
	sharpens a dataset by moving each data point according to the Gaussian
	mean-shift algorithm (GMS). (1) We give a criterion to stop the procedure
	as soon as clustering structure has arisen and show that this reliably
	produces image segmentations as good as those of GMS but much faster.
	(2) We prove that GBMS has convergence of cubic order with Gaussian
	clusters (much faster than GMS's, which is of linear order) and that
	the local principal component converges last, which explains the
	powerful clustering and denoising properties of GBMS. (3) We show
	a connection with spectral clustering that suggests GBMS is much
	faster. (4) We further accelerate GBMS by interleaving connected-components
	and blurring steps, achieving 2x--4x speedups without introducing
	an approximation error. In summary, our accelerated GBMS is a simple,
	fast, nonparametric algorithm that achieves segmentations of state-of-the-art
	quality.},
  doi = {http://doi.acm.org/10.1145/1143844.1143864},
  isbn = {1-59593-383-2},
  location = {Pittsburgh, Pennsylvania},
  owner = {euHeart},
  timestamp = {2008.09.26}
}

@INPROCEEDINGS{CAS95,
  author = {Caselles, Vicent},
  title = {Geometric models for active contours},
  booktitle = {Proceedings of the 1995 IEEE International Conference on Image Processing.
	Part 3 (of 3), Oct 23-26 1995},
  year = {1995},
  volume = {3},
  series = {IEEE International Conference on Image Processing},
  pages = {9-12},
  address = {Washington, DC, USA},
  publisher = {IEEE, Los Alamitos, CA, USA},
  note = {TY - CONF U1 - 96013008609 L2 - http://dx.doi.org/10.1109/ICIP.1995.537567
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Active contours Motion tracking Riemannian
	space},
  abstract = {A geometric formulation of active contours for 2D, 3D boundary detection
	and motion tracking is presented. The technique is based on active
	contours evolving in time according to intrinsic geometric measures
	of the image. The evolving contours naturally split and merge, allowing
	the simultaneous detection of several objects and both interior and
	exterior boundaries. The proposed approach is based on the relation
	between active contours and the computation of minimal distance curves
	or minimal surfaces in a Riemannian space whose metric is derived
	from the image. Previous models of geometric active contours are
	improved, allowing stable boundary detection when their gradients
	suffer from large variations, including gaps. Numerical experiments
	are also presented.},
  keywords = {Computational geometry Three dimensional Tracking (position) Image
	analysis Boundary conditions Mathematical models Numerical methods
	Algorithms Topology Object recognition Edge detection},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{CAS97,
  author = {Caselles, Vicent and Kimmel, Ron and Sapiro, Guillermo},
  title = {Geodesic active contours},
  journal = {International Journal of Computer Vision},
  year = {1997},
  volume = {22},
  pages = {61-79},
  number = {1},
  note = {TY - JOUR U1 - 97053637280 L2 - http://dx.doi.org/10.1023/A:1007979827043
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Riemannian space Topology free boundary detection},
  abstract = {A novel scheme for the detection of object boundaries is presented.
	The technique is based on active contours evolving in time according
	to intrinsic geometric measures of the image. The evolving contours
	naturally split and merge, allowing the simultaneous detection of
	several objects and both interior and exterior boundaries. The proposed
	approach is based on the relation between active contours and the
	computation of geodesics or minimal distance curves. The minimal
	distance curve lays in a Riemannian space whose metric is defined
	by the image content. This geodesic approach for object segmentation
	allows to connect classical `snakes' based on energy minimization
	and geometric active contours based on the theory of curve evolution.
	Previous models of geometric active contours are improved, allowing
	stable boundary detection when their gradients suffer from large
	variations, including gaps. Formal results concerning existence,
	uniqueness, stability, and correctness of the evolution are presented
	as well. The scheme was implemented using an efficient algorithm
	for curve evolution. Experimental results of applying the scheme
	to real images including objects with holes and medical data imagery
	demonstrate its power. The results may be extended to 3D object segmentation
	as well.},
  keywords = {Image segmentation Curve fitting Computational geometry Three dimensional
	computer graphics Algorithms Image analysis Edge detection Object
	recognition},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{CAS93,
  author = {Caselles, Vincent and Kimmel, Ron and Sapiro, Guillermo and Sbert,
	C.},
  title = {A Geometrical Model for Active Contours in 3D Images},
  journal = {Numerische Mathematik.},
  year = {1993},
  volume = {66},
  pages = {1-31},
  number = {1},
  note = {Numerische Mathematik},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{CHA01,
  author = {C. Chalopin and G. Finet and I. E. Magnin},
  title = {Modeling the 3D coronary tree for labeling purposes.},
  journal = {Med Image Anal},
  year = {2001},
  volume = {5},
  pages = {301--315},
  number = {4},
  month = {Dec},
  abstract = {Coronary artery diseases are usually revealed using X-ray angiographies.
	Such images are complex to analyze because they provide a 2D projection
	of a 3D object. Medical diagnosis suffers from inter- and intra-clinician
	variability. Therefore, reliable software for the 3D reconstruction
	and labeling of the coronary tree is strongly desired. It requires
	the matching of the vessels in the different available angiograms,
	and an approach which identifies the arteries by their anatomical
	names is a way to solve this difficult problem. This paper focuses
	on the automatic labeling of the left coronary tree in X-ray angiography.
	Our approach is based on a 3D topological model, built from the 3D
	anthropomorphic phantom, Coronix. The phantom is projected under
	different angles of view to provide a data base of 2D topological
	models. On the other hand, the vessel skeleton is extracted from
	the patient's angiogram. The algorithm compares the skeleton with
	the 2D topological model which has the most similar vascular net
	shape. The method performs in a hierarchical manner, first labeling
	the main artery, then the sub-branches. It handles inter-individual
	anatomical variations, segmentation errors and image ambiguities.
	We tested the method on standard angiograms of Coronix and on clinical
	examinations of nine patients. We demonstrated successful scores
	of 90\% correct labeling for the main arteries and 60\% for the sub-branches.
	The method appears to be particularly efficient for the arteries
	in focus. It is therefore a very promising tool for the automatic
	3D reconstruction of the coronary tree from monoplane temporal angiographic
	clinical sequences.},
  institution = {CREATIS, CNRS Research Unit (UMR 5515), INSERM, INSA 502, 69621 cedex,
	Villeurbanne, France.},
  keywords = {Algorithms; Coronary Angiography; Coronary Vessels, anatomy /&/ histology;
	Humans; Imaging, Three-Dimensional; Models, Cardiovascular; Phantoms,
	Imaging; Software},
  owner = {euHeart},
  pii = {S1361841501000470},
  pmid = {11731308},
  timestamp = {2009.02.19}
}

@INPROCEEDINGS{CHE00,
  author = {Chen, Fang and Rose, Stephen E. and Wilson, Stephen J. and Veidt,
	Martin and Bennett, Cameron J. and Doddrell, David M.},
  title = {Analysis of myocardial motion using generalized spline models and
	tagged magnetic resonance images},
  booktitle = {Medical Imaging 2000: Image Processing, Feb 14-Feb 17 2000},
  year = {2000},
  volume = {3979 (I},
  series = {Proceedings of SPIE - The International Society for Optical Engineering},
  pages = {1209-1217},
  address = {San Diego, CA, USA},
  publisher = {Society of Photo-Optical Instrumentation Engineers, Bellingham, WA,
	USA},
  note = {TY - CONF U1 - 00085260811 L2 - http://dx.doi.org/10.1117/12.387628
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Myocardial motion Generalized spline models
	Snakes tracking Cardiac diastole Displacement field reconstruction},
  abstract = {Heart wall motion abnormalities are the very sensitive indicators
	of common heart diseases, such as myocardial infarction and ischemia.
	Regional strain analysis is especially important in diagnosing local
	abnormalities and mechanical changes in the myocardium. In this work,
	we present a complete method for the analysis of cardiac motion and
	the evaluation of regional strain in the left ventricular wall. The
	method is based on the generalized spline models and tagged magnetic
	resonance images (MRI) of the left ventricle. The whole method combines
	dynamical tracking of tag deformation, simulating cardiac movement
	and accurately computing the regional strain distribution. More specifically,
	the analysis of cardiac motion is performed in three stages. Firstly,
	material points within the myocardium are tracked over time using
	a semi-automated snake-based tag tracking algorithm developed for
	this purpose. This procedure is repeated in three orthogonal axes
	so as to generate a set of one-dimensional sample measurements of
	the displacement field. The 3D-displacement field is then reconstructed
	from this sample set by using a generalized vector spline model.
	The spline reconstruction of the displacement field is explicitly
	expressed as a linear combination of a spline kernel function associated
	with each sample point and a polynomial term. Finally, the strain
	tensor (linear or nonlinear) with three direct components and three
	shear components is calculated by applying a differential operator
	directly to the displacement function. The proposed method is computationally
	effective and easy to perform on tagged MR images. The preliminary
	study has shown potential advantages of using this method for the
	analysis of myocardial motion and the quantification of regional
	strain.},
  keywords = {Magnetic resonance imaging Image analysis Cardiovascular system Computer
	simulation Algorithms Image reconstruction Mathematical models Vectors
	Functions Polynomials Tensors Medical imaging},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{CHE97,
  author = {Chen, F. and Suter, D.},
  title = {Elastic spline models for human cardiac motion estimation},
  booktitle = {Proceedings of the 1997 IEEE Nonrigid and Articulated Motion Workshop,
	Jun 16 1997},
  year = {1997},
  series = {Proceedings of the IEEE Nonrigid and Articulated Motion Workshop},
  pages = {120-127},
  address = {San Juan, PR, USA},
  publisher = {IEEE, Piscataway, NJ, USA},
  note = {TY - CONF U1 - 97083777410 L2 - http://dx.doi.org/10.1109/NAMW.1997.609862
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Elastic splines Motion estimation},
  abstract = {Elastic splines (including dynamic `snakes' and elastic contours),
	minimizing an energy norm of the membrane and/or thin-plate types,
	have been used to model many surfaces in visual reconstruction and
	related biomedical applications. In this paper, we model the displacement
	of the material between successive cardiac images using vector splines.
	We define a family of elastic splines. These splines can be tuned
	to enforce different type and different degrees of smoothness. We
	assess how well these splines can be used to reconstruct human cardiac
	motion.},
  keywords = {Cardiovascular system Image reconstruction Biomedical engineering
	Vectors Medical imaging},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{CHE05,
  author = {Chen, Xujian and Teoh, Eam Khwang},
  title = {3D object segmentation using B-Surface},
  journal = {Image and Vision Computing},
  year = {2005},
  volume = {23},
  pages = {1237-1249},
  number = {14},
  note = {TY - JOUR},
  abstract = {3D object segmentation is important in computer vision such as target
	detection in biomedical image analysis. A new method, called B-Surface
	algorithm, is generated for 3D object segmentation. An improved 3D
	external force field combined with the normalized GVF is utilized.
	After the initialization of a surface model near the target, B-Surface
	starts to deform to locate the boundary of the object. First, it
	overcomes the difficulty that comes from analyzing 3D volume image
	slice by slice. And the speed of B-Surface deformation is enhanced
	since the internal forces are not needed to compute in every iteration
	deformation step. Next, the normal at every surface point can be
	calculated easily since B-Surface is a continuous deformable model.
	And it has the ability to achieve high compression ratio (ratio of
	data to parameters) by presenting the whole surface with only a relatively
	small number of control points. Experimental results and analysis
	are presented in this paper. We can see that the B-Surface algorithm
	can find the surface of the target efficiently.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{CHE04,
  author = {Chen, Xujian and Teoh, Earn Khwang},
  title = {3D Growing deformable B-surface model for object detection},
  booktitle = {ICARCV 2004 - Proceedings of the 8th International Conference on
	Control, Automation, Robotics and Vision},
  year = {2004},
  series = {ICARCV 2004 - Proceedings of the 8th International Conference on
	Control, Automation, Robotics and Vision},
  pages = {357-362},
  address = {Kunming, China},
  publisher = {Institute of Electrical and Electronics Engineers Inc., New York,
	NY 10016-5997, United States},
  note = {05159037666 Compilation and indexing terms, Copyright 2005 Elsevier
	Engineering Information, Inc. Object detection 3D growing deformable
	B-surface model Boundary detection Image features},
  abstract = {A new method, called 3D growing deformable B-Surface model, is proposed
	for object detection which works in 3D space directly. First, the
	coarse boundary of the subject is extracted. The 3D external force
	&pound;eld of the subject is generated based on this coarse boundary
	using modified GVF (gradient vector now). After the initialization
	of a surface patch, growing B-Surface model starts to deform it to
	locate the boundary of the object. Next, this surface patch is anchored
	to the surface of the subject and a new surface patch grows up based
	it. This process is repeated until a closed surface of the subject
	is obtained. 3D growing deformable B-Surface model overcomes the
	difficulty that comes from analyzing 3D volume image slice by slice.
	And the computation load of B-Surface is reduced since the internal
	force is not necessary in every iteration deformation step. Next,
	the geometric information on every surface point can be calculated
	easily. And it has the ability to achieve high compression ratio
	(ratio of data to parameters) by presenting the whole surface with
	only a relatively small number of control points. Growing B-Surface
	model simplifies the initialization step of the surface model. &copy;
	2004 IEEE.},
  keywords = {Image segmentation Three dimensional Mathematical models Deformation
	Algorithms Matrix algebra Optimization Computer simulation Object
	recognition},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{CIP97,
  author = {Cipolla, Roberto and Blake, Andrew},
  title = {Image divergence and deformation from closed curves},
  journal = {International Journal of Robotics Research},
  year = {1997},
  volume = {16},
  pages = {77-96},
  number = {1},
  note = {TY - JOUR U1 - 97033576923 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc. U2 - Image velocity field
	Closed form functions},
  abstract = {This article describes a novel method for measuring the differential
	invariants of the image velocity field from the integral of normal
	image velocities around image contours. This is equivalent to measuring
	the temporal changes in the area of a closed contour. This avoids
	having to recover a dense image velocity field and take partial derivatives.
	It also does not require point or line correspondences. Moreover,
	integration provides some immunity to image measurement noise. It
	is shown how an active observer making small, deliberate motions
	can use the estimates of the divergence and deformation of the image
	velocity field to determine the object-surface orientation and time
	to contact. The results of real-time experiments are presented in
	which arbitrary image shapes are tracked using B-spline snakes, and
	the invariants are computed efficiently as closed-form functions
	of the B-spline control points. This information is used to guide
	a robot manipulator in obstacle collision avoidance, object manipulation,
	and navigation.},
  keywords = {Image processing Invariance Real time systems Manipulators Collision
	avoidance Computer vision},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{COA08,
  author = {Coatrieux, J. -L.},
  title = {Moment-based approaches in imaging part 2: invariance [A Look at
	. . .]},
  journal = IEEE_M_EMB,
  year = {2008},
  volume = {27},
  pages = {81--83},
  number = {1},
  abstract = {This short survey of moment invariants points out interesting clues.
	Moments offer a sound theoretical framework for solving the generic
	problems encountered in many imaging applications. The diverse families
	of orthogonal moments provide the flexibility that may be required
	to face a particular target. However, they have to satisfy the time
	computation constraints that are inherent in many applications. Suk
	and Flusser proposed a solution of simultaneously dealing with affine
	transformation and blur (with centrosymmetric PSF) for pattern recognition,
	template matching, and image registration. Flusser and Zivota suggested
	a set of combined moments that are invariant to both rotation and
	blurring. Based on complex moments, Liu and Zhang derived a subset
	of moment features that are not affected by image blurring and geometric
	transformation such as translation, scale, and rotation. All these
	works, however, point out the problems related to the number of invariants
	to be selected, the choice of the size of the region of interest
	where moments are computed, and the dependence with object features
	(i.e., symmetry).},
  doi = {10.1109/MEMB.2007.911462},
  issn = {0739-5175},
  keywords = {computer vision, image matching, image registration, image restoration,
	medical image processing, computer vision, generic problems, geometric
	transformation, image blurring, image registration, medical imaging,
	moment invariants, moment-based approach, object features, orthogonal
	moments, pattern recognition, template matching, time computation
	constraints},
  owner = {euHeart},
  timestamp = {2008.09.15}
}

@INPROCEEDINGS{COA05,
  author = {Coatrieux, Jean Louis and Hernández, Alfredo I. and Mabo, Philippe
	and Garreau, Mireille and Haigron, Pascal},
  title = {Transvenous Path Finding in Cardiac Resynchronization Therapy},
  booktitle = {Functional Imaging and Modeling of the Heart},
  year = {2005},
  pages = {236--245},
  abstract = {Cardiovascular diseases are a major health concern all over the world
	and, especially, heart failure has gained more importance in the
	recent years. Improving diagnosis and therapy is therefore critical
	and among the several resources at our disposal, implantable devices
	is expected to have a better rate of success. This paper is focused
	on two topics: (i) our views of the main challenges to face in order
	to reach these objectives and (ii) a specific target regarding the
	pose of leads for multisite pacemakers by means of virtual endoscopy
	pre-operative planning and path finding throughout the coronary venous
	tree.},
  owner = {euHeart},
  timestamp = {2008.09.10},
  url = {http://dx.doi.org/10.1007/11494621\_24}
}

@ARTICLE{COA06,
  author = {Coatrieux, J. -L. and Rioual, K. and Goksu, C. and Unanua, E. and
	Haigron, P.},
  title = {Ray Casting With ''On-the-Fly'' Region Growing: 3-D Navigation Into
	Cardiac MSCT Volume},
  journal = IEEE_J_ITBM,
  year = {2006},
  volume = {10},
  pages = {417--420},
  number = {2},
  month = {April },
  abstract = {We describe an extended ray casting scheme for 3-D navigation into
	the heart cavities for preoperative planning using multislice X-Ray
	Computed Tomography data. The key benefit is that artifacts due to
	contrast inhomogeneities can be eliminated during volume traversal,
	thus improving the visual perception of the endocardial wall.},
  doi = {10.1109/TITB.2005.864373},
  owner = {euHeart},
  timestamp = {2008.09.09}
}

@ARTICLE{COA10,
  author = {Jean-Louis Coatrieux and Jérôme Velut and Jean-Louis Dillenseger
	and Christine Toumoulin},
  title = {From medical imaging to image-guided therapy.},
  journal = {Med Sci (Paris)},
  year = {2010},
  volume = {26},
  pages = {1103--1109},
  number = {12},
  month = {Dec},
  __markedentry = {[euHeart]},
  abstract = {This survey on medical imaging provides a look into three major components.
	The first one deals with the full steps through which it must be
	apprehended: from the sensors to the reconstruction, from the image
	analysis up to its interpretation. The second aspect describes the
	physical principles used for imaging (magnetic resonance, acoustic,
	optics, etc.). The last section shows how imaging is involved in
	therapeutic procedures and in particular the new physical therapies.
	All along this paper, the research perspectives are sketched.},
  institution = {Laboratoire Traitement du signal et de l'image, Inserm U642, Université
	de Rennes 1, 35000 Rennes, France. jean-louis.coatrieux@univ-rennes1.fr.},
  language = {fre},
  medline-pst = {ppublish},
  owner = {euHeart},
  pii = {00/00/10/1D/},
  pmid = {21187052},
  timestamp = {2011.01.10}
}

@ARTICLE{COH00,
  author = {Cohen, Fernand S. and Ibrahim, Walid and Pintavirooj, Chuchart},
  title = {Ordering and Parameterizing Scattered 3D Data for B-Spline Surface
	Approximation.},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2000},
  volume = {22},
  pages = {642-648},
  number = {6},
  abstract = {Surface representation is intrinsic to many applications in medical
	imaging, computer vision, and computer graphics. We present a method
	that is based on surface modeling by B-Spline. The B-Spline constructs
	a smooth surface that best fits a set of scattered unordered 3D range
	data points obtained from either a structured light system (a range
	finder), or from point coordinates on the external contours of a
	set of surface sections, as for example in histological coronal brain
	sections. B-Spline stands as of one the most efficient surface representations.
	It possesses many properties such as boundedness, continuity, local
	shape controllability, and invariance to affine transformations that
	makes it very suitable and attractive for surface representation.
	Despite its attractive properties, however, B-Spline has not been
	widely applied for representing a 3D scattered nonordered data set.
	This may be due to the problem in finding an ordering and a choice
	for the topological parameters of the B-Spline that lead to a physically
	meaningful surface parameterization based on the scattered data set.
	The parameters needed for the B-Spline surface construction, as well
	as finding the ordering of the data points, are calculated based
	on the geodesics of the surface extended Gaussian map. The set of
	control points is analytically calculated by solving a minimum mean
	square error problem for best surface fitting. For a noise immune
	odeling, we elect to use an approximating rather than an interpolating
	B-Spline. We also examine ways of making the B-Spline fitting technique
	robust to local deformation and noise.},
  keywords = {Surface fitting, B-Spline, Gaussian map, geodesics, NURBS.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@PHDTHESIS{COH92,
  author = {Cohen, Isaac},
  title = {Mod\`eles d\'eformables 2-D et 3-D : Application \`a la Segmentation
	d'Images M\'edicales},
  school = {Paris IX - Dauphin\'e},
  year = {1992},
  type = {Thèse de doctorat},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@TECHREPORT{COH89,
  author = {Cohen, Laurent D.},
  title = {On Active Contour Models},
  institution = {INRIA Rocquencourt},
  year = {1989},
  type = {Programmation, Calcul Symbolique et Intelligence Artificielle},
  number = {Rapport de Recherche 1075},
  keywords = {active contours, balloon.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{COH93,
  author = {Cohen, Laurent D. and Cohen, Isaac},
  title = {Finite-element methods for active contour models and balloons for
	2-D and 3-D images},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {1993},
  volume = {15},
  pages = {1131-1147},
  number = {11},
  note = {TY - JOUR U1 - 94011166692 L2 - http://dx.doi.org/10.1109/34.244675
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Active contour model Attraction potential
	Deformable model Two dimensional Edgel Feature extraction Regularization
	Segmentation Surface reconstruction Energy minimizing curve},
  abstract = {The use of energy-minimizing curves, known as 'snakes' to extract
	features of interest in images has been introduced by Kass, Witkin
	and Terzopoulos [23]. A balloon model was introduced in [12] as a
	way to generalize and solve some of the problems encountered with
	the original method. A 3-D generalization of the balloon model as
	a 3-D deformable surface, which evolves in 3-D images, is presented.
	It is deformed under the action of internal and external forces attracting
	the surface toward detected edgels by means of an attraction potential.
	We also show properties of energy-minimizing surfaces concerning
	their relationship with 3-D edge points. To solve the minimization
	problem for a surface, two simplified approaches are shown first,
	defining a 3-D surface as a series of 2-D planar curves. Then, after
	comparing finite-element method and finite-difference method in the
	2-D problem, we solve the 3-D model using the finite-element method
	yielding greater stability and faster convergence. This model is
	applied for segmenting magnetic resonance images.},
  keywords = {Finite element method Contour measurement Balloons Finite difference
	method Magnetic resonance imaging Three dimensional Convergence of
	numerical methods Mathematical models Curve fitting Differential
	equations Image analysis},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@TECHREPORT{COH90,
  author = {Cohen, Laurent D. and Cohen, Isaac},
  title = {A Finite Element Method Applied To New Active Contour Models and
	3D Reconstruction From Cross Sections},
  institution = {INRIA Rocquencourt},
  year = {1990},
  type = {Programme 6 -- Robotique, Image et Vision},
  number = {Rapport de Recherche 1245},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{COH07,
  author = {Laurent D Cohen and Thomas Deschamps},
  title = {Segmentation of 3D tubular objects with adaptive front propagation
	and minimal tree extraction for 3D medical imaging.},
  journal = {Comput Methods Biomech Biomed Engin},
  year = {2007},
  volume = {10},
  pages = {289--305},
  number = {4},
  month = {Aug},
  abstract = {We present a new fast approach for segmentation of thin branching
	structures, like vascular trees, based on Fast-Marching (FM) and
	Level Set (LS) methods. FM allows segmentation of tubular structures
	by inflating a "long balloon" from a user given single point. However,
	when the tubular shape is rather long, the front propagation may
	blow up through the boundary of the desired shape close to the starting
	point. Our contribution is focused on a method to propagate only
	the useful part of the front while freezing the rest of it. We demonstrate
	its ability to segment quickly and accurately tubular and tree-like
	structures. We also develop a useful stopping criterion for the causal
	front propagation. We finally derive an efficient algorithm for extracting
	an underlying 1D skeleton of the branching objects, with minimal
	path techniques. Each branch being represented by its centerline,
	we automatically detect the bifurcations, leading to the "Minimal
	Tree" representation. This so-called "Minimal Tree" is very useful
	for visualization and quantification of the pathologies in our anatomical
	data sets. We illustrate our algorithms by applying it to several
	arteries datasets.},
  doi = {10.1080/10255840701328239},
  institution = {CEREMADE, UMR CNRS 7534, University Paris-9 Dauphine, Paris, France.
	cohen@ceremade.dauphine.fr},
  keywords = {Algorithms; Aorta, anatomy /&/ histology; Arteries, anatomy /&/ histology;
	Colon, anatomy /&/ histology; Humans; Image Interpretation, Computer-Assisted;
	Imaging, Three-Dimensional; Models, Theoretical; Pattern Recognition,
	Automated},
  owner = {euHeart},
  pii = {779054167},
  pmid = {17671862},
  timestamp = {2008.10.23},
  url = {http://dx.doi.org/10.1080/10255840701328239}
}

@INPROCEEDINGS{COL94,
  author = {Collet, C. and Thourel, P.},
  title = {Active contour models for infrared cloudy shapes segmentation},
  booktitle = {Proceedings of the 1994 IEEE Oceans Conference. Part 2 (of 3), Sep
	13-16 1994},
  year = {1994},
  volume = {2},
  series = {Oceans Conference Record (IEEE)},
  pages = {444-448},
  address = {Brest, Fr},
  publisher = {IEEE, Piscataway, NJ, USA},
  note = {TY - CONF U1 - 95032631237 L2 - http://dx.doi.org/10.1109/OCEANS.1994.364085
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Infrared cloudy shapes segmentation Active
	contour model Snake Image sequence Spline curve},
  abstract = {This study proposes a new method for the detection of infrared cloudy
	shapes segmentation. This active contour model called `snake' introduces
	the concept of deformable closed edges. An illustration of the new
	technique for cloudy edge detection is presented. A normalization
	of energy functions governing the snake evolution which allows the
	comparison of the final energy level of the snake for different patterns
	is proposed.},
  keywords = {Clouds Infrared imaging Geometry Edge detection Sensors Vision Aerospace
	applications Image segmentation},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{COU02,
  author = {Coulon, O. and Hickman, SJ. and Parker, GJ and Barker, GJ. and Miller,
	DH and Arridge, SR},
  title = {Quantification of spinal cord atrophy from magnetic resonance images
	via a B-spline active surface model},
  journal = {Magnetic Resonance in Medicine},
  year = {2002},
  volume = {47},
  pages = {1176-1185},
  number = {6},
  timestamp = {2007.10.18}
}

@ARTICLE{Cra04,
  author = {Edmund J Crampin and Matthew Halstead and Peter Hunter and Poul Nielsen
	and Denis Noble and Nicolas Smith and Merryn Tawhai},
  title = {Computational physiology and the Physiome Project.},
  journal = {Exp Physiol},
  year = {2004},
  volume = {89},
  pages = {1--26},
  number = {1},
  month = {Jan},
  abstract = {Bioengineering analyses of physiological systems use the computational
	solution of physical conservation laws on anatomically detailed geometric
	models to understand the physiological function of intact organs
	in terms of the properties and behaviour of the cells and tissues
	within the organ. By linking behaviour in a quantitative, mathematically
	defined sense across multiple scales of biological organization--from
	proteins to cells, tissues, organs and organ systems--these methods
	have the potential to link patient-specific knowledge at the two
	ends of these spatial scales. A genetic profile linked to cardiac
	ion channel mutations, for example, can be interpreted in relation
	to body surface ECG measurements via a mathematical model of the
	heart and torso, which includes the spatial distribution of cardiac
	ion channels throughout the myocardium and the individual kinetics
	for each of the approximately 50 types of ion channel, exchanger
	or pump known to be present in the heart. Similarly, linking molecular
	defects such as mutations of chloride ion channels in lung epithelial
	cells to the integrated function of the intact lung requires models
	that include the detailed anatomy of the lungs, the physics of air
	flow, blood flow and gas exchange, together with the large deformation
	mechanics of breathing. Organizing this large body of knowledge into
	a coherent framework for modelling requires the development of ontologies,
	markup languages for encoding models, and web-accessible distributed
	databases. In this article we review the state of the field at all
	the relevant levels, and the tools that are being developed to tackle
	such complexity. Integrative physiology is central to the interpretation
	of genomic and proteomic data, and is becoming a highly quantitative,
	computer-intensive discipline.},
  institution = {Centre for Mathematical Biology, Mathematical Institute, University
	of Oxford, 24-29 St Giles, Oxford, OX1 3LB, UK.},
  keywords = {Biophysics; Computer Simulation; Heart, physiology; Humans; Lung,
	physiology; Models, Biological},
  owner = {euHeart},
  pmid = {15109205},
  timestamp = {2008.09.26}
}

@ARTICLE{DAN04,
  author = {Peter G Danias and Arkadios Roussakis and John P A Ioannidis},
  title = {Diagnostic performance of coronary magnetic resonance angiography
	as compared against conventional X-ray angiography: a meta-analysis.},
  journal = {J Am Coll Cardiol},
  year = {2004},
  volume = {44},
  pages = {1867--1876},
  number = {9},
  month = {Nov},
  abstract = {OBJECTIVES: This study was designed to define the current role of
	coronary magnetic resonance angiography (CMRA) for the diagnosis
	of coronary artery disease (CAD). BACKGROUND: Coronary magnetic resonance
	angiography has been proposed as a promising noninvasive method for
	diagnosis of CAD, but individual studies evaluating its clinical
	value have been of limited sample size. METHODS: We identified all
	studies (MEDLINE and EMBASE) that evaluated CAD by both CMRA and
	conventional angiography in >/=10 subjects during the period 1991
	to January 2004. We recorded true and false positive and true and
	false negative CMRA assessments for detection of CAD using X-ray
	angiography as the reference standard. Analysis was done at segment,
	vessel, and subject level. RESULTS: We analyzed 39 studies (41 separate
	comparisons). Across 25 studies (27 comparisons) with data on 4,620
	segments (993 subjects), sensitivity and specificity for detection
	of CAD were 73\% and 86\%, respectively. Vessel-level analyses (16
	studies, 2,041 vessels) showed sensitivity 75\% and specificity 85\%.
	Subject-level analyses (13 studies, 607 subjects) showed sensitivity
	88\% and specificity 56\%. At the segment level, sensitivity was
	69\% to 79\% for all but the left circumflex (61\%) coronary artery;
	specificity was 82\% to 91\%. There was considerable between-study
	heterogeneity, but weighted summary receiver-operating characteristic
	curves agreed with these estimates. There were no major differences
	between subgroups based on technical or population characteristics,
	year of publication, reported blinding, or sample size. CONCLUSIONS:
	In evaluable segments of the native coronary arteries, CMRA has moderately
	high sensitivity for detecting significant proximal stenoses and
	may have value for exclusion of significant multivessel CAD in selected
	subjects considered for diagnostic catheterization.},
  doi = {10.1016/j.jacc.2004.07.051},
  institution = {2nd Cardiology Clinic, Hygeia Hospital, Athens, Greece. pdanias@hygeia.gr},
  keywords = {Coronary Angiography, methods; Coronary Artery Disease, diagnosis;
	False Positive Reactions; Humans; Image Processing, Computer-Assisted;
	Magnetic Resonance Angiography, methods; Radiographic Image Enhancement;
	Sensitivity and Specificity; X-Rays},
  language = {eng},
  medline-pst = {ppublish},
  owner = {euHeart},
  pii = {S0735-1097(04)01634-1},
  pmid = {15519021},
  timestamp = {2010.01.06},
  url = {http://dx.doi.org/10.1016/j.jacc.2004.07.051}
}

@ARTICLE{DAS04.1,
  author = {Das, B. and Banerjee, S.},
  title = {Inertial snake for contour detection in ultrasonography images},
  journal = {IEE Proceedings: Vision, Image and Signal Processing},
  year = {2004},
  volume = {151},
  pages = {235-240},
  number = {3},
  note = {TY - JOUR U1 - 04328304849 L2 - http://dx.doi.org/10.1049/ip-vis:20040310
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Contour detection Inertial snakes Active contour
	models},
  abstract = {Snakes, or active contour models are used extensively for image segmentation
	in varied fields. However, some major challenges restrict their use
	in many fields. The authors propose a new inertial snake model, that
	introduces an inertial effect of the control points into the snake
	framework. The proposed inertial force along with the first- and
	second-order continuity forces controls the spline motion through
	the concavities and also against weak edge forces. This smart force
	field, added to the inertial energy framework, posses the ability
	to adaptively reduce its effect near the true edges, so that the
	energy minimising spline converges into the edges. A greedy snake
	has been used for computation of the energy minimising spline. The
	algorithm has been tested on phantoms and ultrasound images as well.
	It is shown in the results that the proposed algorithm classifies
	the object from the background class in most of the images perfectly.
	Ultrasound images of a lower limb artery of an adult woman have been
	tested with this algorithm, and also extended for motion tracking.
	&copy; IEE, 2004.},
  keywords = {Edge detection Image segmentation Object recognition Motion estimation
	Algorithms Mathematical models Computational complexity Ultrasonic
	imaging},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{DAS04.2,
  author = {Das, Bipul and Saha, P. Kumar and Wehrli, Felix W.},
  title = {Object class uncertainty induced snake with applications to medical
	image segmentation},
  booktitle = {Medical Imaging},
  year = {2004},
  volume = {5370 I},
  pages = {369-380},
  address = {San Diego, CA, United States},
  publisher = {International Society for Optical Engineering},
  abstract = {Object segmentation is of paramount interest in many medical imaging
	applications. Among others, "snake" - an "active contour" - is a
	popular boundary-based segmentation framework where a spline is continuously
	deformed to lock onto an object boundary. The dynamics of a snake
	is governed by different internal and external forces. A major limitation
	of this framework has been the difficulty in using object-intensity
	driven features into snake dynamics which may prevent uncontrolled
	expansion/contraction once the snake leaks through a weak boundary
	region. In this paper, object-intensity force is effectively introduced
	into the snake-model using class uncertainty theory. Given a priori
	knowledge of object/background intensity distributions, class uncertainty
	yields object/background class of any location and establishes the
	confidence level of the classification. This confidence level has
	previously been demonstrated to be high inside the object/background
	regions and low near boundaries with intermediate intensities. This
	class uncertainty information adds an expanding (outward) force at
	locations pertaining to intensity-based object class and a squeezing
	(inward) force inside background regions. Consequently, the method
	possesses potential to resist an uncontrolled expansion of the snake
	(for an expanding type) into the background through a weak boundary
	while reducing the effect of this force near the boundary using the
	confidence information. The theory of object class uncertainty induced
	snake is developed and an implementation with efficient graphical
	interface is achieved. Preliminary results of application of the
	proposed snake approach on different images are presented and comparisons
	with conventional snake approaches are demonstrated.},
  keywords = {Image segmentation Elasticity Deformation Parameter estimation Data
	reduction Fuzzy sets Algorithms Mathematical models Medical imaging},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{DAU06,
  author = {J. Claude Daubert and Christophe Leclercq and Erwan Donal and Philippe
	Mabo},
  title = {Cardiac resynchronisation therapy in heart failure: current status.},
  journal = {Heart Fail Rev},
  year = {2006},
  volume = {11},
  pages = {147--154},
  number = {2},
  month = {Jun},
  abstract = {Cardiac resynchronization in heart failure already has a history of
	12 years. However, the major advances have been the result of large
	multi center trials dating from 2001. In all these trials patients
	with a LVEF < or = 35\% were included, and a QRS above 120 msec.
	Follow up was from 3-36 months. The majority of these trials showed
	a positive effect in reduction of composite and points of death or
	hospitalization for major cardiovascular events. Many of these trials
	also showed a diminution of left ventricular and systolic diameter
	or volume. Even in NYHA class II patients an improvement was seen.
	Some unanswered questions still remain as regards the agreement on
	electrical or electromechanical dyssynchrony criteria. There is a
	number of patients with "wide" QRS who do not improve and conversely
	a number of patients with a narrow QRS who witness improvement. The
	benefit in patients with atrial fibrillation also remains unanswered.
	Finally the value of this modality in patients with mild heart failure
	or asymptomatic left ventricular systolic dysfunction, NYHA class
	I-II remains to be determined in large on going trials. Another question
	is whether biventricular or left ventricular patient is preferable.
	Finally whether biventricular patient should be complemented by a
	defibrillator insertion is being currently studied. Cardiac resynchronization
	therapy along or in combination with an ICD improves symptoms, reduces
	major morbidity and mortality in patients with a left ventricular
	EF<35\%, ventricular dilatation and a QRS > or = 120 msec in NYHA
	class III-IV. Further indications are currently being examined.},
  doi = {10.1007/s10741-006-9485-9},
  institution = {culaires, CHU Rennes, France.},
  keywords = {Cardiac Pacing, Artificial; Heart Failure, classification/pathology/therapy;
	Humans; Randomized Controlled Trials as Topic; Severity of Illness
	Index},
  owner = {euHeart},
  pmid = {16937034},
  timestamp = {2009.05.04},
  url = {http://dx.doi.org/10.1007/s10741-006-9485-9}
}

@ARTICLE{DAV96,
  author = {Davatzikos, Christos and Bryan, R. Nick},
  title = {Using a deformable surface model to obtain a shape representation
	of the cortex},
  journal = {IEEE Transactions on Medical Imaging},
  year = {1996},
  volume = {15},
  pages = {785-795},
  number = {6},
  note = {97013488363 Compilation and indexing terms, Copyright 2005 Elsevier
	Engineering Information, Inc. 0278-0062 Brain cortex Deformable surface
	model},
  abstract = {This paper examines the problem of obtaining a mathematical representation
	of the outer cortex of the human brain, which is a key problem in
	several applications, including morphological analysis of the brain,
	and spatial normalization and registration of brain images. A parameterization
	of the outer cortex is first obtained using a deformable surface
	algorithm which, motivated by the structure of the cortex, is constructed
	to find the central layer of thick surfaces. Based on this parameterization,
	a hierarchical representation of the outer cortical structure is
	proposed through its depth map and its curvature maps at various
	scales. Various experiments on magnetic resonance data are presented.},
  keywords = {Image analysis Brain models Mathematical morphology Parameter estimation
	Algorithms Magnetic resonance imaging Medical imaging},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{DAV93,
  author = {Davatzikos, Chris and Prince, Jerry L.},
  title = {Adaptive active contour algorithms for extracting and mapping thick
	curves},
  booktitle = {CVPR},
  year = {1993},
  pages = {524 - 528},
  address = {New York, NY, USA},
  note = {Active contour algorithms;Boundary estimation;},
  abstract = {Thick curves arise naturally in certain applications such as magnetic
	resonance imaging of the brain; they can also arise in computer vision
	problems through morphological dilation of boundaries of objects.
	In this paper we describe two new adaptive active contour algorithms
	for the extraction and mapping of the skeleton of a thick curve.
	They are based on conditions that have been derived in previous work
	which guarantee uniqueness and fidelity of the solution. Both algorithms
	modify the regularization constant K<sub>o</sub> in attempt to maintain
	convexity of the energy function while simultaneously improving the
	fidelity of the result. The first algorithm changes K<sub>o</sub>
	over time while the second adapts K<sub>o</sub> spatially. We evaluate
	both algorithms on experiments with synthetic curves; both demonstrate
	an improved performance compared to a fixed parameter active contour
	algorithm.},
  copyright = {Compilation and indexing terms, Copyright 2007 Elsevier Inc. All rights
	reserved},
  key = {Image processing},
  keywords = {Mapping;Algorithms;Computer vision;Mathematical techniques;Magnetic
	resonance imaging;Medical imaging;},
  language = {English},
  owner = {euHeart},
  timestamp = {2009.02.24},
  url = {http://dx.doi.org/10.1109/CVPR.1993.341080}
}

@ARTICLE{DEA90,
  author = {P. Dean and L. Mascio and D. Ow and D. Sudar and J. Mullikin},
  title = {Proposed standard for image cytometry data files.},
  journal = {Cytometry},
  year = {1990},
  volume = {11},
  pages = {561--569},
  number = {5},
  abstract = {A number of different types of computers running a variety of operating
	systems are presently used for the collection and analysis of image
	cytometry data. In order to facilitate the development of sharable
	data analysis programs, to allow for the transport of image cytometry
	data from one installation to another, and to provide a uniform and
	controlled means for including textual information in data files,
	this document describes a data storage format that is proposed as
	a standard for use in image cytometry. In this standard, data from
	an image measurement are stored in a minimum of two files. One file
	is written in ASCII to include information about the way the image
	data are written and optionally, information about the sample, experiment,
	equipment, etc. The image data are written separately into a binary
	file. This standard is proposed with the intention that it will be
	used internationally for the storage and handling of biomedical image
	cytometry data. The method of data storage described in this paper
	is similar to those methods published in American Association of
	Physicists in Medicine (AAPM) Report Number 10 and in ACR-NEMA Standards
	Publication Number 300-1985.},
  doi = {10.1002/cyto.990110502},
  institution = {Biomedical Sciences Division, Lawrence Livermore National Laboratory,
	Livermore, California 94551.},
  keywords = {Automatic Data Processing; Flow Cytometry, instrumentation; Software;
	ICS format; IDS format},
  owner = {euHeart},
  pmid = {2379446},
  timestamp = {2009.04.22},
  url = {http://dx.doi.org/10.1002/cyto.990110502}
}

@ARTICLE{DEL99,
  author = {Delingette, Herv\'e},
  title = {General Object Reconstruction Based on Simplex Meshes},
  journal = {International Journal of Computer Vision},
  year = {1999},
  volume = {32},
  pages = {111--146},
  number = {2},
  month = sep,
  abstract = {In this paper, we propose a general tridimensional reconstruction
	algorithm of range and volumetric images, based on deformable simplex
	meshes. Simplex meshes are topologically dual of triangulations and
	have the advantage of permitting smooth deformations in a simple
	and efficient manner. Our reconstruction algorithm can handle surfaces
	without any restriction on their shape or topology. The different
	tasks performed during the reconstruction include the segmentation
	of given objects in the scene, the extrapolation of missing data,
	and the control of smoothness, density, and geometric quality of
	the reconstructed meshes. The reconstruction takes place in two stages.
	First, the initialization stage creates a simplex mesh in the vicinity
	of the data model either manually or using an automatic procedure.
	Then, after a few iterations, the mesh topology can be modified by
	creating holes or by increasing its genus. Finally, an iterative
	refinement algorithm decreases the distance of the mesh from the
	data while preserving high geometric and topological quality. Several
	reconstruction examples are provided with quantitative and qualitative
	results.},
  owner = {euHeart},
  timestamp = {2009.02.24},
  url = {http://dx.doi.org/10.1023/A:1008157432188}
}

@INPROCEEDINGS{DEL94.1,
  author = {H. Delingette},
  title = {Simplex Meshes: a General Representation for {3D} Shape Reconstruction},
  booktitle = {CVPR},
  year = {1994},
  pages = {856-857},
  address = {Seattle, USA},
  month = {June},
  abstract = {In this report, we develop the concept of simplex mesh as a representation
	of deformable models. Simplex meshes are simply connected meshes
	that are topologically dual of triangulations. In a previous work,
	we have introduced the simplex mesh representation for performing
	recognition of partially occluded smooth objects. In this paper,
	we present a physically-based approach for recovering three-dimensional
	objects, based on the geometry of simplex meshes. Elastic behavior
	is modeled by local stabilizing functionals, controlling the mean
	curvature through the simplex angle extracted at each vertex. Those
	functionals are viewpoint-invariant, intrinsic and scale-sensitive.
	They control either the normal orientation or the curvature continuity
	of the mesh or its closeness to a given reference shape.
	
	Unlike deformable surfaces defined on regular grids, simplex meshes
	can have their topology locally altered. We have developed an adaptation
	and renement process that respectively concentrates vertices and
	increases the mesh resolution at highly curved or inaccurate parts.
	The adaptivity of a mesh is governed by the minimization of a local
	energy. Furthermore, we have defined some general mesh transformations
	that enables the recovery of complex models from parts of simpler
	shapes. We present several modeling results extracted from structured
	range data or three dimensional images, including objects of complex
	topology and a complete model of a human body.},
  keywords = {conference, reconstruction},
  owner = {euHeart},
  postscript = {ftp://ftp-sop.inria.fr/epidaure/Publications/Delingette/cvpr94.ps.Z},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{DEL94.2,
  author = {H. Delingette},
  title = {Adaptive and Deformable models based on Simplex Meshes},
  booktitle = {IEEE Workshop of Non-Rigid and Articulated Objects},
  year = {1994},
  address = {Austin, Texas},
  month = {November},
  keywords = {workshop, reconstruction},
  owner = {euHeart},
  postscript = {ftp://ftp-sop.inria.fr/epidaure/Publications/Delingette/mnao.ps.gz},
  timestamp = {2009.02.24}
}

@ARTICLE{DEL92,
  author = {H. Delingette and M. H\'ebert and K. Ikeuchi},
  title = {Shape Representation and Image Segmentation Using Deformable Surfaces},
  journal = {Image and Vision Computing},
  year = {1992},
  volume = {10},
  pages = {132--144},
  number = {3},
  month = {April},
  keywords = {journal, reconstruction},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{DEL00,
  author = {Delingette, H. and Montagnat, J.},
  title = {New Algorithms for Controlling Active Contours Shape and Topology.},
  journal = {Lecture Notes in Computer Sciences},
  year = {2000},
  volume = {1843},
  pages = {381-395},
  abstract = {In recent years, the field of active-contour based image segmentation
	have seen the emergence of two competing approaches. The first and
	oldest approach represents active contours in an explicit (or parametric)
	manner corresponding to the Lagrangian formulation. The second approach
	represent active contours in an implicit manner corresponding to
	the Eulerian framework. After comparing these two approaches, we
	describe several new topological and physical constraints applied
	on parametric active contours in order to combine the advantages
	of these two contour representations. We introduce three key algorithms
	for independently controlling active contour parameterization, shape
	and topology. We compare our result to the level-set method and show
	similar results with a significant speed-up.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{DES06.1,
  author = {Mathieu Desbrun and Eva Kanso and Yiying Tong},
  title = {Discrete differential forms for computational modeling},
  booktitle = {SIGGRAPH Courses},
  year = {2006},
  pages = {39--54},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  doi = {http://doi.acm.org/10.1145/1185657.1185665},
  isbn = {1-59593-364-6},
  location = {Boston, Massachusetts},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{DES02,
  author = {Desbrun, Mathieu and Meyer, Mark and Alliez, Pierre},
  title = {Intrinsic parameterizations of surface meshes},
  booktitle = {23rd Annual Conference (EUROGRAPHICS 2002), Sep 2-6 2002},
  year = {2002},
  volume = {21},
  series = {Computer Graphics Forum},
  pages = {209-218},
  address = {Saarbrucken, Germany},
  publisher = {Blackwell Science Ltd},
  note = {TY - CONF U1 - 02407120892 L2 - http://dx.doi.org/10.1111/1467-8659.00580
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Surface meshes},
  abstract = {Parameterization of discrete surfaces is a fundamental and widely-used
	operation in graphics, required, for instance, for texture mapping
	or remeshing. As 3D data becomes more and more detailed, there is
	an increased need for fast and robust techniques to automatically
	compute least-distorted parameterizations of large meshes. In this
	paper, we present new theoretical and practical results on the parameterization
	of triangulated surface patches. Given a few desirable properties
	such as rotation and translation invariance, we show that the only
	admissible parameterizations form a two-dimensional set and each
	parameterization in this set can be computed using a simple, sparse,
	linear system. Since these parameterizations minimize the distortion
	of different intrinsic measures of the original mesh, we call them
	Intrinsic Parameterizations. In addition to this partial theoretical
	analysis, we propose robust, efficient and tunable tools to obtain
	least-distorted parameterizations automatically. In particular, we
	give details on a novel, fast technique to provide an optimal mapping
	without fixing the boundary positions, thus providing a unique Natural
	Intrinsic Parameterization. Other techniques based on this parameterization
	family, designed to ease the rapid design of parameterizations, are
	also proposed.},
  keywords = {Surface structure Linear systems Textures Set theory Computer graphics},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{DES99,
  author = {Mathieu Desbrun and Mark Meyer and Peter Schr\"oder and Alan H. Barr},
  title = {Implicit fairing of irregular meshes using diffusion and curvature
	flow},
  booktitle = {SIGGRAPH},
  year = {1999},
  pages = {317--324},
  address = {New York, NY, USA},
  publisher = {ACM Press/Addison-Wesley Publishing Co.},
  abstract = {In this paper, we develop methods to rapidly remove rough features
	from irregularly triangulated data intended to portray a smooth surface.
	The main task is to remove undesirable noise and uneven edges while
	retaining desirable geometric features. The problem arises mainly
	when creating high-fidelity computer graphics objects using imperfectly-measured
	data from the real world. 
	
	Our approach contains three novel features: an implicit integration
	method to achieve efficiency, stability, and large time-steps; a
	scale-dependent Laplacian operator to improve the diffusion process;
	and finally, a robust curvature flow operator that achieves a smoothing
	of the shape itself, distinct from any parameterization. Additional
	features of the algorithm include automatic exact volume preservation,
	and hard and soft constraints on the positions of the points in the
	mesh.
	
	We compare our method to previous operators and related algorithms,
	and prove that our curvature and Laplacian operators have several
	mathematically-desirable qualities that improve the appearance of
	the resulting surface. In consequence, the user can easily select
	the appropriate operator according to the desired type of fairing.
	Finally, we provide a series of examples to graphically and numerically
	demonstrate the quality of our results.},
  doi = {http://doi.acm.org/10.1145/311535.311576},
  isbn = {0-201-48560-5},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{DES01,
  author = {T. Deschamps and L. D. Cohen},
  title = {Fast extraction of minimal paths in 3D images and applications to
	virtual endoscopy.},
  journal = {Med Image Anal},
  year = {2001},
  volume = {5},
  pages = {281--299},
  number = {4},
  month = {Dec},
  abstract = {The aim of this article is to build trajectories for virtual endoscopy
	inside 3D medical images, using the most automatic way. Usually the
	construction of this trajectory is left to the clinician who must
	define some points on the path manually using three orthogonal views.
	But for a complex structure such as the colon, those views give little
	information on the shape of the object of interest. The path construction
	in 3D images becomes a very tedious task and precise a priori knowledge
	of the structure is needed to determine a suitable trajectory. We
	propose a more automatic path tracking method to overcome those drawbacks:
	we are able to build a path, given only one or two end points and
	the 3D image as inputs. This work is based on previous work by Cohen
	and Kimmel [Int. J. Comp. Vis. 24 (1) (1997) 57] for extracting paths
	in 2D images using Fast Marching algorithm.Our original contribution
	is twofold. On the first hand, we present a general technical contribution
	which extends minimal paths to 3D images and gives new improvements
	of the approach that are relevant in 2D as well as in 3D to extract
	linear structures in images. It includes techniques to make the path
	extraction scheme faster and easier, by reducing the user interaction.We
	also develop a new method to extract a centered path in tubular structures.
	Synthetic and real medical images are used to illustrate each contribution.On
	the other hand, we show that our method can be efficiently applied
	to the problem of finding a centered path in tubular anatomical structures
	with minimum interactivity, and that this path can be used for virtual
	endoscopy. Results are shown in various anatomical regions (colon,
	brain vessels, arteries) with different 3D imaging protocols (CT,
	MR).},
  institution = {Medical Imaging Systems Group, Philips Research France, PRF, 51 rue
	Carnot, B.P. 301, 92516 Cedex, Suresnes, France.},
  keywords = {Algorithms; Endoscopy, methods; Humans; Image Processing, Computer-Assisted,
	methods; Imaging, Three-Dimensional; User-Computer Interface},
  owner = {euHeart},
  pii = {S1361841501000469},
  pmid = {11731307},
  timestamp = {2008.10.23}
}

@ARTICLE{DES08,
  author = {Maxime Descoteaux and D. Louis Collins and Kaleem Siddiqi},
  title = {A geometric flow for segmenting vasculature in proton-density weighted
	MRI.},
  journal = {Med Image Anal},
  year = {2008},
  volume = {12},
  pages = {497--513},
  number = {4},
  month = {Aug},
  abstract = {Modern neurosurgery takes advantage of magnetic resonance images (MRI)
	of a patient's cerebral anatomy and vasculature for planning before
	surgery and guidance during the procedure. Dual echo acquisitions
	are often performed that yield proton-density (PD) and T2-weighted
	images to evaluate edema near a tumor or lesion. In this paper we
	develop a novel geometric flow for segmenting vasculature in PD images,
	which can also be applied to the easier cases of MR angiography data
	or Gadolinium enhanced MRI. Obtaining vasculature from PD data is
	of clinical interest since the acquisition of such images is widespread,
	the scanning process is non-invasive, and the availability of vessel
	segmentation methods could obviate the need for an additional angiographic
	or contrast-based sequence during preoperative imaging. The key idea
	is to first apply Frangi's vesselness measure [Frangi, A., Niessen,
	W., Vincken, K.L., Viergever, M.A., 1998. Multiscale vessel enhancement
	filtering. In: International Conference on Medical Image Computing
	and Computer Assisted Intervention, vol. 1496 of Lecture Notes in
	Computer Science, pp. 130-137] to find putative centerlines of tubular
	structures along with their estimated radii. This measure is then
	distributed to create a vector field which allows the flux maximizing
	flow algorithm of Vasilevskiy and Siddiqi [Vasilevskiy, A., Siddiqi,
	K., 2002. Flux maximizing geometric flows. IEEE Transactions on Pattern
	Analysis and Machine Intelligence 24 (12), 1565-1578] to be applied
	to recover vessel boundaries. We carry out a qualitative validation
	of the approach on PD, MR angiography and Gadolinium enhanced MRI
	volumes and suggest a new way to visualize the segmentations in 2D
	with masked projections. We validate the approach quantitatively
	on a single-subject data set consisting of PD, phase contrast (PC)
	angiography and time of flight (TOF) angiography volumes, with an
	expert segmented version of the TOF volume viewed as the ground truth.
	We then validate the approach quantitatively on 19 PD data sets from
	a new digital brain phantom, with semi-automatically obtained labels
	from the corresponding angiography volumes viewed as ground truth.
	A significant finding is that both for the single-subject and multi-subject
	studies, 90\% or more of the vasculature in the ground truth segmentation
	is recovered from the automatic segmentation of the other volumes.},
  doi = {10.1016/j.media.2008.02.003},
  institution = {Odyssée Project Team, INRIA, Sophia-Antipolis, France.},
  keywords = {Algorithms; Brain Neoplasms, diagnosis/surgery; Brain, anatomy /&/
	histology; Cerebrovascular Circulation; Humans; Magnetic Resonance
	Angiography, methods; Magnetic Resonance Imaging, methods; Phantoms,
	Imaging; Protons},
  language = {eng},
  medline-pst = {ppublish},
  owner = {euHeart},
  pii = {S1361-8415(08)00023-6},
  pmid = {18375175},
  timestamp = {2009.10.14},
  url = {http://dx.doi.org/10.1016/j.media.2008.02.003}
}

@ARTICLE{DOD92,
  author = {J. T. Dodge and B. G. Brown and E. L. Bolson and H. T. Dodge},
  title = {Lumen diameter of normal human coronary arteries. Influence of age,
	sex, anatomic variation, and left ventricular hypertrophy or dilation.},
  journal = {Circulation},
  year = {1992},
  volume = {86},
  pages = {232--246},
  number = {1},
  month = {Jul},
  abstract = {BACKGROUND. Precise knowledge of the expected "normal" lumen diameter
	at a given coronary anatomic location is a first step toward developing
	a quantitative estimate of coronary disease severity that could be
	more useful than the traditional "percent stenosis." METHODS AND
	RESULTS. Eighty-three arteriograms were carefully selected from among
	9,160 consecutive studies for their smooth lumen borders indicating
	freedom from atherosclerotic disease. Of these, 60 men and 10 women
	had no abnormalities of cardiac function, seven men had idiopathic
	dilated cardiomyopathy, and six men had left ventricular hypertrophy
	associated with significant aortic stenosis. Lumen diameter was measured
	at 96 points in 32 defined coronary segments or major branches. Measurements
	were scaled to the catheter, corrected for imaging distortion, and
	had a mean repeat measurement error of 0.12 mm. When sex, anatomic
	dominance, and branch length were accounted for, normal lumen diameter
	at each of the standard anatomic points could usually be specified
	with a population variance of +/- 0.6 mm or less (SD) and coefficient
	of variation of less than 0.25 (SD/mean). For example, the left main
	artery measured 4.5 +/- 0.5 mm, the proximal left anterior descending
	coronary artery (LAD) 3.7 +/- 0.4 mm, and the distal LAD 1.9 +/-
	0.4 mm. For the LAD, lumen diameter was not affected by anatomic
	dominance (right versus left), but for the right coronary artery,
	proximal diameter varied between 3.9 +/- 0.6 and 2.8 +/- 0.5 mm (p
	less than 0.01) and for the left circumflex, between 3.4 +/- 0.5
	and 4.2 +/- 0.6 mm (p less than 0.01). Women had smaller epicardial
	arterial diameter than men (-9\%; p less than 0.001), even after
	normalization for body surface area (p less than 0.01). Branch artery
	caliber was unaffected by the anatomic dominance but increased with
	branch length, expressed as a fraction of the origin-to-apex distance
	(p less than 0.001). Lumen diameter was not affected by age or by
	vessel tortuosity but was significantly increased among men with
	left ventricular hypertrophy (+ 17\%; p less than 0.001) or dilated
	cardiomyopathy (+ 12\%; p less than 0.001). CONCLUSIONS. This is
	a reference normal data set against which to compare lumen dimensions
	in various pathological states. It should be of particular value
	in the investigation of diffuse atherosclerotic disease.},
  institution = {Department of Medicine, University of Washington, Seattle.},
  keywords = {Adult; Aging, physiology; Cardiomegaly, radiography; Cardiomyopathy,
	Dilated, radiography; Coronary Angiography; Female; Heart Diseases,
	radiography; Humans; Image Processing, Computer-Assisted; Male; Middle
	Aged; Models, Cardiovascular; Reference Values; Reproducibility of
	Results; Sex Characteristics},
  owner = {euHeart},
  pmid = {1535570},
  timestamp = {2008.09.12}
}

@ARTICLE{DOD88,
  author = {J. T. Dodge and B. G. Brown and E. L. Bolson and H. T. Dodge},
  title = {Intrathoracic spatial location of specified coronary segments on
	the normal human heart. Applications in quantitative arteriography,
	assessment of regional risk and contraction, and anatomic display.},
  journal = {Circulation},
  year = {1988},
  volume = {78},
  pages = {1167--1180},
  number = {5 Pt 1},
  month = {Nov},
  abstract = {The clinically important coronary segmental anatomy has been described
	in a format useful for quantitative analysis and standardized display.
	We have determined the intrathoracic location and course of each
	of the 23 coronary artery segments and branches commonly used for
	clinical description of disease. Measurements were averaged from
	perpendicular angiographic view-pairs in 37 patients with normal-sized
	hearts. Each segment or branch is described by several points along
	its course; each point is specified in polar coordinates as the radial
	distance from the principal coronary ostium and by angles about the
	patient, corresponding to those describing rotation in c-arm radiographic
	systems. This computer-assisted measurement method is accurate to
	within +/- 0.2 cm (SD) and +/- 2 degrees in phantom studies. Coronary
	segment location among a group of normal-sized hearts can be specified
	to within +/- 1.0 cm (SD). For example, the left anterior descending
	coronary artery segment at the apex of the heart is 12.2 +/- 1.0
	cm from the left coronary ostium, 32 +/- 4 degrees to the left of
	the anterioposterior axis, and at 46 +/- 7 degrees of caudal angulation.
	There are several clinically important applications of this new knowledge.
	First, this anatomic format provides the basis for estimating regional
	myocardial contraction and the relative size of the myocardial region
	at risk from a given arterial occlusion. Second, precise knowledge
	of "normal" segment location greatly simplifies the computation of
	dimensional correction factors for quantitative arteriography. Third,
	viewing angles most appropriate for videodensitometric assessment
	of lesion lumen area may be computed from these data. The theoretical
	basis and numerical values needed for most of the above estimates
	are provided. Finally, a computer program has been written to generate
	a three-dimensional tree-branch vascular model from these anatomic
	locations. This easily used interactive program aids in teaching
	coronary angiographic anatomy and, of importance, permits selection
	of viewing angles that "best" visualize the traditionally difficult
	parts of the coronary tree.},
  institution = {Department of Medicine, University of Washington, Seattle.},
  keywords = {Adult; Aged; Body Surface Area; Coronary Vessels, anatomy /&/ histology;
	Female; Humans; Image Processing, Computer-Assisted; Male; Middle
	Aged; Reference Values; Sex Characteristics},
  owner = {euHeart},
  pmid = {3180376},
  timestamp = {2008.09.12}
}

@INPROCEEDINGS{DON06,
  author = {Shen Dong and Peer-Timo Bremer and Michael Garland and Valerio Pascucci
	and John C. Hart},
  title = {Spectral surface quadrangulation},
  booktitle = {SIGGRAPH},
  year = {2006},
  pages = {1057--1066},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  doi = {http://doi.acm.org/10.1145/1179352.1141993},
  isbn = {1-59593-364-6},
  location = {Boston, Massachusetts},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{DON05,
  author = {Dong, S. and Kircher, S. and Garland, M.},
  title = {Harmonic functions for quadrilateral remeshing of arbitrary manifolds},
  journal = {Computer Aided Geometric Design},
  year = {2005},
  volume = {22},
  pages = {392-423},
  number = {5},
  note = {05289204865 Compilation and indexing terms, Copyright 2005 Elsevier
	Engineering Information, Inc. 0167-8396 Quad-dominant remeshing Gradient
	flow tracing Harmonic fields Harmonic 1-forms},
  abstract = {In this paper, we propose a new quadrilateral remeshing method for
	manifolds of arbitrary genus that is at once general, flexible, and
	efficient. Our technique is based on the use of smooth harmonic scalar
	fields defined over the mesh. Given such a field, we compute its
	gradient field and a second vector field that is everywhere orthogonal
	to the gradient. We then trace integral lines through these vector
	fields to sample the mesh. The two nets of integral lines together
	are used to form the polygons of the output mesh. Curvature-sensitive
	spacing of the lines provides for anisotropic meshes that adapt to
	the local shape. Our scalar field construction allows users to exercise
	extensive control over the structure of the final mesh. The entire
	process is performed without computing an explicit parameterization
	of the surface, and is thus applicable to manifolds of any genus
	without the need for cutting the surface into patches. &copy; 2005
	Elsevier B.V. All rights reserved.},
  keywords = {Functions Computer aided design Vectors Anisotropy Surfaces Parameter
	estimation Computer graphics Automation Harmonic analysis},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{DRE98,
  author = {Dreger, Andreas and Gross, Markus Hans and Schlegel, Joachim},
  title = {Multiresolution Triangular B-Spline Surfaces},
  booktitle = {IEEE Computer Graphics International},
  year = {1998},
  pages = {166-177},
  address = {Hannover},
  abstract = {We present multiresolution B-spline surfaces of arbitrary order defined
	over triangular domains. Unlike existing methods, the basic idea
	of our approach is to construct the triangular basis functions from
	their tensor product relatives in the spirit of box splines by projecting
	them into the barycentric plane. The scheme works for splines of
	any order where the fundamental building blocks of the surface are
	hierarchies of triangular B-spline scaling functions and wavelets
	spanning the complement spaces between levels of different resolution.
	Although our decomposition and reconstruction schemes operate in
	principle on a tensor product grid in 3D, the sparsity of the arrangement
	enables us to design efficient linear time algorithms. The resulting
	basis functions are used to approximate triangular surfaces and provide
	many useful properties, such as multiresolution editing, local level
	of detail, continuity control, surface compression and much more.
	The performance of our approach is illustrated by various examples
	including parametric and nonparametric surface editing and compression.},
  keywords = {Triangular B-spline wavelets, box splines, multiresolution editing,
	hierarchical surface representation, surface compression, decomposition,
	reconstruction.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{DU08,
  author = {Du, Xian and Velut, J. and Bolbos, R. and Beuf, O. and Odet, C. and
	Benoit-Cattin, H. },
  title = {3-D knee cartilage segmentation using a smoothing B-Spline active
	surface},
  booktitle = {Proc. 15th IEEE International Conference on Image Processing ICIP
	2008},
  year = {2008},
  pages = {2924--2927},
  abstract = {We present an adaptive solution for guinea pig knee cartilage segmentation
	using a 3-D smoothing B-Spline active surface. An adaptive parametric
	combination of edge-based forces and balloon force solves the problem
	of capture range of external forces. The comparison between the results
	of the experiments using this method and previous 3-D validated snake
	segmentation shows that the accuracy and robustness are improved.},
  doi = {10.1109/ICIP.2008.4712407},
  issn = {1522-4880},
  keywords = {image segmentation, medical image processing, splines (mathematics),
	3D knee cartilage segmentation, adaptive parametric combination,
	balloon force, biomedical image processing, edge-based forces, smoothing
	B-spline active surface, snake segmentation, MRI, biomedical image
	processing, image segmentation, spline functions},
  owner = {euHeart},
  timestamp = {2009.04.17}
}

@ARTICLE{ECA08,
  author = {Ecabert, O. and Peters, J. and Schramm, H. and Lorenz, C. and von
	Berg, J. and Walker, M. J. and Vembar, M. and Olszewski, M. E. and
	Subramanyan, K. and Lavi, G. and Weese, J. },
  title = {Automatic Model-Based Segmentation of the Heart in CT Images},
  journal = IEEE_J_MI,
  year = {2008},
  volume = {27},
  pages = {1189--1201},
  number = {9},
  abstract = {Automatic image processing methods are a prerequisite to efficiently
	analyze the large amount of image data produced by computed tomography
	(CT) scanners during cardiac exams. This paper introduces a model-based
	approach for the fully automatic segmentation of the whole heart
	(four chambers, myocardium, and great vessels) from 3-D CT images.
	Model adaptation is done by progressively increasing the degrees-of-freedom
	of the allowed deformations. This improves convergence as well as
	segmentation accuracy. The heart is first localized in the image
	using a 3-D implementation of the generalized Hough transform. Pose
	misalignment is corrected by matching the model to the image making
	use of a global similarity transformation. The complex initialization
	of the multicompartment mesh is then addressed by assigning an affine
	transformation to each anatomical region of the model. Finally, a
	deformable adaptation is performed to accurately match the boundaries
	of the patient's anatomy. A mean surface-to-surface error of 0.82
	mm was measured in a leave-one-out quantitative validation carried
	out on 28 images. Moreover, the piecewise affine transformation introduced
	for mesh initialization and adaptation shows better interphase and
	interpatient shape variability characterization than commonly used
	principal component analysis.},
  doi = {10.1109/TMI.2008.918330},
  issn = {0278-0062},
  keywords = {Active shape models, Hough transform, active shape models, computed
	tomography, deformable models, model-based segmentation, shape modeling,
	three-dimensional (3-D) cardiac segmentation, three-dimensional cardiac
	segmentation},
  owner = {euHeart},
  timestamp = {2008.09.10}
}

@INPROCEEDINGS{ECK95,
  author = {Eck, Matthias and DeRose, Tony and Duchamp, Tom and Hoppe, Hugues
	and Lounsbery, Michael and Stuetzle, Werner},
  title = {Multiresolution analysis of arbitrary meshes},
  booktitle = {Proceedings of the 22nd Annual ACM Conference on Computer Graphics
	and Interactive Techniques, Aug 9-11 1995},
  year = {1995},
  series = {Proceedings of the ACM SIGGRAPH Conference on Computer Graphics},
  pages = {173-180},
  address = {Los Angeles, CA, USA},
  publisher = {ACM, New York, NY, USA},
  note = {TY - CONF U1 - 95102886237 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc. U2 - Multiresolution
	analysis Arbitrary meshes Subdivision connectivity Wavelet coefficients},
  abstract = {In computer graphics and geometric modeling, shapes are often represented
	by triangular meshes. With the advent of laser scanning systems,
	meshes of extreme complexity are rapidly becoming commonplace. Such
	meshes are notoriously expensive to store, transmit, render, and
	are awkward to edit. Multiresolution analysis offers a simple, unified,
	and theoretically sound approach to dealing with these problems.
	Lounsbery et al. have recently developed a technique for creating
	multiresolution representations for a restricted class of meshes
	with subdivision connectivity. Unfortunately, meshes encountered
	in practice typically do not meet this requirement. In this paper
	we present a method for overcoming the subdivision connectivity restriction,
	meaning that completely arbitrary meshes can now be converted to
	multiresolution form. The method is based on the approximation of
	an arbitrary initial mesh M by a mesh M^J that has subdivision connectivity
	and is guaranteed to be within a specified tolerance. The key ingredient
	of our algorithm is the construction of a parametrization of M over
	a simple domain. We expect this parametrization to be of use in other
	contexts, such as texture mapping or the approximation of complex
	meshes by NURBS patches.},
  keywords = {Computational geometry Computer simulation Approximation theory Algorithms
	Wavelet transforms Computer graphics},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{ECK96,
  author = {Eck, Matthias and Hoppe, Hugues},
  title = {Automatic reconstruction of B-spline surfaces of arbitrary topological
	type},
  booktitle = {Proceedings of the 1996 Computer Graphics Conference, SIGGRAPH, Aug
	4-9 1996},
  year = {1996},
  series = {Proceedings of the ACM SIGGRAPH Conference on Computer Graphics},
  pages = {325-334},
  address = {New Orleans, LA, USA},
  note = {TY - CONF U1 - 97023519573 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc. U2 - B spline surfaces},
  abstract = {Creating freeform surfaces is a challenging task even with advanced
	geometric modeling systems. Laser range scanners offer a promising
	alternative for model acquisition - the 3D scanning of existing objects
	or clay maquettes. The problem of converting the dense point sets
	produced by laser scanners into useful geometric models is referred
	to as surface reconstruction. In this paper, we present a procedure
	for reconstructing a tensor product B-spline surface from a set of
	scanned 3D points. Unlike previous work which considers primarily
	the problem of fitting a single B-spline patch, our goal is to directly
	reconstruct a surface of arbitrary topological type. We must therefore
	define the surface as a network of B-spline patches. A key ingredient
	in our solution is a scheme for automatically constructing both a
	network of patches and a parametrization of the data points over
	these patches. In addition, we define the B-spline surface using
	a surface spline construction, and demonstrate that such an approach
	leads to an efficient procedure for fitting the surface while maintaining
	tangent plane continuity. We explore adaptive refinement of the patch
	network in order to satisfy user-specified error tolerances, and
	demonstrate our method on both synthetic and real data.},
  keywords = {Three dimensional computer graphics Computational geometry Curve fitting
	Scanning Computer aided design Image analysis Lasers Computer simulation
	Adaptive filtering Image reconstruction},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{EDE01,
  author = {Herbert Edelsbrunner and John Harer and Afra Zomorodian},
  title = {Hierarchical morse complexes for piecewise linear 2-manifolds},
  booktitle = {Symposium on Computational Geometry},
  year = {2001},
  pages = {70--79},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  doi = {http://doi.acm.org/10.1145/378583.378626},
  isbn = {1-58113-357-X},
  location = {Medford, Massachusetts, United States},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{EIL96.1,
  author = {Eilers, P. H. C. and Marx, B. D.},
  title = {Flexible Smoothing with B-splines and Penalties},
  journal = {Statistical Science},
  year = {1996},
  volume = {11},
  pages = {89},
  number = {2},
  note = {TY - JOUR U1 - 97053655334 L2 - http://dx.doi.org/10.1214/ss/1038425655
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc.},
  abstract = {B-splines are attractive for nonparametric modelling, but choosing
	the optimal number and positions of knots is a complex task. Equidistant
	knots can be used, but their small and discrete number allows only
	limited control over smoothness and fit. We propose to use a relatively
	large number of knots and a difference penalty on coefficients of
	adjacent B-splines. We show connections to the familiar spline penalty
	on the integral of the squared second derivative. A short overview
	of B-splines, of their construction and of penalized likelihood is
	presented. We discuss properties of penalized B-splines and propose
	various criteria for the choice of an optimal penalty parameter.
	Nonparametric logistic regression, density estimation and scatterplot
	smoothing are used as examples. Some details of the computations
	are presented.},
  keywords = {Generalized linear models, smoothing, nonparametric models, splines,
	density estimation},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{EIL96.2,
  author = {Eilers, P. H. C. and Marx, B. D.},
  title = {Flexible Smoothing with B-splines and Penalties: Rejoinder},
  journal = {Statistical Science},
  year = {1996},
  volume = {11},
  pages = {115},
  number = {2},
  note = {TY - JOUR Compilation and indexing terms, Copyright 2005 Elsevier
	Engineering Information, Inc. U1 - 97053655330},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{ETI02,
  author = {Alex Etienne and René M Botnar and Arianne M C Van Muiswinkel and
	Peter Boesiger and Warren J Manning and Matthias Stuber},
  title = {"Soap-Bubble" visualization and quantitative analysis of 3D coronary
	magnetic resonance angiograms.},
  journal = {Magn Reson Med},
  year = {2002},
  volume = {48},
  pages = {658--666},
  number = {4},
  month = {Oct},
  abstract = {In order to compare coronary magnetic resonance angiography (MRA)
	data obtained with different scanning methodologies, adequate visualization
	and presentation of the coronary MRA data need to be ensured. Furthermore,
	an objective quantitative comparison between images acquired with
	different scanning methods is desirable. To address this need, a
	software tool ("Soap-Bubble") that facilitates visualization and
	quantitative comparison of 3D volume targeted coronary MRA data was
	developed. In the present implementation, the user interactively
	specifies a curved subvolume (enclosed in the 3D coronary MRA data
	set) that closely encompasses the coronary arterial segments. With
	a 3D Delaunay triangulation and a parallel projection, this enables
	the simultaneous display of multiple coronary segments in one 2D
	representation. For objective quantitative analysis, frequently explored
	quantitative parameters such as signal-to-noise ratio (SNR); contrast-to-noise
	ratio (CNR); and vessel length, sharpness, and diameter can be assessed.
	The present tool supports visualization and objective, quantitative
	comparisons of coronary MRA data obtained with different scanning
	methods. The first results obtained in healthy adults and in patients
	with coronary artery disease are presented.},
  doi = {10.1002/mrm.10253},
  institution = {Department of Medicine (Cardiovascular Division), Beth Israel Deaconess
	Medical Center and Harvard Medical School, Boston, Massachusetts
	02215, USA.},
  keywords = {Adult; Arteries, anatomy /&/ histology; Contrast Media; Coronary Vessels,
	anatomy /&/ histology; Gadolinium, diagnostic use; Humans; Image
	Enhancement; Image Processing, Computer-Assisted, methods; Imaging,
	Three-Dimensional; Magnetic Resonance Angiography, methods; Organometallic
	Compounds, diagnostic use; Software},
  owner = {euHeart},
  pmid = {12353283},
  timestamp = {2009.07.02},
  url = {http://dx.doi.org/10.1002/mrm.10253}
}

@INPROCEEDINGS{EVE01,
  author = {Nicolas Eveno and Patrice Delmas and Pierre-Yves Coulon},
  title = {Vers l'Extraction Automatique des L\`evres d'un Visage Parlant},
  booktitle = {GRETSI},
  year = {2001},
  address = {Toulouse},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{FEN08,
  author = {Feng, Jian and Wang, Xueyan and Luo, Shuqian},
  title = {A Worm Model Based on Artificial Life for Automatic Segmentation
	of Medical Images},
  journal = {Medical Imaging and Informatics},
  year = {2008},
  pages = {35--43},
  abstract = {An intelligent deformable model called worm model is constructed.
	The worm has a central nervous system, vision, perception and motor
	systems. It is able to memorize, recognize objects and control the
	motion of its body. The new model overcomes the defects of existing
	methods since it is able to process the segmentation of the image
	intelligently using more information available rather than using
	pixels and gradients only. The experimental results of segmentation
	of the corpus callosum from MRI brain images show that the proposed
	worm model is able to segment medical images automatically and accurately.
	For those images that are more complex or with fragmentary boundaries,
	the predominance of the worm model is especially clear.},
  owner = {euHeart},
  timestamp = {2009.03.03},
  url = {http://dx.doi.org/10.1007/978-3-540-79490-5_6}
}

@ARTICLE{FER06,
  author = {Maros Ferencik and Jennifer B Lisauskas and Ricardo C Cury and Udo
	Hoffmann and Suhny Abbara and Stephan Achenbach and W. Clem Karl
	and Thomas J Brady and Raymond C Chan},
  title = {Improved vessel morphology measurements in contrast-enhanced multi-detector
	computed tomography coronary angiography with non-linear post-processing.},
  journal = {Eur J Radiol},
  year = {2006},
  volume = {57},
  pages = {380--383},
  number = {3},
  month = {Mar},
  abstract = {Multi-detector computed tomography (MDCT) permits detection of coronary
	plaque. However, noise and blurring impair accuracy and precision
	of plaque measurements. The aim of the study was to evaluate MDCT
	post-processing based on non-linear image deblurring and edge-preserving
	noise suppression for measurements of plaque size. Contrast-enhanced
	MDCT coronary angiography was performed in four subjects (mean age
	55 +/- 5 years, mean heart rate 54 +/- 5 bpm) using a 16-slice scanner
	(Siemens Sensation 16, collimation 16 x 0.75 mm, gantry rotation
	420 ms, tube voltage 120 kV, tube current 550 mAs, 80 mL of contrast).
	Intravascular ultrasound (IVUS; 40 MHz probe) was performed in one
	vessel in each patient and served as a reference standard. MDCT vessel
	cross-sectional images (1 mm thickness) were created perpendicular
	to centerline and aligned with corresponding IVUS images. MDCT images
	were processed using a deblurring and edge-preserving noise suppression
	algorithm. Then, three independent blinded observers segmented lumen
	and outer vessel boundaries in each modality to obtain vessel cross-sectional
	area and wall area in the unprocessed MDCT cross-sections, post-processed
	MDCT cross-sections and corresponding IVUS. The wall area measurement
	difference for unprocessed and post-processed MDCT images relative
	to IVUS was 0.4 +/- 3.8 mm2 and -0.2 +/- 2.2 mm2 (p < 0.05), respectively.
	Similarly, Bland-Altman analysis of vessel cross-sectional area from
	unprocessed and post-processed MDCT images relative to IVUS showed
	a measurement difference of 1.0 +/- 4.4 and 0.6 +/- 4.8 mm2, respectively.
	In conclusion, MDCT permitted accurate in vivo measurement of wall
	area and vessel cross-sectional area as compared to IVUS. Post-processing
	to reduce blurring and noise reduced variability of wall area measurements
	and reduced measurement bias for both wall area and vessel cross-sectional
	area.},
  doi = {10.1016/j.ejrad.2005.12.024},
  institution = {Department of Radiology, Massachusetts General Hospital and Harvard
	Medical School, 165 Cambridge Street, Suite 400, Boston, MA 02114,
	USA. maros.ferencik@hms.harvard.edu},
  keywords = {Algorithms; Artifacts; Contrast Media; Coronary Angiography; Coronary
	Artery Disease, radiography/ultrasonography; Coronary Vessels, pathology/ultrasonography;
	Female; Humans; Image Processing, Computer-Assisted; Male; Middle
	Aged; Nonlinear Dynamics; Tomography, X-Ray Computed, methods},
  owner = {euHeart},
  pii = {S0720-048X(05)00447-X},
  pmid = {16442768},
  timestamp = {2009.05.20},
  url = {http://dx.doi.org/10.1016/j.ejrad.2005.12.024}
}

@ARTICLE{FEY02,
  author = {P. J. de Feyter and K. Nieman},
  title = {New coronary imaging techniques: what to expect?},
  journal = {Heart},
  year = {2002},
  volume = {87},
  pages = {195--197},
  number = {3},
  month = {Mar},
  keywords = {Coronary Disease, diagnosis; Diagnostic Imaging, methods/standards;
	Humans; Magnetic Resonance Angiography, methods/standards; Sensitivity
	and Specificity; Tomography, X-Ray Computed, methods/standards},
  language = {eng},
  medline-pst = {ppublish},
  owner = {euHeart},
  pmid = {11847148},
  timestamp = {2010.01.06}
}

@ARTICLE{FIS01,
  author = {Fischl, Bruce and Liu, Arthur and Dale, Anders M.},
  title = {Automated Manifold Surgery: Constructing Geometrically Accurate and
	Topologically Correct Models of the Human Cerebral Cortex.},
  journal = {Transactions on Medical Imaging},
  year = {2001},
  volume = {20},
  pages = {70-80},
  number = {1},
  abstract = {Highly accurate surface models of the cerebral cortex are becoming
	increasingly important as tools in the investigation of the functional
	organization of the human brain. The construction of such models
	is difficult using current neuroimaging technology due to the high
	degree of cortical folding. Even single voxel misclassifications
	can result in erroneous connections being created between adjacent
	banks of a sulcus, resulting in a topologically inaccurate model.
	These topological defects cause the cortical model to no longer be
	homeomorphic to a sheet, preventing the accurate inflation, flattening,
	or spherical morphing of the reconstructed cortex. Surface deformation
	techniques can guarantee the topological correctness of a model,
	but are time-consuming and may result in geometrically inaccurate
	models. In order to address this need we have developed a technique
	for taking a model of the cortex, detecting and fixing the topological
	defects while leaving that majority of the model intact, resulting
	in a surface that is both geometrically accurate and topologically
	correct.},
  keywords = {Human cerebral cortex ; topology ; segmentation.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{FIS07,
  author = {Matthew Fisher and Peter Schr\"{o}der and Mathieu Desbrun and Hugues
	Hoppe},
  title = {Design of tangent vector fields},
  booktitle = {SIGGRAPH '07: ACM SIGGRAPH 2007 papers},
  year = {2007},
  pages = {56},
  address = {New York, NY, USA},
  publisher = {ACM},
  abstract = {Tangent vector fields are an essential ingredient in controlling surface
	appearance for applications ranging from anisotropic shading to texture
	synthesis and non-photorealistic rendering. To achieve a desired
	effect one is typically interested in smoothly varying fields that
	satisfy a sparse set of user-provided constraints. Using tools from
	Discrete Exterior Calculus, we present a simple and efficient algorithm
	for designing such fields over arbitrary triangle meshes. By representing
	the field as scalars over mesh edges (i.e., discrete 1-forms), we
	obtain an intrinsic, coordinate-free formulation in which field smoothness
	is enforced through discrete Laplace operators. Unlike previous methods,
	such a formulation leads to a linear system whose sparsity permits
	efficient pre-factorization. Constraints are incorporated through
	weighted least squares and can be updated rapidly enough to enable
	interactive design, as we demonstrate in the context of anisotropic
	texture synthesis.},
  doi = {http://doi.acm.org/10.1145/1275808.1276447},
  location = {San Diego, California},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{FLE07,
  author = {Julien Fleureau and Mireille Garreau and Dominique Boulmier and Alfredo
	Hernández},
  title = {3D multi-object segmentation of cardiac MSCT imaging by using a multi-agent
	approach.},
  booktitle = {Conf Proc IEEE Eng Med Biol Soc},
  year = {2007},
  volume = {2007},
  pages = {6004--6007},
  abstract = {We propose a new technique for general purpose, semi-interactive and
	multi-object segmentation in N-dimensional images, applied to the
	extraction of cardiac structures in MultiSlice Computed Tomography
	(MSCT) imaging. The proposed approach makes use of a multi-agent
	scheme combined with a supervised classification methodology allowing
	the introduction of a priori information and presenting fast computing
	times. The multi-agent system is organised around a communicating
	agent which manages a population of situated agents which segment
	the image through cooperative and competitive interactions. The proposed
	technique has been tested on several patient data sets. Some typical
	results are finally presented and discussed.},
  doi = {10.1109/IEMBS.2007.4353716},
  institution = {INSERM, U642, Rennes, Université de Rennes 1, LTSI, Rennes, F-35000,
	France. julien.fleureau@univ-rennes1.fr},
  keywords = {Algorithms; Expert Systems; Heart, radiography; Humans; Imaging, Three-Dimensional,
	methods; Pattern Recognition, Automated, methods; Radiographic Image
	Enhancement, methods; Radiographic Image Interpretation, Computer-Assisted,
	methods; Reproducibility of Results; Sensitivity and Specificity;
	Tomography, X-Ray Computed, instrumentation/methods},
  owner = {euHeart},
  pmid = {18003382},
  timestamp = {2008.09.09},
  url = {http://dx.doi.org/10.1109/IEMBS.2007.4353716}
}

@INPROCEEDINGS{FLE06,
  author = {Fleureau, J. and Garreau, M. and Hernandez, A. I. and Simon, A. and
	Boulmier, D. },
  title = {Multi-object and N-D segmentation of cardiac MSCT data using SVM
	classifiers and a connectivity algorithm},
  booktitle = {Proc. Computers in Cardiology},
  year = {2006},
  pages = {817--820},
  abstract = {In this paper we present a new technique for general purpose, semi-interactive
	and multi-object segmentation in N-dimensional images. This method
	associates supervised classification methodologies with a region
	growing algorithm coupled with a connectivity approach. These concepts
	are combined in a competitive context implemented via a distributed
	real-time technology which allows multi-object detection. This approach
	is original by its atypical multi-object extraction, a rapidity of
	execution and the facility to introduce a priori information by the
	selection of a limited number of seed points inside the objects of
	interest. We apply this new method for the segmentation of cardiac
	structures observed in multislice computed tomography (MSCT) imaging.
	First results obtained on real 3D data reveals the good behaviour
	of the method, considering segmentation accuracy while minimizing
	user interaction and computational load.},
  keywords = {cardiovascular system, computerised tomography, diagnostic radiography,
	feature extraction, image classification, image segmentation, medical
	image processing, support vector machines, N-dimensional image segmentation,
	SVM classifiers, cardiac MSCT imaging, connectivity algorithm, distributed
	real-time technology, multiobject extraction, multislice computed
	tomography, region growing algorithm, semiinteractive multiobject
	segmentation, supervised classification, user interaction},
  owner = {euHeart},
  timestamp = {2008.09.10}
}

@INPROCEEDINGS{FLI94,
  author = {Flickner, Myron and Sawhney, Harpreet and Pryor, Duaine and Lotspiech,
	Jeff},
  title = {Intelligent interactive image outlining using spline snakes},
  booktitle = {Conference on Signals, Systems \& Computers},
  year = {1994},
  volume = {1},
  pages = {731-735},
  address = {Pacific Grove, CA, USA},
  publisher = {IEEE, Los Alamitos, CA, USA},
  note = {TY - CONF U1 - 95062736702 L2 - http://dx.doi.org/10.1109/ACSSC.1994.471548
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Intelligent interactive image outlining Spline
	snakes Image manipulation Parameterization Spline control points
	B spline curve},
  abstract = {Outlining 2D shapes and image regions is often required in image manipulation
	and analysis. Most polygon and curve drawing tools in graphics do
	not use the knowledge of image intensities and its derivatives in
	guiding the outlining process, and hence are quite tedious and error
	prone. This paper presents a fast interactive drawing tool based
	on a parametric spline formulation of snakes that exploits the underlying
	image intensities. Specifically, interactively specified contours
	are naturally attracted to strong image edges in the vicinity because
	spline snakes are designed to migrate to significant ridges in the
	gradient magnitude image. Furthermore, allowable stretching and bending
	can be controlled by minimizing explicitly the bending and stretching
	energies of the curve. Parameterization of spline snakes in terms
	of a limited number of control points yields a fast minimization
	solution thus enabling interactive rubber banding of the curve on
	the screen. This feedback allows users to quickly guide the snake
	on the desired path.},
  keywords = {Mathematical techniques Image analysis Drawing (graphics) Feedback
	Parameter estimation Mathematical models Statistical methods Matrix
	algebra Image processing},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{FLO97,
  author = {Floater, Michael S.},
  title = {Parametrization and Smooth Approximation of Surface Triangulations},
  journal = {Computer Aided Geometric Design.},
  year = {1997},
  volume = {14},
  pages = {231-250},
  number = {3},
  note = {Elsevier},
  abstract = {A method based on graph theory is investigated for creating global
	parametrizations for surface triangulations for the purpose of smooth
	surface fitting. The parametrizations, which are planar triangulations,
	are the solutions of linear systems based on convex combinations.
	A particular parametrization, called shape-preserving, is found to
	lead to visually smooth surface approximations.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{FLO04,
  author = {Floater, Michael S. and Hormann, Kai},
  title = {Surface Parameterization : a Tutorial and Survey},
  journal = {Advances in Multiresolution for Geometric Modelling},
  year = {2004},
  pages = {157-186},
  abstract = {This paper provides a tutorial and survey of methods for parameterizing
	surfaces with a view to applications in geometric modelling and computer
	graphics. We gather various concepts from differential geometry which
	are relevant to surface mapping and use them to understand the strengths
	and weaknesses of the many methods for parameterizing piecewise linear
	surfaces and their relationship to one another.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{Florin2005,
  author = {Charles Florin and Nikos Paragios and Jim Williams},
  title = {Particle filters, a quasi-Monte-Carlo-solution for segmentation of
	coronaries.},
  journal = {Med Image Comput Comput Assist Interv},
  year = {2005},
  volume = {8},
  pages = {246--253},
  number = {Pt 1},
  abstract = {In this paper we propose a Particle Filter-based approach for the
	segmentation of coronary arteries. To this end, successive planes
	of the vessel are modeled as unknown states of a sequential process.
	Such states consist of the orientation, position, shape model and
	appearance (in statistical terms) of the vessel that are recovered
	in an incremental fashion, using a sequential Bayesian filter (Particle
	Filter). In order to account for bifurcations and branchings, we
	consider a Monte Carlo sampling rule that propagates in parallel
	multiple hypotheses. Promising results on the segmentation of coronary
	arteries demonstrate the potential of the proposed approach.},
  institution = { Visualization Department, Siemens Corporate Research, Princeton,
	NJ, USA.},
  keywords = {Algorithms; Artificial Intelligence; Computer Simulation; Coronary
	Angiography, methods; Coronary Artery Disease, radiography; Coronary
	Vessels, pathology; Humans; Imaging, Three-Dimensional, methods;
	Magnetic Resonance Angiography, methods; Models, Biological; Models,
	Statistical; Monte Carlo Method; Particle Size; Pattern Recognition,
	Automated, methods; Radiographic Image Enhancement, methods; Radiographic
	Image Interpretation, Computer-Assisted, methods; Reproducibility
	of Results; Sensitivity and Specificity},
  language = {eng},
  medline-pst = {ppublish},
  owner = {euHeart},
  pmid = {16685852},
  timestamp = {2010.02.12}
}

@ARTICLE{FON93,
  author = {Fong, Philip and Seidel, Hans-Peter},
  title = {An implementation of triangular B-spline surfaces over arbitrary
	triangulations},
  journal = {Computer Aided Geometric Design.},
  year = {1993},
  volume = {10},
  pages = {267 - 275},
  number = {3-4},
  keywords = {Blossoming, B-patch, B-spline surfaces, blending functions, control
	points, simplex splines, ploar forms.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{FOU88,
  author = {Fournier, Alain and Fiume, Eugene},
  title = {Constant-time filtering with space-variant kernels},
  journal = {Computer Graphics (ACM) SIGGRAPH '88 Conference Proceedings, Aug
	1-5 1988},
  year = {1988},
  volume = {22},
  pages = {229-238},
  number = {4},
  note = {TY - JOUR U1 - 89110923777 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc. U2 - Constant-Time Filtering
	Space-Variant Kernels Texture Mapping Convolutions Picture/Image
	Generation},
  abstract = {Filtering is essential in computer graphics, most notably in texture
	mapping. Several techniques have been previously developed which
	allow prefiltering of a texture in time that is independent of the
	number of texture elements under the filter kernel. These are limited,
	however, to space-invariant kernels whose shape in texture space
	is the same independently of their positions, and usually are also
	limited to a small range of filters. We present a technique that
	permits constant-time filtering for space-variant kernels. The essential
	step is to approximate a filter surface in texture space by a sum
	of suitably-chosen basis functions. The convolution of a filter with
	a texture is replaced by the weighted sum of the convolution of the
	basis functions with the texture, which can be precomputed. To achieve
	constant time, convolutions with the basis functions are computed
	and stored in a pyramidal fashion, and the right level of the pyramid
	is selected so that only a constant number of points on the filter
	kernel need be evaluated. The technique allows the use of arbitrary
	filters, and as such is useful to explore interesting mappings and
	special filtering techniques. We give examples of applications to
	perspective and conformal mappings, and to the use of filters such
	as gaussians and sinc functions.},
  keywords = {Image Processing Computer Graphics Mathematical Techniques--Conformal
	Mapping Signal Filtering and Prediction -- Imaging Techniques},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{FRA01,
  author = {Frangi, A. F. and Niessen, W. J. and Viergever, M. A.},
  title = {Three-dimensional modeling for functional analysis of cardiac images,
	a review},
  journal = {IEEE Transactions on Medical Imaging},
  year = {2001},
  volume = {20},
  pages = {2--5},
  number = {1},
  abstract = {Three-dimensional (3-D) imaging of the heart is a rapidly developing
	area of research in medical imaging. Advances in hardware and methods
	for fast spatio-temporal cardiac imaging are extending the frontiers
	of clinical diagnosis and research on cardiovascular diseases. In
	the last few years, many approaches have been proposed to analyze
	images and extract parameters of cardiac shape and function from
	a variety of cardiac imaging modalities. In particular, techniques
	based on spatio-temporal geometric models have received considerable
	attention. This paper surveys the literature of two decades of research
	on cardiac modeling. The contribution of the paper is three-fold:
	(1) to serve as a tutorial of the field for both clinicians and technologists,
	(2) to provide an extensive account of modeling techniques in a comprehensive
	and systematic manner, and (3) to critically review these approaches
	in terms of their performance and degree of clinical evaluation with
	respect to the final goal of cardiac functional analysis. From this
	review it is concluded that whereas 3-D model-based approaches have
	the capability to improve the diagnostic value of cardiac images,
	issues as robustness, 3-D interaction, computational complexity and
	clinical validation still require significant attention.},
  doi = {10.1109/42.906421},
  issn = {0278-0062},
  keywords = {cardiology, diseases, medical image processing, parameter estimation,
	physiological models, reviews, 3-D interaction, cardiac images, cardiac
	modeling, cardiovascular diseases, clinical evaluation, clinical
	validation, functional analysis, medical diagnostic imaging, parameters
	extraction, three-dimensional modeling},
  owner = {euHeart},
  timestamp = {2008.10.20}
}

@INCOLLECTION{FRA98,
  author = {Frangi, Alejandro F. and Niessen, Wiro J. and Vincken, Koen L. and
	Viergever, Max A.},
  title = {Multiscale Vessel Enhancement Filtering},
  year = {1998},
  pages = {130+},
  abstract = {The multiscale second order local structure of an image (Hessian )
	is examined with the purpose of developing a vessel enhancement filter.
	A vesselness measure is obtained on the basis of all eigenvalues
	of the Hessian. This measure is tested on two dimensional DSA and
	three dimensional aortoiliac and cerebral MRA data. Its clinical
	utility is shown by the simultaneous noise and background suppression
	and vessel enhancement in maximum intensity projections and volumetric
	displays.},
  citeulike-article-id = {1408046},
  citeulike-linkout-0 = {http://www.springerlink.com/content/84rpbx096y455vtv},
  comment = {Original vessel filter from Frangi},
  journal = {Medical Image Computing and Computer-Assisted Interventation — MICCAI'98},
  keywords = {vessel},
  owner = {euHeart},
  posted-at = {2007-06-23 23:54:32},
  priority = {2},
  timestamp = {2009.10.15},
  url = {http://www.springerlink.com/content/84rpbx096y455vtv}
}

@INPROCEEDINGS{FRI08,
  author = {Friman, O. and Hindennach, M. and Peitgen, H.-O.},
  title = {Template-based multiple hypotheses tracking of small vessels},
  booktitle = {Proc. 5th IEEE International Symposium on Biomedical Imaging: From
	Nano to Macro ISBI 2008},
  year = {2008},
  pages = {1047--1050},
  abstract = {A template tracking approach to the segmentation of small 3D vessel
	structures is presented. The main contributions are a general formulation
	of a vessel template function and a multiple hypotheses tracking
	framework that is shown to improve the tracking robustness. The methodology
	is demonstrated using CT angiography data of the liver to which a
	hybrid region growing and tracking segmentation is applied.},
  doi = {10.1109/ISBI.2008.4541179},
  keywords = {blood vessels, computerised tomography, diagnostic radiography, image
	segmentation, liver, medical image processing, CT angiography data,
	hybrid region growing, liver, multiple hypotheses tracking framework,
	small 3D vessel segmentation, small vessel tracking, template based
	multiple hypotheses tracking, template tracking approach, tracking
	robustness, tracking segmentation, vessel template function, arteries,
	liver, multiple hypotheses, segmentation, template, tracking, vessels},
  owner = {euHeart},
  timestamp = {2009.10.09}
}

@ARTICLE{GAL03,
  author = {Galanda, Martin and Weibel, Robert},
  title = {Using an energy minimization technique for polygon generalization},
  journal = {Cartography and Geographic Information Science},
  year = {2003},
  volume = {30},
  pages = {263-279},
  number = {3},
  note = {TY - JOUR U1 - 03457714431 L2 - http://dx.doi.org/10.1559/152304003100011199
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Energy minimization},
  abstract = {Snakes are iterative energy-minimizing splines controlled by both
	internal constraint forces (internal energy) and external forces
	(external energy). This paper investigates the use of snakes for
	the resolution of conflicts in polygonal subdivisions (i.e., polygon
	maps or polygon mosaics) resulting from the violation of metric constraints
	which exist if a polygonal object is too small, too narrow, or too
	close to another polygon. Such metric conflicts are denoted as size
	and proximity conflicts. In the generalization of polygonal subdivisions,
	internal energy reflects the resistance of an object to deformation
	and external energy describes the need for generalization. This paper
	suggests the usage of a snakes-based algorithm which is triggered
	in such a way that it achieves the translation, a local and global
	increase (or decrease) of polygons, or an arbitrary combination of
	these transformations, depending on the conflicts encountered. Hence,
	size and proximity conflicts within a group of polygons can be solved
	simultaneously and holistically. Furthermore, snakes support the
	propagation of a change of a polygon's geometry to all adjacent neighbors.
	The proposed algorithm has been implemented in a prototype system
	that also supports a variety of other polygon generalization algorithms.
	The main difficulties identified are the intricate setup and fine-tuning
	of the snakes parameters and the computer resources required by the
	algorithm. However, the experiments showed that the proposed algorithm
	is a valuable method for the automated generalization of polygonal
	subdivisions.},
  keywords = {Energy management Maps Algorithms Geographic information systems},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{GAR10,
  author = {Garcia, Marie-Paule and Toumoulin, Christine and Haigron, Pascal
	and Velut, Jérôme and Garreau, Mireille and Boulmier, Dominique},
  title = {Coronary vein tracking from MSCT using a minimum cost path approach},
  booktitle = {ISBI},
  year = {2010},
  pages = {17--20},
  month = {April},
  publisher = {IEEE},
  abstract = {In this paper, we deal with the problem of tracking the coronary venous
	tree from Multi-Slice Computed Tomography (MSCT) angiography. Contrast
	inhomogeneities are a major issue. The proposed tracking procedure
	is based on minimum-cost path computation and makes use of ‘Fast-Marching’
	technique. The algorithm aims at propagating a front inside a vascular
	structure and extracting a centered path. To achieve this goal, a
	specific cost function which combines the vessel local orientation
	to a vesselness measure is designed. Experiments on synthetic data
	and real data have been performed. Coronary veins with contrast difficulties
	are extracted with a low computing time.},
  owner = {euHeart},
  timestamp = {2010.01.18}
}

@ARTICLE{GAR08,
  author = {J. Garot and A. Ohanessian and T. Hovasse},
  title = {[Cardiovascular magnetic resonance and multislice coronary CT in
	ischemic cardiomyopathy]},
  journal = {Ann Cardiol Angeiol (Paris)},
  year = {2008},
  volume = {57},
  pages = {359--364},
  number = {6},
  month = {Dec},
  abstract = {Cardiovascular magnetic resonance imaging and multislice coronary
	CT are frequently used in patients with suspected or known coronary
	artery disease. However, clinical indications of such noninvasive
	imaging techniques remain debated. This manuscript points out the
	advantages and limitations of each technique while clarifying their
	potential clinical indications.},
  doi = {10.1016/j.ancard.2008.10.004},
  institution = { France. j.garot@icps.com.fr},
  keywords = {Humans; Magnetic Resonance Angiography; Myocardial Ischemia, therapy;
	Tomography, X-Ray Computed},
  language = {fre},
  medline-pst = {ppublish},
  owner = {euHeart},
  pii = {S0003-3928(08)00175-3},
  pmid = {18980755},
  timestamp = {2010.01.06},
  url = {http://dx.doi.org/10.1016/j.ancard.2008.10.004}
}

@ARTICLE{GAR06,
  author = {Garreau, Mireille and Simon, Antoine and Boulmier, Dominique and
	Coatrieux, Jean-Louis and Le Breton, Hervé},
  title = {Assessment of Left Ventricular Function in Cardiac MSCT Imaging by
	a 4D Hierarchical Surface-Volume Matching Process},
  journal = {International Journal of Biomedical Imaging},
  year = {2006},
  volume = {2006},
  pages = {10},
  eid = {Article ID 37607},
  abstract = {Multislice computed tomography (MSCT) scanners offer new perspectives
	for cardiac kinetics evaluation with 4D dynamic sequences of high
	contrast and spatiotemporal resolutions. A new method is proposed
	for cardiac motion extraction in multislice CT. Based on a 4D hierarchical
	surface-volume matching process, it provides the detection of the
	heart left cavities along the acquired sequence and the estimation
	of their 3D surface velocity fields. A Markov random field model
	is defined to find, according to topological descriptors, the best
	correspondences between a 3D mesh describing the left endocardium
	at one time and the 3D acquired volume at the following time. The
	global optimization of the correspondences is realized with a multiresolution
	process. Results obtained on simulated and real data show the capabilities
	to extract clinically relevant global and local motion parameters
	and highlight new perspectives in cardiac computed tomography imaging.},
  doi = {10.1155/IJBI/2006/37607},
  owner = {euHeart},
  timestamp = {2008.09.09}
}

@ARTICLE{GIO02,
  author = {Giorgi, Benedetta and Dymarkowski, Steven and Maes, Frederik and
	Kouwenhoven, Marc and Bogaert, Jan},
  title = {{Improved Visualization of Coronary Arteries Using a New Three-Dimensional
	Submillimeter MR Coronary Angiography Sequence with Balanced Gradients}},
  journal = {Am. J. Roentgenol.},
  year = {2002},
  volume = {179},
  pages = {901-910},
  number = {4},
  abstract = {OBJECTIVE. The goal of our study was to evaluate a new three-dimensional
	real-time navigator MR coronary angiography sequence to noninvasively
	visualize the coronary arteries. SUBJECTS AND METHODS. Fifteen healthy
	volunteers underwent MR coronary angiography with a new balanced
	turbo field-echo sequence in comparison with the standard turbo field-echo
	sequence. Signal-to-noise, blood-to-myocardium, blood-to-fat, and
	blood-to-pericardial fluid contrast ratios of the left and right
	coronary artery systems were measured. Image quality was graded,
	the length and diameter of the coronary arteries were measured, and
	the number of visible side branches was assessed. RESULTS. The balanced
	turbo field-echo images yielded a higher blood-to-myocardium and
	blood-to-pericardial fluid contrast ratio, a similar blood-to-fat
	contrast ratio, and a lower signal-to-noise ratio than the turbo
	field-echo images. On a 5-point grading scale (1, nondiagnostic or
	unreadable; 2, poor; 3, moderate; 4, good; 5, excellent), image quality
	was rated significantly better for the balanced turbo field-echo
	sequence than for the turbo field-echo sequence (left coronary artery,
	4.0 {+/-} 0.6 vs 3.6 {+/-} 0.5 [p = 0.015]; right coronary artery,
	4.4 {+/-} 0.4 vs 3.6 {+/-} 0.4 [p < 0.0001], respectively), resulting
	in a significantly longer segment of the three major coronary arteries
	visualized (left anterior descending coronary artery, 92 {+/-} 21
	mm vs 79 {+/-} 24 mm; left circumflex coronary artery, 70 {+/-} 7
	mm vs 60 {+/-} 18 mm; right coronary artery, 112 {+/-} 28 mm vs 95
	{+/-} 27 mm) and a significantly higher number of side branches visualized
	(left anterior descending coronary artery, 2.9 {+/-} 1.3 vs 1.5 {+/-}
	1.3; left circumflex coronary artery, 2.1 {+/-} 1.7 vs 1.0 {+/-}
	1.2; right coronary artery, 3.7 {+/-} 1.7 vs 2.6 {+/-} 1.5). Mean
	imaging time per coronary artery was significantly shorter for the
	balanced turbo field-echo sequence (5.7 {+/-} 1.0 min) than for the
	turbo field-echo sequence (8.4 {+/-} 1.4 min) (p < 0.0001). CONCLUSION.
	Compared with standard turbo field-echo MR coronary angiography,
	optimized balanced turbo field-echo MR coronary angiography improves
	the visualization of the coronary arteries and their side branches
	within a significantly shorter imaging time.},
  eprint = {http://www.ajronline.org/cgi/reprint/179/4/901.pdf},
  owner = {euHeart},
  timestamp = {2009.01.12},
  url = {http://www.ajronline.org/cgi/content/abstract/179/4/901}
}

@INPROCEEDINGS{GRA03,
  author = {Grasset, Rapha{\"e}l and Gascuel, Jean-Dominique},
  title = {R\'ealit\'e Augment\'ee et Environnement Collaboratif : Un Tour d'Horizon},
  booktitle = {AFIG'03},
  year = {2003},
  month = {December},
  note = {Actes des},
  owner = {euHeart},
  timestamp = {2009.02.24},
  url = {http://artis.imag.fr/Publications/2003/GG03}
}

@INPROCEEDINGS{GRE93,
  author = {Greiner, G. and Seidel, H.-P.},
  title = {Modeling with Triangular B-splines},
  booktitle = {ACM/IEEE Solid Modeling Symposium},
  year = {1993},
  pages = {211-220},
  abstract = {Triangular B-splines are a new tool for the modeling of complex objects
	with non-rectangular topology. The new B-spline scheme is based on
	blending functions and control points and allows modeling piecewise
	polynomial surfaces over arbitrary triangulations with an optimal
	degree of smoothness. },
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{GU05,
  author = {Gu, A Xianfeng and He, A Ying and Qin, A Hong},
  title = {Manifold splines},
  booktitle = {Proceedings of the 2005 ACM symposium on Solid and physical modeling},
  year = {2005},
  pages = {27-38},
  address = {Cambridge, Massachusetts},
  publisher = {ACM Press},
  abstract = {Constructing splines whose parametric domain is an arbitrary manifold
	and effectively computing such splines in real-world applications
	are of fundamental importance in solid and shape modeling, geometric
	design, graphics, etc. This paper presents a general theoretical
	and computational framework, in which spline surfaces defined over
	planar domains can be systematically extended to manifold domains
	with arbitrary topology with or without boundaries. We study the
	affine structure of domain manifolds in depth and prove that the
	existence of manifold splines is equivalent to the existence of a
	manifold's affine atlas. Based on our theoretical breakthrough, we
	also develop a set of practical algorithms to generalize triangular
	B-spline surfaces from planar domains to manifold domains. We choose
	triangular B-splines mainly because of its generality and many of
	its attractive properties. As a result, our new spline surface defined
	over any manifold is a piecewise polynomial surface with high parametric
	continuity without the need for any patching and/or trimming operations.
	Through our experiments, we hope to demonstrate that our novel manifold
	splines are both powerful and efficient in modeling arbitrarily complicated
	geometry and representing continuously-varying physical quantities
	defined over shapes of arbitrary topology.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{GU02.1,
  author = {Gu, Xianfeng and Gortler, Steven J. and Hoppe, Hugues},
  title = {Geometry images},
  booktitle = {ACM Transactions on Graphics; Proceedings of ACM SIGGRAPH 2002, Jul
	23-26 2002},
  year = {2002},
  volume = {21},
  series = {ACM Transactions on Graphics},
  pages = {355-360},
  address = {United States},
  publisher = {Association for Computing Machinery},
  note = {TY - CONF U1 - 03097372548 L2 - http://dx.doi.org/10.1145/566654.566589
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Remeshing Surface parametrization Wavelet-based
	coder},
  abstract = {Surface geometry is often modeled with irregular triangle meshes.
	The process of remeshing refers to approximating such geometry using
	a mesh with (semi)-regular connectivity, which has advantages for
	many graphics applications. However, current techniques for remeshing
	arbitrary surfaces create only semi-regular meshes. The original
	mesh is typically decomposed into a set of disk-like charts, onto
	which the geometry is parametrized and sample. In this paper, we
	propose to remesh an arbitrary surface onto a completely regular
	structure we call a geometry image. It captures geometry as a simple
	2D array of quantized points. Surface signals like normals and colors
	are stored in similar 2D arrays using the same implicit surface parametrization
	- texture coordinates are absent. To create a geometry image, we
	cut an arbitrary mesh along a network of edge paths, and parametrize
	the resulting single chart onto a square. Geometry images can be
	encoded using traditional image compression algorithms, such as wavelet-based
	coders.},
  keywords = {Color image processing Two dimensional Image compression Image coding
	Wavelet transforms Algorithms Computational geometry Color computer
	graphics},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{GU03,
  author = {Gu, Xianfeng and Yau, Shing-Tung},
  title = {Global Conformal Surface Parameterization},
  booktitle = {SIGGRAPH - Symposium on Geometry Processing},
  year = {2003},
  pages = {127-137},
  abstract = {We solve the problem of computing global conformal parameterizations
	for surfaces with nontrivial topologies. The parameterization is
	global in the sense that it preserves the conformality everywhere
	except for a few points, and has no boundary of discontinuity. We
	analyze the structure of the space of all global conformal parameterizations
	of a given surface and find all possible solutions by constructing
	a basis of the underlying linear solution space. This space has a
	natural structure solely determined by the surface geometry, so our
	computing result is independent of connectivity, insensitive to resolution,
	and independent of the algorithms to discover it. Our algorithm is
	based on the properties of gradient fields of conformal maps, which
	are closedness, harmonity, conjugacy, duality and symmetry. These
	properties can be formulated by sparse linear systems, so the method
	is easy to implement and the entire process is automatic. We also
	introduce a novel topological modification method to improve the
	uniformity of the parameterization. Based on the global conformal
	parameterization of a surface, we can construct a conformal atlas
	and use it to build conformal geometry images which have very accurate
	reconstructed normals.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{GU02.2,
  author = {Gu, Xianfeng and Yau, Shing-Tung},
  title = {Computing Conformal Structures of Surfaces},
  journal = {Communications in Information and Systems},
  year = {2002},
  volume = {2},
  pages = {121-146},
  number = {2},
  abstract = {This paper solves the problem of computing conformal structures of
	general 2-manifolds represented as triangular meshes. We approximate
	the De Rham cohomology by simplicial cohomology and represent the
	Laplace-Beltrami operator, the Hodge star operator by linear systems.
	A basis of holomorphic one-forms is constructed explicitly. We then
	obtain a period matrix by integrating holomorphic differentials along
	a homology basis. We also study the global conformal mappings between
	genus zero surfaces and spheres, and between general surfaces and
	planes. Our method of computing conformal structures can be applied
	to tackle fundamental problems in computer aid geometry design and
	computer graphics, such as geometry classification and identification,
	and surface global parametrization.},
  keywords = {Mesh, conformal structure, texture mapping, Holomorphic forms, harmonic
	forms, period matrix.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{GUI06,
  author = {Eric Guilbert and Hui Lin},
  title = {B-Spline curve smoothing under position constraints for line generalisation},
  booktitle = {GIS '06: Proceedings of the 14th annual ACM international symposium
	on Advances in geographic information systems},
  year = {2006},
  pages = {3--10},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {Currently, most of the operations performed for the construction of
	marine charts are still done manually. However, with the development
	of more and more powerful techniques, new processing methods must
	be developed in order to deal with the increasing amount of data
	and to achieve automatic construction. For that purpose, a new method
	for line smoothing is introduced in this paper and is applied to
	the generalisation of isobathymetric lines (lines connecting points
	at a same depth). The lines are modelled by B-spline curves which
	maintain their smooth feature. Smoothing is performed by reducing
	the curvature using a snake model. The generalisation constraint
	of navigation safety is satisfied by applying position constraints
	on the line. Spatial conflicts are also taken into consideration
	and are removed during the process. Parameters are automatically
	defined so that the method can be applied to large sets without user
	intervention. The method has been applied on real data sets and examples
	are provided and discussed for scale reduction of lines.},
  doi = {http://doi.acm.org/10.1145/1183471.1183474},
  isbn = {1-59593-529-0},
  location = {Arlington, Virginia, USA},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{GUN97,
  author = {Gunn, Steve R. and Nixon, Mark S.},
  title = {Robust snake implementation; a dual active contour},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {1997},
  volume = {19},
  pages = {63-68},
  number = {1},
  note = {TY - JOUR U1 - 97023529081 L2 - http://dx.doi.org/10.1109/34.566812
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Snakes Active contours Regularization Local
	shape Parameterization},
  abstract = {A conventional active contour formulation suffers difficulty in appropriate
	choice of an initial contour and values of parameters. Recent approaches
	have aimed to resolve these problems but can compromise other performance
	aspects. To relieve the problem in initialization, we use a dual
	active contour, which is combined with a local shape model to improve
	the parameterization. One contour expands from inside the target
	feature, the other contracts from the outside. The two contours are
	interlinked to provide a balanced technique with an ability to reject
	'weak' local energy minima.},
  keywords = {Image analysis Feature extraction Performance Edge detection Image
	understanding Mathematical models Computer vision},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{GUS99,
  author = {Igor Guskov and Wim Sweldens and Peter Schr\"oder},
  title = {Multiresolution signal processing for meshes},
  booktitle = {SIGGRAPH},
  year = {1999},
  pages = {325--334},
  address = {New York, NY, USA},
  publisher = {ACM Press/Addison-Wesley Publishing Co.},
  doi = {http://doi.acm.org/10.1145/311535.311577},
  isbn = {0-201-48560-5},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{GWY93,
  author = {Gwydir, S.H. and Buettner, H.M. and Dunn, S.M.},
  title = {Feature extraction for non-rigid motion analysis using continuity
	splines},
  booktitle = {Proceedings of the 1993 IEEE 19th Annual Northeast Bioengineering
	Conference, Mar 18-19 1993},
  year = {1993},
  series = {Bioengineering, Proceedings of the Northeast Conference},
  pages = {50-51},
  address = {Newark, NJ, USA},
  publisher = {Publ by IEEE, Piscataway, NJ, USA},
  note = {TY - CONF U1 - 93071017781 L2 - http://dx.doi.org/10.1109/NEBC.1993.404420
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Diagnostic imaging Feature extraction Motion
	analysis Nerve injury},
  abstract = {Quantitative analysis of neurite outgrowth and growth cone motility
	provides key information for the development of new therapeutic strategies
	for treating peripheral nerve injury. Our work may reveal insights
	into nerve disease; however, this theory is still speculative since
	injury and disease present such different situations. The goal of
	this research is to track the motion of the growth cone through sequential
	time frames, and to provide quantitative analysis of specific neurite
	modalities. The motion of the growth cone can be characterized as
	non-rigid, since the contour of the growth cone is constantly deforming.
	The problem of tracking the motion of non-rigid objects is a relatively
	new area in research. Energy-minimizing active contour models have
	recently been proposed to track deformable objects through sequential
	time frames. Active contours, or snakes, reach equilibrium by locking
	on to salient image features. Active contour methods will be used
	to track the motion of the growth cone.},
  keywords = {Neurophysiology Medical imaging Image analysis Computer vision Patient
	treatment},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{GUE08,
  author = {M. Akif Gülsün and Hüseyin Tek},
  title = {Robust vessel tree modeling.},
  journal = {Med Image Comput Comput Assist Interv Int Conf Med Image Comput Comput
	Assist Interv},
  year = {2008},
  volume = {11},
  pages = {602--611},
  number = {Pt 1},
  abstract = {In this paper, we present a novel method for extracting center axis
	representations (centerlines) of blood vessels in contrast enhanced
	(CE)-CTA/MRA, robustly and accurately. This graph-based optimization
	algorithm which employs multi-scale medialness filters extracts vessel
	centerlines by computing the minimum-cost paths. Specifically, first,
	new medialness filters are designed from the assumption of circular/elliptic
	vessel cross-sections. These filters produce contrast and scale independent
	responses even the presence of nearby structures. Second, they are
	incorporated to the minimum-cost path detection algorithm in a novel
	way for the computational efficiency and accuracy. Third, the full
	vessel centerline tree is constructed from this optimization technique
	by assigning a saliency measure for each centerline from their length
	and radius information. The proposed method is computationally efficient
	and produces results that are comparable in quality to the ones created
	by experts. It has been tested on more than 100 coronary artery data
	set where the full coronary artery trees are extracted in 21 seconds
	in average on a 3.2 GHz PC.},
  institution = {Imaging and Visualization, Siemens Corporate Research, Princeton,
	NJ, USA. akif.gulsun@siemens.com},
  keywords = {Algorithms; Angiography, methods; Artificial Intelligence; Blood Vessels,
	anatomy /&/ histology; Computer Simulation; Humans; Image Enhancement,
	methods; Image Interpretation, Computer-Assisted, methods; Models,
	Biological; Models, Statistical; Pattern Recognition, Automated,
	methods; Reproducibility of Results; Sensitivity and Specificity;
	Tomography, X-Ray Computed, methods},
  owner = {euHeart},
  pmid = {18979796},
  timestamp = {2009.08.21}
}

@ARTICLE{HAK00,
  author = {Haker, Steven and Angenent, Sigurd and Tannenbaum, Allen and Kikinis,
	Ron and Sapiro, Guillermo and Halle, Michael},
  title = {Conformal surface parameterization for texture mapping},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  year = {2000},
  volume = {6},
  pages = {181-189},
  number = {2},
  note = {TY - JOUR U1 - 00085278864 L2 - http://dx.doi.org/10.1109/2945.856998
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Conformal surface parameterization Texture
	mapping},
  abstract = {In this paper, we give an explicit method for mapping any simply connected
	surface onto the sphere in a manner which preserves angles. This
	technique relies on certain conformal mappings from differential
	geometry. Our method provides a new way to automatically assign texture
	coordinates to complex undulating surfaces. We demonstrate a finite
	element method that can be used to apply our mapping technique to
	a triangulated geometric description of a surface.},
  keywords = {Image quality Computational geometry Finite element method Partial
	differential equations Image analysis},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{HAN08,
  author = {Han, Bohyung and Comaniciu, D. and Zhu, Ying and Davis, L. S. },
  title = {Sequential Kernel Density Approximation and Its Application to Real-Time
	Visual Tracking},
  journal = IEEE_J_PAMI,
  year = {2008},
  volume = {30},
  pages = {1186--1197},
  number = {7},
  abstract = {Visual features are commonly modeled with probability density functions
	in computer vision problems, but current methods such as a mixture
	of Gaussians and kernel density estimation suffer from either the
	lack of flexibility by fixing or limiting the number of Gaussian
	components in the mixture or large memory requirement by maintaining
	a nonparametric representation of the density. These problems are
	aggravated in real-time computer vision applications since density
	functions are required to be updated as new data becomes available.
	We present a novel kernel density approximation technique based on
	the mean-shift mode finding algorithm and describe an efficient method
	to sequentially propagate the density modes over time. Although the
	proposed density representation is memory efficient, which is typical
	for mixture densities, it inherits the flexibility of nonparametric
	methods by allowing the number of components to be variable. The
	accuracy and compactness of the sequential kernel density approximation
	technique is illustrated by both simulations and experiments. Sequential
	kernel density approximation is applied to online target appearance
	modeling for visual tracking, and its performance is demonstrated
	on a variety of videos.},
  doi = {10.1109/TPAMI.2007.70771},
  issn = {0162-8828},
  keywords = {Gaussian processes, approximation theory, computer vision, estimation
	theory, probability, tracking, Gaussian estimation, kernel density
	estimation, mean-shift mode finding algorithm, online target appearance
	modeling, probability density function, real-time computer vision
	application, real-time visual tracking, sequential kernel density
	approximation, visual feature modeling, Computer vision, Statistical,
	Tracking},
  owner = {euHeart},
  timestamp = {2008.10.20}
}

@INPROCEEDINGS{HAN96,
  author = {Han, Song and Medioni, Gerard},
  title = {Spherical winged B-snakes},
  booktitle = {Proceedings of the 1996 IEEE International Conference on Image Processing,
	ICIP'96. Part 2 (of 3), Sep 16-19 1996},
  year = {1996},
  volume = {2},
  series = {IEEE International Conference on Image Processing},
  pages = {389-392},
  address = {Lausanne, Switz},
  publisher = {IEEE, Los Alamitos, CA, USA},
  note = {TY - CONF U1 - 97013502631 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc. U2 - Spherical triangular
	B splines Spherical winged B snakes Closed shape representation},
  abstract = {We introduce spherical triangular B-splines for closed shape representation,
	and discuss the shape reconstruction using our new model `winged
	B-snakes', which are deformable surfaces coupled with active edges
	and junctions. We show results of using the spherical winged B-snakes
	for simultaneous surface reconstruction and feature detection from
	range images or scattered 3D data.},
  keywords = {Data structures Feature extraction Edge detection Curve fitting Least
	squares approximations Image reconstruction},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{HAN95,
  author = {Han, Song and Medioni, Gerard},
  title = {Triangular NURBS Surface Modeling of Scattered Data.},
  booktitle = {IEEE visualization},
  year = {1995},
  pages = {295-302},
  address = {San Francisco},
  abstract = {Scattered data modeling is useful in many scientific fields and industrial
	applications to reveal the properties and relationships in empirically
	acquired data sets. If the data contain discontinuities, the data
	analysts must manually mark the segmenting boundaries before using
	the current software packages. Here, we automatically detect discontinuities
	from noisy sparse scattered data and use triangular NURBS surfaces
	to model and visualize the data. We use Guy and Medioni's global
	voting method to interpolate from sparse data three dense potential
	fields for surfaces, edges, and junctions. The global voting interpolants
	encode several human perceptual grouping principles such as co-surfacity,
	proximity, and constancy of curvature. The inferred potential fields
	are stored in three volumetric grids, giving each voxel the probability
	of being a surface point, an edge point, and a junction point. Then
	we use a new model called winged B-snakes, which are deformable triangular
	NURBS surfaces embedded with active curves, to fit the surfaces and
	align the edges and junctions. Finally, a smooth C1 surface which
	preserves discontinuity edges and junctions is constructed. Fine-tuning
	and surface fairing is automatically done by adjusting the weights.
	We present experimental results on both functional and spherical
	data sets.},
  keywords = {Scattered Data, Surface Reconstruction, Discontinuity Detection, Perceptual
	Grouping, Triangular NURBS, Membrane/Thin-Plate Energy, Marching
	Cubes, Iso-surface extraction.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{HAT86,
  author = {Hatamian, M. },
  title = {A real-time two-dimensional moment generating algorithm and its single
	chip implementation},
  journal = {IEEE Transactions on Acoustics, Speech and Signal Processing},
  year = {1986},
  volume = {34},
  pages = {546--553},
  number = {3},
  abstract = {We present a fast algorithm and its single chip VLSI implementation
	for generating moments of two-dimensional (2-D) digital images for
	real-time image processing applications. Using this algorithm, the
	number of multiplications for computing 16 moments of a 512 &#215;
	512 image is reduced by more than 5 orders of magnitude compared
	to the direct implementation; the number of additions is reduced
	by a factor of 4. This also makes the software implementation extremely
	fast. Using the chip, 16 moments &#956;<sup>p,q</sup>(p = 0, 1, 2,
	3, q = 0, 1, 2, 3) of a 512 &#215; 512 8 bits/pixel image can be
	calculated in real time (i.e., 30 frames per second). Each moment
	value is computed as a 64- bit integer. The basic building block
	of the algorithm is a single-pole digital filter implemented with
	a simple accumulator. These filters are cascaded together in both
	horizontal and vertical directions in a highly regular structure
	which makes it very suitable for VLSI implementation. The chip has
	been implemented in 2.5 &#956; CMOS technology, it occupies 6100
	&#956;m &#215; 6100 &#956;m of silicon area. The chip can also be
	used as a general cell in a systolic architecture for implementing
	2-D transforms having polynomial basis functions.},
  issn = {0096-3518},
  owner = {euHeart},
  timestamp = {2008.09.23}
}

@INPROCEEDINGS{HE04,
  author = {He, Ying and Qin, Hong},
  title = {Surface Reconstruction with Triangular B-splines},
  booktitle = {Geometric Modeling and Processing},
  year = {2004},
  pages = {279-290},
  address = {Beijing, China},
  publisher = {IEEE Computer Society},
  abstract = {This paper presents a novel modeling technique for reconstructing
	a triangular B-spline surface from a set of scanned 3D points. Unlike
	existing surface reconstruction methods based on tensor-product B-splines
	which primarily generate a network of patches and then enforce certain
	continuity (usually, G1 or C1) between adjacent patches, our algorithm
	can avoid the complicated procedures of surface trimming and patching.
	In our framework, the user simply specifies the degree n of the triangular
	B-spline surface and fitting error tolerance . The surface reconstruction
	procedure generates a single triangular B-spline patch that has Cn-1
	continuity over smooth regions and C0 on sharp features. More importantly,
	all the knots and control points are determined by minimizing a linear
	combination of interpolation and fairness functionals. Examples are
	presented which demonstrate the effectiveness of the technique for
	real data sets.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{HEO04,
  author = {Heo, Hoon and Ahn, Y. and Chae, Ok-Sam},
  title = {B-spline contour fitting in image sequences using genetic algorithm},
  booktitle = {Proceedings of the International Conference on Imaging Science, Systems
	and Technology, CISST'04, Jun 21-24 2004},
  year = {2004},
  series = {Proceedings of the International Conference on Imaging Science, Systems
	and Technology, CISST'04},
  pages = {118-123},
  address = {Las Vegas, NV, United States},
  publisher = {CSREA Press, Bogart, GA 30622, United States},
  note = {TY - CONF U1 - 05068824828 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc. U2 - Dental CT images
	Image sequences Genetic B-spline fitting Contour extraction CT slices},
  abstract = {A snake model based on genetic B-spline fitting is presented for the
	extraction of an accurate object contour in image sequences such
	as dental CT slices. In the CT image sequences, the shape of object
	boundary changes significantly between two adjacent slices and objects
	are closely located. Classical snake algorithms have not been successful
	in such an image sequence due to the difficulty in initialization
	and existence of multiple extrema. In order to extract accurate contour,
	we propose a new contour extraction algorithm, which is designed
	to take advantage of previous slice contour.},
  keywords = {Image analysis Genetic algorithms Boundary conditions Optimization
	Curve fitting Probability Computerized tomography Feature extraction},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@PHDTHESIS{HER02,
  author = {Hern\'andez Hoyos, Marcella},
  title = {Segmentation anisotrope 3D pour la quantification en imagerie vasculaire
	par r\'esonnance magn\'etique},
  school = {INSA Lyon},
  year = {2002},
  type = {{Thèse de doctorat}},
  note = {{BLOCH I. (rap.), DOUEK P., HERNANDEZ J.T., MAGNIN I.E., NAHUM R.,
	ORKISZ M. et SEQUEIRA J. (rap.)}},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{HER07,
  author = {Monica Hernandez and Alejandro F Frangi},
  title = {Non-parametric geodesic active regions: method and evaluation for
	cerebral aneurysms segmentation in 3DRA and CTA.},
  journal = {Med Image Anal},
  year = {2007},
  volume = {11},
  pages = {224--241},
  number = {3},
  month = {Jun},
  abstract = {Segmentation of vascular structures is a difficult and challenging
	task. In this article, we present an algorithm devised for the segmentation
	of such structures. Our technique consists in a geometric deformable
	model with associated energy functional that incorporates high-order
	multiscale features in a non-parametric statistical framework. Although
	the proposed segmentation method is generic, it has been applied
	to the segmentation of cerebral aneurysms in 3DRA and CTA. An evaluation
	study over 10 clinical datasets indicate that the segmentations obtained
	by our method present a high overlap index with respect to the ground-truth
	(91.13\% and 73.31\%, respectively) and that the mean error distance
	from the surface to the ground truth is close to the in-plane resolution
	(0.40 and 0.38 mm, respectively). Besides, our technique favorably
	compares to other alternative techniques based on deformable models,
	namely parametric geodesic active regions and active contours without
	edges.},
  doi = {10.1016/j.media.2007.01.002},
  institution = {Aragon Institute of Engineering Research, University of Zaragoza,
	Zaragoza, Spain. mhg@unizar.es},
  keywords = {Algorithms; Artifacts; Cerebral Angiography, methods; Contrast Media;
	Humans; Image Processing, Computer-Assisted, methods; Imaging, Three-Dimensional;
	Intracranial Aneurysm, diagnosis/radiography; Magnetic Resonance
	Angiography; Models, Biological; Models, Statistical; Tomography,
	X-Ray Computed},
  language = {eng},
  medline-pst = {ppublish},
  owner = {euHeart},
  pii = {S1361-8415(07)00002-3},
  pmid = {17434784},
  timestamp = {2009.09.21},
  url = {http://dx.doi.org/10.1016/j.media.2007.01.002}
}

@ARTICLE{HIC04,
  author = {S. J. Hickman and A. Hadjiprocopis and O. Coulon and D. H. Miller
	and G. J. Barker},
  title = {Cervical spinal cord MTR histogram analysis in multiple sclerosis
	using a 3D acquisition and a B-spline active surface segmentation
	technique.},
  journal = {Magn Reson Imaging},
  year = {2004},
  volume = {22},
  pages = {891--895},
  number = {6},
  month = {Jul},
  abstract = {The application of a three-dimensional magnetization transfer (MT)
	sequence and B-spline active surface segmentation method to produce
	MT histograms of the cervical spinal cord in a pilot study of controls
	and multiple sclerosis (MS) patients is presented. Subjects' cervical
	spinal cords were imaged with (a) a volume-acquired inversion-prepared
	fast spoiled gradient echo sequence and (b) a volume-acquired noninversion-prepared
	fast spoiled gradient echo MT sequence. The images were segmented
	using the B spline active surface technique and MT histograms were
	produced from the MT images. The method was sensitive enough to detect
	differences between seven MS patients and 10 controls in mean MT
	ratio (42.4 pu versus 44.0 pu, p = 0.03) and peak location (45.2
	versus 46.8, p = 0.03). The spinal cord volumes obtained from the
	two sequences were associated with each other (parameter estimate
	0.972, 95\% confidence intervals 0.742, 1.202, p < 0.001).},
  doi = {10.1016/j.mri.2004.01.056},
  keywords = {Adult; Cervical Vertebrae; Humans; Imaging, Three-Dimensional; Magnetic
	Resonance Imaging; Middle Aged; Multiple Sclerosis; Pilot Projects;
	Sensitivity and Specificity; Spinal Cord},
  owner = {Dje},
  pii = {S0730725X04000773},
  pmid = {15234459},
  timestamp = {2007.10.21},
  url = {http://dx.doi.org/10.1016/j.mri.2004.01.056}
}

@PHDTHESIS{HIR03,
  author = {Anil Nirmal Hirani},
  title = {Discrete exterior calculus},
  year = {2003},
  address = {Pasadena, CA, USA},
  note = {Adviser-Jerrold E. Marsden},
  abstract = {This thesis presents the beginnings of a theory of discrete exterior
	calculus (DEC). Our approach is to develop DEC using only discrete
	combinatorial and geometric operations on a simplicial complex and
	its geometric dual. The derivation of these may require that the
	objects on the discrete mesh, but not the mesh itself, are interpolated.
	
	
	Our theory includes not only discrete equivalents of differential
	forms, but also discrete vector fields and the operators acting on
	these objects. Definitions are given for discrete versions of all
	the usual operators of exterior calculus. The presence of forms and
	vector fields allows us to address their various interactions, which
	are important in applications. In many examples we find that the
	formulas derived from DEC are identical to the existing formulas
	in the literature. We also show that the circumcentric dual of a
	simplicial complex plays a useful role in the metric dependent part
	of this theory. The appearance of dual complexes leads to a proliferation
	of the operators in the discrete theory.
	
	
	One potential application of DEC is to variational problems which
	come equipped with a rich exterior calculus structure. On the discrete
	level, such structures will be enhanced by the availability of DEC.
	One of the objectives of this thesis is to fill this gap. There are
	many constraints in numerical algorithms that naturally involve differential
	forms. Preserving such features directly on the discrete level is
	another goal, overlapping with our goals for variational problems.
	
	
	In this thesis we have tried to push a purely discrete point of view
	as far as possible. We argue that this can only be pushed so far,
	and that interpolation is a useful device. For example, we found
	that interpolation of functions and vector fields is a very convenient.
	In future work we intend to continue this interpolation point of
	view, extending it to higher degree forms, especially in the context
	of the sharp, Lie derivative and interior product operators. Some
	preliminary ideas on this point of view are presented in the thesis.
	We also present some preliminary calculations of formulas on regular
	nonsimplicial complexes.},
  order_no = {AAI3086864},
  owner = {euHeart},
  publisher = {California Institute of Technology},
  timestamp = {2009.02.24}
}

@ARTICLE{Hoyos2006,
  author = {Hoyos, Marcela and OrÅ‚owski, Piotr and PiÄ…tkowska-Janko, Ewa and
	Bogorodzki, Piotr and Orkisz, Maciej},
  title = {Vascular Centerline Extraction in 3D MR Angiograms for Phase Contrast
	MRI Blood Flow Measurement},
  journal = {International Journal of Computer Assisted Radiology and Surgery},
  year = {2006},
  volume = {1},
  pages = {51--61},
  number = {1},
  month = mar,
  abstract = {Abstract&nbsp;&nbsp;The accuracy of 2D phase contrast (PC) magnetic
	resonance angiography (MRA) depends on the alignment between the
	vessels and the imaging plane. PC MRA imaging of blood flow is challenging
	when the flow in several vessels is to be evaluated with one acquisition.
	For this purpose, semi-automatic determination of the plane most
	perpendicular to several vessels is proposed based on centerlines
	extracted from 3D MRA. Arterial centerlines are extracted from 3D
	MRA based on iterative estimation-prediction, multi-scale analysis
	of image moments, and a second-order shape model. The optimal plane
	is determined by minimizing misalignment between its normal vector
	and the centerlinesâ€™ tangent vectors. The method was evaluated
	on a phantom and on 35 patients, by seeking the optimal plane for
	cerebral blood flow quantification simultaneously in internal carotids
	and vertebral arteries. In the phantom, difference of orientation
	and of height between known and calculated planes was 1.2Â° and 2.5&nbsp;mm,
	respectively. In the patients, all but one centerline were correctly
	extracted and the misalignment of the plane was within 12Â° per artery.
	Semi-automatic centerline extraction simplifies and automates determination
	of the plane orthogonal to one vessel, thereby permitting automatic
	simultaneous minimization of the misalignment with several vessels
	in PC MRA.},
  owner = {euHeart},
  timestamp = {2010.07.08},
  url = {http://dx.doi.org/10.1007/s11548-006-0005-0}
}

@INPROCEEDINGS{HU04,
  author = {Hu, Jiongjiong and Yu, Huimin and Fang, Bo},
  title = {Morphologically restricted b-snake model for clustering cell image
	segmentation},
  booktitle = {Proceedings - 2004 International Conference on Intelligent Mechatronics
	and Automation, Aug 26-31 2004},
  year = {2004},
  series = {Proceedings - 2004 International Conference on Intelligent Mechatronics
	and Automation},
  pages = {685-689},
  address = {Chengdu, China},
  publisher = {Institute of Electrical and Electronics Engineers Inc., New York,
	NY 10016-5997, United States},
  note = {TY - CONF U1 - 04498704617 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc. U2 - Active contour B-spline
	Cell image segmentation Cell boundaries},
  abstract = {In order to separate the clustering cells that frequently appear in
	cell images, an automatic segmentation algorithm is presented. The
	algorithm generates a modified "nonlinear distance image," and then
	uses active contour model to find cell boundaries. An improved iterative
	erosion method, which uses a dynamic structure rather than a fixed
	one to produce the distance image, is proposed in the algorithm.
	A novel B-spline active contour model, whose energy depends not only
	on the image itself but also on the nonlinear distance image, is
	then presented in this paper. Initiated via morphological operation,
	the model would finally detect the cell boundaries. Experimental
	results show that our algorithm is effective for automatic segmentation
	on clustering cell images.},
  keywords = {Control equipment Computational complexity Parameter estimation Mathematical
	morphology Algorithms Approximation theory Mathematical models Image
	segmentation},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{HUA06,
  author = {Huang, Fuzhen and Su, Jianbo},
  title = {Moment-based Shape Priors for Geometric Active Contours},
  booktitle = {Proc. 18th International Conference on Pattern Recognition ICPR 2006},
  year = {2006},
  volume = {2},
  pages = {56--59},
  abstract = {In this paper a new method that incorporates moment-based shape information
	into geometric active contours is presented. As any shape may theoretically
	be characterized by its set of moments, the shape prior is represented
	based on Legendre moments. By combining the shape prior with the
	powerful geometric active contours proposed by Chan and Vese, the
	improved model can retain all the advantage of the Chan-Vese model
	and have the additional ability of being able to compactly represent
	global shape of an object. Experiments on synthetic and real image
	segmentation show the efficiency of our method},
  doi = {10.1109/ICPR.2006.808},
  issn = {1051-4651},
  keywords = {Legendre polynomials, computational geometry, image segmentation,
	method of moments, Legendre moments, geometric active contours, image
	segmentation, moment-based shape priors, shape information},
  owner = {euHeart},
  timestamp = {2008.09.23}
}

@INPROCEEDINGS{HUA98,
  author = {Huang, Jiantao and Amini, Amir A.},
  title = {Anatomical object volumes from deformable B-spline surface models},
  booktitle = {ICIP},
  year = {1998},
  volume = {1},
  pages = {732-736},
  address = {Chicago, IL, USA},
  publisher = {IEEE Comp Soc, Los Alamitos, CA, USA},
  note = {98124502656 Compilation and indexing terms, Copyright 2005 Elsevier
	Engineering Information, Inc. Shape reconstruction Conjugate gradient
	decent method},
  abstract = {Accurate delineation of anatomical objects in 3D volumetric data is
	a significant problem in medical imaging. We have built a system
	that takes as input a spatial stack of 2D image slices being studied.
	The output of the system is a smooth 3D surface delineating the medical
	structure of interest, and the volume enclosed by the 3D surface.
	This paper presents a new method to represent a 3D anatomical object
	by a deformable B-spline tube, that facilitates the calculation of
	the volume enclosed based on a closed-form volume calculation formula.
	In order to validate the deformable B-spline surface fitting and
	associated volume calculation, 19 simulated 3D images were considered
	and theoretical volumes were compared with volumes produced by our
	system. The errors were less than 4.9%.},
  keywords = {Medical imaging Three dimensional Calculations Computer simulation
	Mathematical models Matrix algebra Image analysis},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{HYC91,
  author = {Hyche, M. Eric and Ezquerra, Norberto F. and Lawton, Daryl},
  title = {Vasculature detection in angiograms using active contours},
  booktitle = {EMBC},
  year = {1991},
  volume = {13},
  pages = {1054-1055},
  address = {Orlando, FL, USA},
  publisher = {Publ by IEEE, Piscataway, NJ, USA},
  note = {TY - CONF U1 - 92080562043 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc. U2 - Vasculature Detection
	Active Contour Methods Coronary Blood Vessels Vessel Detection Methods
	Spline Energy Minimization},
  abstract = {Active contour models, or snakes, are applied to the problem of detecting
	coronary blood vessels in single and multiple-frame cardiac angiographic
	image sequences. If an appropriately placed repulsive force was used,
	the snake found the vessel in approximately 25% the number of iterations.
	The primary tradeoff in this task is between user intervention and
	computation time. For multiple-frame detection, the method works
	well when vessel displacements are relatively small, which is the
	case for data at 30 frames per second. However, for data at 15 frames
	per second or less, this method should probably be used in conjunction
	with other validation methods in order to assure finding the vessels
	across the entire cardiac cycle.},
  keywords = {Image Processing - Image Analysis Optimization Biomedical Engineering
	-- Angiocardiography},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{JAC05,
  author = {Marcel Jackowski and Xenophon Papademetris and Lawrence W Dobrucki
	and Albert J Sinusas and Lawrence H Staib},
  title = {Characterizing vascular connectivity from microCT images.},
  journal = {Med Image Comput Comput Assist Interv Int Conf Med Image Comput Comput
	Assist Interv},
  year = {2005},
  volume = {8},
  pages = {701--708},
  number = {Pt 2},
  abstract = {X-ray microCT (computed tomography) has become a valuable tool in
	the analysis of vascular architecture in small animals. Because of
	its high resolution, a detailed assessment of blood vessel physiology
	and pathology is possible. Vascular measurement from noninvasive
	imaging is important for the study and quantification of vessel disease
	and can aid in diagnosis, as well as measure disease progression
	and response to therapy. The analysis of tracked vessel trajectories
	enables the derivation of vessel connectivity information, lengths
	between vessel junctions as well as level of ramification, contributing
	to a quantitative analysis of vessel architecture. In this paper,
	we introduce a new vessel tracking methodology based on wave propagation
	in oriented domains. Vessel orientation and vessel likelihood are
	estimated based on an eigenanalysis of gray-level Hessian matrices
	computed at multiple scales. An anisotropic wavefront then propagates
	through this vector field with a speed modulated by the maximum vesselness
	response at each location. Putative vessel trajectories can be found
	by tracing the characteristics of the propagation solution between
	different points. We present preliminary results from both synthetic
	and mouse microCT image data.},
  institution = {Department of Diagnostic Radiology , Yale University, New Haven,
	CT 06520, USA.},
  keywords = {Algorithms; Angiography, methods; Animals; Artificial Intelligence;
	Imaging, Three-Dimensional, methods; Mice; Pattern Recognition, Automated,
	methods; Radiographic Image Enhancement, methods; Radiographic Image
	Interpretation, Computer-Assisted, methods; Reproducibility of Results;
	Sensitivity and Specificity; Tomography, X-Ray Computed, methods},
  owner = {euHeart},
  pmid = {16686021},
  timestamp = {2009.02.25}
}

@ARTICLE{JAC04,
  author = {Jacob, Mathews and Blu, Thierry and Unser, Michael},
  title = {Efficient energies and algorithms for parametric snakes},
  journal = {IEEE Transactions on Image Processing},
  year = {2004},
  volume = {13},
  pages = {1231-1244},
  number = {9},
  note = {TY - JOUR U1 - 04378347637 L2 - http://dx.doi.org/10.1109/TIP.2004.832919
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Parametric active contour models Active contour
	Image energy Surface integral},
  abstract = {Parametric active contour models are one of the preferred approaches
	for image segmentation because of their computational efficiency
	and simplicity. However, they have a few drawbacks which limit their
	performance. In this paper, we identify some of these problems and
	propose efficient solutions to get around them. The widely-used gradient
	magnitude-based energy is parameter dependent; its use will negatively
	affect the parametrization of the curve and, consequently, its stiffness.
	Hence, we introduce a new edge-based energy that is independent of
	the parameterization. It is also more robust since it takes into
	account the gradient direction as well. We express this energy term
	as a surface integral, thus unifying it naturally with the region-based
	schemes. The unified framework enables the user to tune the image
	energy to the application at hand. We show that parametric snakes
	can guarantee low curvature curves, but only if they are described
	in the curvilinear abscissa. Since normal curve evolution do not
	ensure constant arc-length, we propose a new internal energy term
	that will force this configuration. The curve evolution can sometimes
	give rise to closed loops in the contour, which will adversely interfere
	with the optimization algorithm. We propose a curve evolution scheme
	that prevents this condition. &copy; 2004 IEEE.},
  keywords = {Algorithms Edge detection Computational complexity Problem solving
	Gradient methods Optimization Mathematical models Vectors Fourier
	transforms Integral equations Partial differential equations Probability
	distributions Image segmentation},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{JAC01,
  author = {Jacob, M. and Blu, T. and Unser, M.},
  title = {A Unifying Approach and Interface for Spline-Based Snakes},
  booktitle = {SPIE International Symposium on Medical Imaging: Image Processing},
  year = {2001},
  volume = {4322},
  pages = {340-347},
  address = {San Diego},
  abstract = {In this paper, we present different solutions for improving spline-based
	snakes. First, we demonstrate their minimum curvature interpolation
	property, and use it as an argument to get rid of the explicit smoothness
	constraint. We also propose a new external energy obtained by integrating
	a non-linearly pre-processed image in the closed region bounded by
	the curve. We show that this energy, besides being efficiently computable,
	is sufficiently general to include the widely used gradient-based
	schemes, Bayesian schemes, their combinations and discriminant-based
	approaches. We also introduce two initialization modes and the appropriate
	constraint energies. },
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{JAH06,
  author = {Cosima Jahnke and Ingo Paetsch and Eike Nagel},
  title = {3D MR coronary angiography: optimization of the technique and preliminary
	results.},
  journal = {Int J Cardiovasc Imaging},
  year = {2006},
  volume = {22},
  pages = {489--491},
  number = {3-4},
  doi = {10.1007/s10554-006-9075-x},
  keywords = {Coronary Angiography, methods; Coronary Artery Disease, diagnosis/pathology;
	Coronary Vessels, anatomy /&/ histology; Humans; Image Interpretation,
	Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance
	Angiography; Reproducibility of Results; Sensitivity and Specificity;
	Time Factors},
  language = {eng},
  medline-pst = {ppublish},
  owner = {euHeart},
  pmid = {16538429},
  timestamp = {2009.09.17},
  url = {http://dx.doi.org/10.1007/s10554-006-9075-x}
}

@INPROCEEDINGS{JIN04,
  author = {Jin, Miao and Wang, Yalin and Yau, Shing-Tung and Gu, Xianfeng},
  title = {Optimal global conformal surface parameterization},
  booktitle = {IEEE Visualization 2004 - Proceedings, VIS 2004, Oct 10-15 2004},
  year = {2004},
  series = {IEEE Visualization 2004 - Proceedings, VIS 2004},
  pages = {267-274},
  address = {Austin, TX, United States},
  publisher = {Institute of Electrical and Electronics Engineers Inc., New York,
	NY 10016-5997, United States},
  note = {TY - CONF U1 - 05179062265 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc. U2 - Object modeling
	Curve surfaces Object representations Surface parameterization},
  abstract = {All orientable metric surfaces are Riemann surfaces and admit global
	conformal parameterizations. Riemann surface structure is a fundamental
	structure and governs many natural physical phenomena, such as heat
	diffusion and electro-magnetic fields on the surface. A good parameterization
	is crucial for simulation and visualization. This paper provides
	an explicit method for finding optimal global conformal parameterizations
	of arbitrary surfaces. It relies on certain holomorphic differential
	forms and conformal mappings from differential geometry and Riemann
	surface theories. Algorithms are developed to modify topology, locate
	zero points, and determine cohomology types of differential forms.
	The implementation is based on a finite dimensional optimization
	method. The optimal parameterization is intrinsic to the geometry,
	preserves angular structure, and can play an important role in various
	applications including texture mapping, remeshing, morphing and simulation.
	The method is demonstrated by visualizing the Riemann surface structure
	of real surfaces represented as triangle meshes. &copy; 2004 IEEE.},
  keywords = {Computational geometry Virtual reality Object recognition Conformal
	mapping Problem solving Mathematical transformations Algorithms Computer
	graphics},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{KAG98,
  author = {Kagan, Pavel and Fischer, Anath and Bar-Yoseph, Pinhas Z.},
  title = {New B-Spline Finite Element Approach for Geometrical Design and Mechanical
	Analysis.},
  journal = {International Journal for Numerical MEthods in Engineering},
  year = {1998},
  volume = {41},
  pages = {435-458},
  abstract = {In most existing CAD systems, geometrical design and mechanical analysis
	are operated as completely separate modules. Intensive interaction
	between these modules is, however, highly desired due to the iterative
	nature of a typical product development process. Formulating a new,
	uniÞed approach to design and analysis that provides a high level
	of interaction is the main purpose of this research. The idea is
	to integrate a mechanically based geometrical design concept with
	the mechanical analysis module in a uniform B-Spline Finite Element
	(BSFE) environment. In this paper, the BSFE method is formulated
	and its validity and adequacy are veriÞed for elastic linear rod
	and plate models. In particular, the feasibility of applying B-spline
	functions as base functions of the Þnite element method for design
	and analysis is demonstrated. Unique scheme attributes based on intrinsic
	properties of B-spline functions are investigated in detail. Method
	adequacy is demonstrated by comparing convergence characteristics,
	complexity and computational cost to the spectral element method.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{KAJ08,
  author = {Fumihiko Kajiya and Toyotaka Yada and Osamu Hiramatsu and Yasuo Ogasawara
	and Yousuke Inai and Masahito Kajiya},
  title = {Coronary microcirculation in the beating heart.},
  journal = {Med Biol Eng Comput},
  year = {2008},
  volume = {46},
  pages = {411--419},
  number = {5},
  month = {May},
  abstract = {The phase opposition of velocity waveforms between coronary arteries
	(predominantly diastolic) and veins (systolic) is the most prominent
	characteristic of coronary hemodynamics. This unique arterial and
	venous flow patterns indicate the importance of intramyocardial capacitance
	vessels and variable resistance vessels during a cardiac cycle. It
	was shown that during diastole the intramyocardial capacitance vessels
	have two functional components, unstressed volume and ordinary capacitance.
	Unstressed volume is defined as the volume of blood in a vessel at
	zero transmural pressure. In vivo observation of systolic narrowing
	of arterioles in mid-wall and in subendocardium indicates the increase
	in resistance by cardiac contraction.},
  doi = {10.1007/s11517-008-0335-x},
  institution = {Department of Medical Engineering, Kawasaki Medical School, 288 Matsushima
	Kurashiki, Okayama 701-0193, Japan. kajiya@me.kawasaki-m.ac.jp},
  keywords = {Blood Flow Velocity, physiology; Coronary Circulation, physiology;
	Humans; Laser-Doppler Flowmetry; Microcirculation, physiology; Models,
	Cardiovascular; Myocardial Contraction, physiology; Vascular Resistance,
	physiology},
  owner = {euHeart},
  pmid = {18365262},
  timestamp = {2008.09.26},
  url = {http://dx.doi.org/10.1007/s11517-008-0335-x}
}

@ARTICLE{KAN99,
  author = {Kang, Dong Joong},
  title = {Fast and stable snake algorithm for medical images},
  journal = {Pattern Recognition Letters},
  year = {1999},
  volume = {20},
  pages = {507-512},
  number = {5},
  note = {TY - JOUR U1 - 99074727612 L2 - http://dx.doi.org/10.1016/S0167-8655(99)00019-7
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Active contours Discrete dynamic models},
  abstract = {A discrete dynamic model for defining and tracking contours in 2-D
	medical images is presented. An active contour in this objective
	is optimized by a dynamic programming algorithm, for which a new
	constraint that has fast and stable properties is introduced. The
	internal energy of the model depends on local behavior of the contour,
	while the external energy is derived from image features. The algorithm
	is able to rapidly detect convex and concave objects even when the
	image quality is poor.},
  keywords = {Medical imaging Image analysis Image enhancement Image reconstruction
	Dynamic programming Algorithms Constraint theory Mathematical models
	Image quality Pattern matching},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{KAN09,
  author = {Guolian Kang and Keying Ye and Nianjun Liu and David B Allison and
	Guimin Gao},
  title = {Weighted multiple hypothesis testing procedures.},
  journal = {Stat Appl Genet Mol Biol},
  year = {2009},
  volume = {8},
  pages = {Article23},
  number = {1},
  abstract = {Multiple hypothesis testing is commonly used in genome research such
	as genome-wide studies and gene expression data analysis (Lin, 2005).
	The widely used Bonferroni procedure controls the family-wise error
	rate (FWER) for multiple hypothesis testing, but has limited statistical
	power as the number of hypotheses tested increases. The power of
	multiple testing procedures can be increased by using weighted p-values
	(Genovese et al., 2006). The weights for the p-values can be estimated
	by using certain prior information. Wasserman and Roeder (2006) described
	a weighted Bonferroni procedure, which incorporates weighted p-values
	into the Bonferroni procedure, and Rubin et al. (2006) and Wasserman
	and Roeder (2006) estimated the optimal weights that maximize the
	power of the weighted Bonferroni procedure under the assumption that
	the means of the test statistics in the multiple testing are known
	(these weights are called optimal Bonferroni weights). This weighted
	Bonferroni procedure controls FWER and can have higher power than
	the Bonferroni procedure, especially when the optimal Bonferroni
	weights are used. To further improve the power of the weighted Bonferroni
	procedure, first we propose a weighted Sidák procedure that incorporates
	weighted p-values into the Sidák procedure, and then we estimate
	the optimal weights that maximize the average power of the weighted
	Sidák procedure under the assumption that the means of the test statistics
	in the multiple testing are known (these weights are called optimal
	Sidák weights). This weighted Sidák procedure can have higher power
	than the weighted Bonferroni procedure. Second, we develop a generalized
	sequential (GS) Sidák procedure that incorporates weighted p-values
	into the sequential Sidák procedure (Scherrer, 1984). This GS idák
	procedure is an extension of and has higher power than the GS Bonferroni
	procedure of Holm (1979). Finally, under the assumption that the
	means of the test statistics in the multiple testing are known, we
	incorporate the optimal Sidák weights and the optimal Bonferroni
	weights into the GS Sidák procedure and the GS Bonferroni procedure,
	respectively. Theoretical proof and/or simulation studies show that
	the GS Sidák procedure can have higher power than the GS Bonferroni
	procedure when their corresponding optimal weights are used, and
	that both of these GS procedures can have much higher power than
	the weighted Sidák and the weighted Bonferroni procedures. All proposed
	procedures control the FWER well and are useful when prior information
	is available to estimate the weights.},
  doi = {10.2202/1544-6115.1437},
  institution = {University of Alabama at Birmingham, Birmingham, AL 35294, USA. gkang@ms.soph.uab.edu},
  keywords = {Computer Simulation; Data Interpretation, Statistical; False Positive
	Reactions; Gene Expression Profiling, methods; Genome-Wide Association
	Study; Humans; Models, Theoretical},
  language = {eng},
  medline-pst = {ppublish},
  owner = {euHeart},
  pmid = {19409067},
  timestamp = {2009.10.16},
  url = {http://dx.doi.org/10.2202/1544-6115.1437}
}

@INPROCEEDINGS{KAN01,
  author = {Kanitsar, A. and Fleischmann, D. and Wegenkittl, R. and Felkel, P.
	and Groller, E.},
  title = {CPR - curved planar reformation},
  booktitle = {Proc. IEEE Visualization VIS 2002},
  year = {2002},
  pages = {37--44},
  abstract = {Visualization of tubular structures such as blood vessels is an important
	topic in medical imaging. One way to display tubular structures for
	diagnostic purposes is to generate longitudinal cross-sections in
	order to show their lumen, wall, and surrounding tissue in a curved
	plane. This process is called curved planar reformation (CPR). We
	present three different methods to generate CPR images. A tube-phantom
	was scanned with computed tomography (CT) to illustrate the properties
	of the different CPR methods. Furthermore we introduce enhancements
	to these methods: thick-CPR, rotating-CPR and multi-path-CPR.},
  doi = {10.1109/VISUAL.2002.1183754},
  keywords = {blood vessels, computerised tomography, data visualisation, medical
	image processing, rendering (computer graphics), blood vessels, computed
	tomography, curved planar reformation, diagnostic purposes, image
	generation, longitudinal cross-sections, medical imaging, multi-path-CPR,
	rotating-CPR, surrounding tissue, thick-CPR, tubular structures,
	visualization},
  owner = {euHeart},
  timestamp = {2009.06.18}
}

@INPROCEEDINGS{KAS87,
  author = {Kass, Michael and Witkin, Andrew and Terzopoulos, Demetri},
  title = {Snakes: Active Contour Models},
  booktitle = {ICCV},
  year = {1987},
  pages = {259-268},
  address = {London, Engl},
  publisher = {IEEE, New York, NY, USA},
  note = {TY - CONF U1 - 87100162084 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc. U2 - ENERGY-MINIMIZING
	SPLINE SNAKES: (ACTIVE CONTOUR MODELS) IMAGE FORCES},
  abstract = {A snake is an energy-minimizing spline guided by external constraint
	forces and influenced by image forces that pull it toward features
	such as lines and edges. Snakes are active contour models: they lock
	onto nearby edges, localizing them accurately. Scale-space continuation
	can be used to enlarge the capture region surrounding a feature.
	Snakes provide a unified account of a number of visual problems,
	including detection of edges, lines, and subjective contours, motion
	tracking, and stereo matching. The authors have used snakes successfully
	for interactive interpretation, in which user-imposed constraint
	forces guide the snake near features of interest.},
  keywords = {MATHEMATICAL TECHNIQUES - Variational Techniques IMAGE PROCESSING},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{KAT08,
  author = {Marcus Katoh and Elmar Spuentrup and Christoph Barmet and Matthias
	Stuber},
  title = {Local re-inversion coronary MR angiography: arterial spin-labeling
	without the need for subtraction.},
  journal = {J Magn Reson Imaging},
  year = {2008},
  volume = {27},
  pages = {913--917},
  number = {4},
  month = {Apr},
  abstract = {PURPOSE: To implement a double-inversion bright-blood coronary MR
	angiography sequence using a cylindrical re-inversion prepulse for
	selective visualization of the coronary arteries. MATERIALS AND METHODS:
	Local re-inversion bright-blood magnetization preparation was implemented
	using a nonselective inversion followed by a cylindrical aortic re-inversion
	prepulse. After an inversion delay that allows for in-flow of the
	labeled blood-pool into the coronary arteries, three-dimensional
	radial steady-state free-precession (SSFP) imaging (repetition/echo
	time, 7.2/3.6 ms; flip angle, 120 degrees, 16 profiles per RR interval;
	field of view, 360 mm; matrix, 512, twelve 3-mm slices) is performed.
	Coronary MR angiography was performed in three healthy volunteers
	and in one patient on a commercial 1.5 Tesla whole-body MR System.
	RESULTS: In all subjects, coronary arteries were selectively visualized
	with positive contrast. In addition, a middle-grade stenosis of the
	proximal right coronary artery was seen in one patient. CONCLUSION:
	A novel T1 contrast-enhancement strategy is presented for selective
	visualization of the coronary arteries without extrinsic contrast
	medium application. In comparison to former arterial spin-labeling
	schemes, the proposed magnetization preparation obviates the need
	for a second data set and subtraction.},
  doi = {10.1002/jmri.21319},
  institution = {Department of Diagnostic and Interventional Radiology, University
	Hospital Saarland, Homburg, Germany. marcus.katoh@uniklinikum-saarland.de},
  keywords = {Adult; Coronary Circulation; Coronary Stenosis, diagnosis; Coronary
	Vessels, anatomy /&/ histology; Humans; Magnetic Resonance Angiography;
	Middle Aged; Spin Labels; Subtraction Technique},
  owner = {root},
  pmid = {18383252},
  timestamp = {2009.07.08},
  url = {http://dx.doi.org/10.1002/jmri.21319}
}

@INPROCEEDINGS{KUH02,
  author = {Khne, Gerald and Weickert, Joachim and Beier, Markus and Effelsberg,
	Wolfgang},
  title = {Fast Implicit Active Contour Models},
  booktitle = {Pattern Recognition},
  year = {2002},
  editor = {Gool., L. Van},
  series = {Lecture Notes in Computer Science},
  address = {Berlin},
  publisher = {Springer},
  abstract = {Implicit active contour models are widely used in image processing
	and computer vision tasks. Most implementations, however, are based
	on explicit updating schemes and are therefore of limited computational
	efficiency. In this paper, we present fast algorithms based on the
	semi-implicit additive operator splitting (AOS) scheme for both the
	geometric and the geodesic active contour model. Our experimental
	results with synthetic and real-world images demonstrate that one
	can gain a speed up by one order of magnitude compared to the widely
	used explicit time discretization.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{KHO03,
  author = {Andrei Khodakovsky and Nathan Litke and Peter Schr\"oder},
  title = {Globally smooth parameterizations with low distortion},
  booktitle = {SIGGRAPH '03: ACM SIGGRAPH 2003 Papers},
  year = {2003},
  pages = {350--357},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {Good parameterizations are of central importance in many digital geometry
	processing tasks. Typically the behavior of such processing algorithms
	is related to the smoothness of the parameterization and how much
	distortion it contains. Since a parameterization maps a bounded region
	of the plane to the surface, a parameterization for a surface which
	is not homeomorphic to a disc must be made up of multiple pieces.
	We present a novel parameterization algorithm for arbitrary topology
	surface meshes which computes a globally smooth parameterization
	with low distortion. We optimize the patch layout subject to criteria
	such as shape quality and metric distortion, which are used to steer
	a mesh simplification approach for base complex construction. Global
	smoothness is achieved through simultaneous relaxation over all patches,
	with suitable transition functions between patches incorporated into
	the relaxation procedure. We demonstrate the quality of our parameterizations
	through numerical evaluation of distortion measures and the excellent
	rate distortion performance of semi-regular remeshes produced with
	these parameterizations. The numerical algorithms required to compute
	the parameterizations are robust and run on the order of minutes
	even for large meshes.},
  doi = {http://doi.acm.org/10.1145/1201775.882275},
  isbn = {1-58113-709-5},
  location = {San Diego, California},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{KIC95,
  author = {Kichenassamy, Satyanad and Kumar, Arun and Olver, Peter and Tannenbaum,
	Allen and Yezzi, Anthony},
  title = {Gradient flows and geometric active contour models},
  booktitle = {Proceedings of the 5th International Conference on Computer Vision,
	Jun 20-23 1995},
  year = {1995},
  series = {Proceedings of the IEEE International Conference on Computer Vision},
  pages = {810-815},
  address = {Cambridge, MA, USA},
  publisher = {IEEE, Piscataway, NJ, USA},
  note = {TY - CONF U1 - 95082831923 L2 - http://dx.doi.org/10.1109/ICCV.1995.466855
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Gradient flows Geometric active contour models
	Riemannian metrics Surface model Euclidean perimeter Gradient evolution
	equations},
  abstract = {In this paper, we analyze the geometric active contour models discussed
	in [6, 18] from a curve evolution point of view and propose some
	modifications based on gradient flows relative to certain new feature-based
	Riemannian metrics. This leads to a novel snake paradigm in which
	the feature of interest may be considered to lie at the bottom of
	a potential well. Thus the snake is attracted very naturally and
	efficiently to the desired feature. Moreover, we consider some 3-D
	active surface models based on these ideas.},
  keywords = {Mathematical models Computer vision Computer graphics Problem solving
	Three dimensional Feature extraction Image processing Computational
	geometry},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{KIM01,
  author = {W. Y. Kim and P. G. Danias and M. Stuber and S. D. Flamm and S. Plein
	and E. Nagel and S. E. Langerak and O. M. Weber and E. M. Pedersen
	and M. Schmidt and R. M. Botnar and W. J. Manning},
  title = {Coronary magnetic resonance angiography for the detection of coronary
	stenoses.},
  journal = {N Engl J Med},
  year = {2001},
  volume = {345},
  pages = {1863--1869},
  number = {26},
  month = {Dec},
  abstract = {BACKGROUND: An accurate, noninvasive technique for the diagnosis of
	coronary disease would be an important advance. We investigated the
	accuracy of coronary magnetic resonance angiography among patients
	with suspected coronary disease in a prospective, multicenter study.
	METHODS: Coronary magnetic resonance angiography was performed during
	free breathing in 109 patients before elective x-ray coronary angiography,
	and the results of the two diagnostic procedures were compared. RESULTS:
	A total of 636 of 759 proximal and middle segments of coronary arteries
	(84 percent) were interpretable on magnetic resonance angiography.
	In these segments, 78 (83 percent) of 94 clinically significant lesions
	(those with a > or = 50 percent reduction in diameter on x-ray angiography)
	were also detected by magnetic resonance angiography. Overall, coronary
	magnetic resonance angiography had an accuracy of 72 percent (95
	percent confidence interval, 63 to 81 percent) in diagnosing coronary
	artery disease. The sensitivity, specificity, and accuracy for patients
	with disease of the left main coronary artery or three-vessel disease
	were 100 percent (95 percent confidence interval, 97 to 100 percent),
	85 percent (95 percent confidence interval, 78 to 92 percent), and
	87 percent (95 percent confidence interval, 81 to 93 percent), respectively.
	The negative predictive values for any coronary artery disease and
	for left main artery or three-vessel disease were 81 percent (95
	percent confidence interval, 73 to 89 percent) and 100 percent (95
	percent confidence interval, 97 to 100 percent), respectively. CONCLUSIONS:
	Among patients referred for their first x-ray coronary angiogram,
	three-dimensional coronary magnetic resonance angiography allows
	for the accurate detection of coronary artery disease of the proximal
	and middle segments. This noninvasive approach reliably identifies
	(or rules out) left main coronary artery or three-vessel disease.},
  doi = {10.1056/NEJMoa010866},
  institution = {Cardiovascular Division, Department of Medicine, Beth Israel Deaconess
	Medical Center and Harvard Medical School, Boston, MA 02215, USA.},
  keywords = {Adult; Aged; Coronary Angiography; Coronary Stenosis, diagnosis/radiography;
	Female; Humans; Imaging, Three-Dimensional; Magnetic Resonance Angiography,
	methods; Male; Middle Aged; Predictive Value of Tests; Prospective
	Studies; Sensitivity and Specificity},
  owner = {euHeart},
  pii = {345/26/1863},
  pmid = {11756576},
  timestamp = {2009.07.02},
  url = {http://dx.doi.org/10.1056/NEJMoa010866}
}

@INPROCEEDINGS{KIR03,
  author = {Kirbas, C. and Quek, F. K. H. },
  title = {Vessel extraction techniques and algorithms: a survey},
  booktitle = {Proc. Third IEEE Symposium on Bioinformatics and Bioengineering},
  year = {2003},
  pages = {238--245},
  abstract = {Vessel segmentation algorithms are critical components of circulatory
	blood vessel analysis systems. We present a survey of vessel extraction
	techniques and algorithms, putting the various approaches and techniques
	in perspective by means of a classification of the existing research.
	While we target mainly the extraction of blood vessels, neurovascular
	structure in particular we also review some of the segmentation methods
	for the tubular objects that show similar characteristics to vessels.
	We divide vessel segmentation algorithms and techniques into six
	main categories: (1) pattern recognition techniques, (2) model-based
	approaches, (3) tracking-based approaches, (4) artificial intelligence-based
	approaches, (5) neural network-based approaches, and (6) miscellaneous
	tube-like object detection approaches. Some of these categories are
	further divided into sub-categories. A table compares the papers
	against such criteria as dimensionality, input type, preprocessing,
	user interaction, and result type.},
  keywords = {artificial intelligence, blood vessels, diagnostic radiography, feature
	extraction, image recognition, medical image processing, neural nets,
	object detection, physiological models, reviews, algorithms, artificial
	intelligence-based approaches, blood vessel delineation, blood vessels,
	circulatory blood vessel analysis systems, dimensionality, image
	segmentation methods, input type, malformations, medical images,
	model-based approaches, neural network-based approaches, neurovascular
	structure, patient diagnosis, pattern recognition techniques, preprocessing,
	stenosis, tracking-based approaches, tube-like object detection approaches,
	user interaction, vessel diagnosis, vessel image extraction techniques},
  owner = {euHeart},
  timestamp = {2008.09.12}
}

@ARTICLE{KIS95,
  author = {Kisworo, M. and Venkatesh, S. and West, G.A.W.},
  title = {Detection of curved edges at subpixel accuracy using deformable models},
  journal = {IEE Proceedings: Vision, Image and Signal Processing},
  year = {1995},
  volume = {142},
  pages = {304-312},
  number = {5},
  note = {TY - JOUR U1 - 95122958832 L2 - http://dx.doi.org/10.1049/ip-vis:19952175
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Curve reconstruction Deformable models Subpixel
	levels Global energy functional integral Euler Lagrange transformation
	Energy minimization equation Lagrangian energy minimization},
  abstract = {One approach to the detection of curves at subpixel accuracy involves
	the reconstruction of such features from subpixel edge data points.
	A new technique is presented for reconstructing and segmenting curves
	with subpixel accuracy using deformable models. A curve is represented
	as a set of interconnected Hermite splines forming a snake generated
	from the subpixel edge information that minimizes the global energy
	functional integral over the set. While previous work on the minimization
	was mostly based on the Euler-Lagrange transformation, the authors
	use the finite element method to solve the energy minimization equation.
	The advantages of this approach over the Euler-Lagrange transformation
	approach are that the method is straightforward, leads to positive
	m-diagonal symmetric matrices, and has the ability to cope with irregular
	geometries such as junctions and corners. The energy functional integral
	solved using this method can also be used to segment the features
	by searching for the location of the maxima of the first derivative
	of the energy over the elementary curve set.},
  keywords = {Image reconstruction Mathematical models Image segmentation Integral
	equations Mathematical transformations Finite element method Matrix
	algebra Image quality Edge detection},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{KIS94,
  author = {Kisworo, M. and Venkatesh, S. and West, G.},
  title = {Modeling edges at subpixel accuracy using the local energy approach},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {1994},
  volume = {16},
  pages = {405-410},
  number = {4},
  note = {TY - JOUR U1 - 94121501919 L2 - http://dx.doi.org/10.1109/34.277593
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Edge detection Local energy Subpixel feature
	detection Edge models Least squared error fitting technique},
  abstract = {In this paper we described new technique for 1-D and 2-D edge feature
	extraction to subpixel accuracy using edge models and the local energy
	approach. A candidate edge is modeled as one of a number of parametric
	edge models, and the fit is refined by a least-squared error fitting
	technique.},
  keywords = {Image processing Imaging techniques Algorithms Mathematical models
	Pattern recognition Computer vision},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{KLE97,
  author = {Klein, Andreas K. and Lee, Forester and Amini, Amir A.},
  title = {Quantitative coronary angiography with deformable spline models},
  journal = {IEEE Transactions on Medical Imaging},
  year = {1997},
  volume = {16},
  pages = {468-482},
  number = {5},
  note = {TY - JOUR U1 - 97123959749 L2 - http://dx.doi.org/10.1109/42.640737
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Coronary angiography Gabor filters Stenosis
	B spline snakes},
  abstract = {Although current edge-following schemes can be very efficient in determining
	coronary boundaries, they may fail when the feature to be followed
	is disconnected (and the scheme is unable to bridge the discontinuity)
	or branch points exist where the best path to follow is indeterminate.
	In this paper, we present new deformable spline algorithms for determining
	vessel boundaries, and enhancing their centerline features. A bank
	of even and odd S-Gabor filter pairs of different orientations are
	convolved with vascular images in order to create an external snake
	energy field. Each filter pair will give maximum response to the
	segment of vessel having the same orientation as the filters. The
	resulting responses across filters of different orientations are
	combined to create an external energy field for snake optimization.
	Vessels are represented by B-Spline snakes, and are optimized on
	filter outputs with dynamic programming. The points of minimal constriction
	and the percent-diameter stenosis are determined from a computed
	vessel centerline. The system has been statistically validated using
	fixed stenosis and flexible-tube phantoms. It has also been validated
	on 20 coronary lesions with two independent operators, and has been
	tested for interoperator and intraoperator variability and reproducibility.
	The system has been found to be specially robust in complex images
	involving vessel branchings and incomplete contrast filling.},
  keywords = {Diseases Blood vessels Physiological models Optimization Dynamic programming
	Statistical methods Angiography},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{KOT08,
  author = {Kotoulas, L. and Andreadis, I. },
  title = {Fast Moment Generating Architectures},
  journal = IEEE_J_CASVT,
  year = {2008},
  volume = {18},
  pages = {533--537},
  number = {4},
  abstract = {Image moments are used extensively in various applications. Depending
	on the task at hand, the type as well as the order of moments to
	be implemented varies greatly. Simple geometric characteristics require
	the first few geometrical moments, while robust image watermarking
	employs high-order orthogonal moments. In this paper, we present
	an architecture which can be easily modified to implement various
	types of moments, including geometric, continuous as well as discrete
	orthogonal, in real-time. Although this architecture is optimized
	for hardware, software implementation is feasible and advantageous
	in some cases.},
  doi = {10.1109/TCSVT.2008.918760},
  issn = {1051-8215},
  keywords = {computational geometry, image reconstruction, watermarking, fast image
	moment generating architecture, image reconstruction, orthogonal
	moments, robust image watermarking, Computer vision, FPGA, Image
	moments, computer vision, field-programmable gate arrays (FPGAs),
	image moments, real-time},
  owner = {euHeart},
  timestamp = {2008.09.23}
}

@BOOK{KUN75,
  title = {La Plaisanterie},
  publisher = {Gallimard Paris},
  year = {1975},
  author = {Milan Kundera},
  isbn = {2070366383},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{LAC03,
  author = {Lachaud, J.-O. and Taton, B.},
  title = {Deformable model with adaptive mesh and automated topology changes},
  booktitle = {3-D Digital Imaging and Modeling (3DIM'03)},
  year = {2003},
  pages = {12-19},
  address = {Alberta, Canada},
  note = {TY - CONF},
  abstract = {Due to their general and robust formulation deformable models offer
	a very appealing approach to 3D image segmentation. However there
	is a trade-off between model genericity, model accuracy and computational
	efficiency. In general, fully generic models require a uniform sampling
	of either the space or their mesh. The segmentation accuracy is thus
	a global parameter. Recovering small image features results in heavy
	computational costs whereas generally only restricted parts of images
	require a high segmentation accuracy. We present a highly deformable
	model that both handles fully automated topology changes and adapts
	its resolution locally according to the geometry of image features.
	The main idea is to replace the Euclidean metric with a Riemannian
	metric that expands interesting parts of the image. Then, a regular
	sampling is maintained with this new metric. This allows to automatically
	handle topology changes while increasing the model resolution locally
	according to the geometry of image components. By this way high quality
	segmentation is achieved with reduced computational costs.},
  keywords = {computational geometry image resolution image sampling image segmentation
	mesh generation solid modelling topology 3D image segmentation Euclidean
	metric Riemannian metric adaptive mesh automated topology changes
	deformable model image geometry features uniform sampling},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{LAG07.1,
  author = {Brunolf W Lagerveld and Rene D ter Wee and Jean J M C H de la Rosette
	and Jos A E Spaan and Hessel Wijkstra},
  title = {Vascular fluorescence casting and imaging cryomicrotomy for computerized
	three-dimensional renal arterial reconstruction.},
  journal = {BJU Int},
  year = {2007},
  volume = {100},
  pages = {387--391},
  number = {2},
  month = {Aug},
  abstract = {Authors from the Netherlands studied the use of a casting technique,
	cryomicrotome imaging and three-dimensional computer analysis to
	visualise and reconstruct the arterial anatomy in a porcine kidney
	model. They found that this could be done satisfactorily to a resolution
	of 50 microm. OBJECTIVE: To assess the combined use of a casting
	technique, cryomicrotomy imaging, and three-dimensional (3D) computer
	analysis as a method for visualizing and reconstructing the arterial
	vascular tree in a porcine renal model. MATERIAL AND METHODS: The
	arterial branches of two porcine kidneys were filled with a fluorescent
	cast, after which they were cut into slices of 50 microm in an imaging
	cryomicrotome. From each section, digital images of the cutting plane
	of the sample were taken and stored in the computer, after which
	stacks of images were rendered in 3D. RESULTS: A 3D computerized
	reconstruction of the arterial vascular tree was constructed and
	showed the complete arterial anatomy up to arterioles of 50 microm.
	CONCLUSION: With visualization by fluorescence imaging cryomicrotomy,
	the anatomical and 3D reconstruction of the renal arterial blood
	supply in a pig kidney is possible up to a resolution of 50 microm.},
  doi = {10.1111/j.1464-410X.2007.06914.x},
  institution = {Department of Urology, St Lucas Andreas Hospital and Onze Lieve Vrouwe
	Gasthuis, Amsterdam, The Netherlands.},
  keywords = {Animals; Blood Vessels, anatomy /&/ histology; Image Processing, Computer-Assisted,
	methods; Imaging, Three-Dimensional, methods; Kidney Diseases, surgery;
	Kidney, blood supply/surgery; Models, Animal; Pilot Projects; Swine},
  owner = {euHeart},
  pii = {BJU6914},
  pmid = {17498198},
  timestamp = {2008.09.26},
  url = {http://dx.doi.org/10.1111/j.1464-410X.2007.06914.x}
}

@INPROCEEDINGS{LAG06,
  author = {Laguitton, S. and Boldak, C. and Bousse, A. and Yang, G. and Toumoulin,
	C. },
  title = {Temporal Tracking of Coronaries in MSCTA by Means of 3D Geometrical
	Moments},
  booktitle = {Proc. 28th Annual International Conference of the IEEE Engineering
	in Medicine and Biology Society EMBS '06},
  year = {2006},
  pages = {924--927},
  abstract = {An algorithm is proposed that perform a temporal tracking of the vessel
	central axis in a 3-D dynamic sequence in multi-slice computed tomography
	(MSCT). The approach is based on geometric moments and a local cylindrical
	approximation. The local characteristics of the vessel are estimated
	on the first volume of the sequence (position on the vessel central
	axis, local diameter, intravascular and background intensities),
	then used to track the vessel along the sequence. The correspondence
	between two volumes is solved through a region matching based on
	a criterion of minimal distance combining moment-based descriptors
	with intensity information. Preliminary results are presented on
	two sequences.},
  doi = {10.1109/IEMBS.2006.260670},
  issn = {1557-170X},
  keywords = {3D Geometric Moments, 3D Sequence Tracking, Coronary Extraction, MSCTA},
  owner = {euHeart},
  timestamp = {2008.09.10}
}

@INPROCEEDINGS{LAG07.2,
  author = {Laguitton, S. and Boldak, C. and Toumoulin, C. },
  title = {Temporal Tracking of Coronaries in Multi-slice Computed Tomography},
  booktitle = {Proc. 29th Annual International Conference of the IEEE Engineering
	in Medicine and Biology Society EMBS 2007},
  year = {2007},
  pages = {4512--4515},
  abstract = {A method is proposed that performs a temporal tracking of the coronaries
	in multi-slice computed tomography (MSCT) dynamic sequences. The
	process exploits geometric moments and a local cylindrical approximation
	of the vessel to estimate the local characteristics of the structure
	in each volume and estimate its displacement along the sequence.
	The research strategy is based on a region matching process to find
	the location of the point in the successive volumes. A spatial tracking
	is then applied to refine its location inside the vessel. Tests have
	been achieved on simulated and real displacements of coronary segments.},
  doi = {10.1109/IEMBS.2007.4353342},
  issn = {1557-170X},
  keywords = {blood vessels, cardiovascular system, medical image processing, coronaries,
	displacement estimation, local cylindrical approximation, multislice
	computed tomography, region matching process, temporal tracking},
  owner = {euHeart},
  timestamp = {2008.09.10}
}

@INPROCEEDINGS{LAI93,
  author = {Lai, Kok F. and Chin, Roland T.},
  title = {On Regularization, Formulation and Initialization of the Active Contour
	Models (Snakes)},
  booktitle = {First Asian Conference on Computer Vision},
  year = {1993},
  pages = {542-545},
  address = {Osaka},
  abstract = {In snake formulation, large regularization enhances the robustness
	against noise and incomplete data, while small values increase the
	accuracy in capturing boundary variations. We present a local minimax
	criterion which automatically determines the optimal regularization
	at every locations along the boundary with no added computation cost.
	We also modify existing energy formulations to repair deciencies
	in internal energy and improve performance in external energy. This
	yields snakes that contain Hough transform as a special case. We
	can therefore initialize the snake efficiently and reliably using
	Hough transform.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{LAM02,
  author = {Lam, S.Y. and Tong, C.S.},
  title = {Conformal Snake algorithm for contour detection},
  journal = {Electronics Letters},
  year = {2002},
  volume = {38},
  pages = {452-453},
  number = {10},
  note = {TY - JOUR U1 - 02226963071 L2 - http://dx.doi.org/10.1049/el:20020335
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Snake algorithm Contour detection},
  abstract = {A novel and effective modification of the original Snake algorithm
	is proposed. The modification can improve the capability of the algorithm
	to detect boundaries with sharp corners or concave parts without
	the need to introduce external forces. The essential idea is to apply
	conformal mapping to transform the image so that the object boundary
	in the new domain can be captured by the Snake algorithm.},
  keywords = {Conformal mapping Iterative methods Algorithms Mathematical models
	Object recognition},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@OTHER{LAR09,
  abstract = {GIMIAS is a workflow-oriented environment for addressing advanced
	biomedical image computing and build personalized computational models,
	which is extensible through the development of application-specific
	plug-ins. In addition, GIMIAS provides an open source framework for
	efficient development of research and clinical software prototypes
	integrating contributions from the Virtual Physiological Human community
	while allowing business-friendly technology transfer and commercial
	product development. This framework has been fully developed in ANSI-C++
	on top of well known open source libraries like VTK, ITK and wxWidgets
	among others. Based on GIMIAS, in this paper is presented a workflow
	for medical image analysis and simulation of the heart.},
  author = {Larrabide, Ignacio and Omedas, Pedro and Martelli, Yves and Planes,
	Xavier and Nieber, Maarten and Moya, Juan and Butakoff, Constantine
	and Sebasti\~an, Rafael and Camara, Oscar and De Craene, Mathieu
	and Bijnens, Bart and Frangi, Alejandro},
  journal = {Functional Imaging and Modeling of the Heart},
  owner = {euHeart},
  pages = {417--426},
  timestamp = {2010.05.17},
  title = {GIMIAS: An Open Source Framework for Efficient Development of Research
	Tools and Clinical Prototypes},
  url = {http://dx.doi.org/10.1007/978-3-642-01932-6_45},
  year = {2009}
}

@INPROCEEDINGS{LAR03,
  author = {Larralde, A. and Boldak, C. and Garreau, M. and Toumoulin, C. and
	Boulmier, D. and Rolland, Y.},
  title = {Evaluation of a 3D Segmentation Software for the Coronary Characterization
	in Multi-slice Computed Tomography},
  booktitle = {Functional Imaging and Modeling of the Heart},
  year = {2003},
  pages = {39--51},
  abstract = {A new generation of sub-second multi-slices computed tomography (MSCT)
	scanners, which allow a complete coronary coverage, is becoming widely
	available. Nevertheless, they need to be associated with 3D processing
	tools to quantify the coronary diseases. This study proposes to evaluate
	a new 3D moment-based method for the extraction of the coronary network
	and the calcification localization in MSCT. We called on two medical
	experts respectively in coronarography and radiology to carry out
	this evaluation. It was based on a comparison between extracted vessels
	and original scan data with objective and subjective criteria. This
	preliminary study has been performed on a set of six data sets, which
	included pathological patterns such as dense and scattered calcifications.
	These results confirm the good performances of the method with high
	scores of sensitivity and constitute a first step toward the detection
	of coronary networks in MSCT data.},
  owner = {euHeart},
  timestamp = {2008.09.09},
  url = {http://dx.doi.org/10.1007/3-540-44883-7_5}
}

@INPROCEEDINGS{LEG96,
  author = {Le Goualher, Georges and Barillot, Christian and Bizais, Yves and
	Scarabin, Jean-Marie},
  title = {Three-dimensional segmentation of cortical sulci using active models},
  booktitle = {Medical Imaging 1996 Image Processing, Feb 12-15 1996},
  year = {1996},
  volume = {2710},
  series = {Proceedings of SPIE - The International Society for Optical Engineering},
  pages = {254-263},
  address = {Newport Beach, CA, United States},
  publisher = {The International Society for Optical Engineering},
  note = {TY - CONF U1 - 02387093484 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc. U2 - Active models Cortical
	anatomy},
  abstract = {We propose a method for the segmentation of cerebral sulci, representing
	them by surfaces. This method is based on the computation of the
	differential characteristics of MRI data. The computation of curvature
	information, using the Lvv operator, allows one to differentiate
	sulcal and gyral regions, resulting in a global detection of the
	cortical scheme. The analytical description of a particular sulcus
	is obtained by initializing an active model on its trace upon the
	brain surface. The result is a surface representing the buried part
	of the sulcus. The "Snake-spline" model allows one to define an algorithm
	which is simpler and more robust than the classical snake. This method
	of segmentation yields good results for the 3D segmentation and visualization
	of cortical sulci.},
  keywords = {Medical imaging Surface structure Magnetic resonance imaging Image
	analysis Visualization Object recognition Heuristic methods Graph
	theory Geometry Algorithms Image segmentation},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{LEC07.1,
  author = {Christophe Leclercq},
  title = {La resynchronisation cardiaque : indications actuelles et futures},
  journal = {Annales de cardiologie et d'angéiologie},
  year = {2007},
  volume = {56},
  pages = {163-167},
  doi = {doi:10.1016/j.ancard.2007.09.005},
  owner = {euHeart},
  timestamp = {2008.09.10}
}

@ARTICLE{LEC07.2,
  author = {Leclercq, C},
  title = {{Importance of concordance between left ventricular pacing sites
	and latest activated regions: myth or reality?}},
  journal = {Heart},
  year = {2007},
  volume = {93},
  pages = {1170-1172},
  number = {10},
  doi = {10.1136/hrt.2006.108837},
  eprint = {http://heart.bmj.com/cgi/reprint/93/10/1170.pdf},
  owner = {euHeart},
  timestamp = {2008.09.10},
  url = {http://heart.bmj.com}
}

@ARTICLE{LEC07.3,
  author = {Leclercq, C},
  title = {{New guidelines for cardiac resynchronisation therapy: simplicity
	or complexity for the doctor?}},
  journal = {Heart},
  year = {2007},
  volume = {93},
  pages = {1017-1019},
  number = {9},
  abstract = {See article on page 1134},
  doi = {10.1136/hrt.2007.122267},
  eprint = {http://heart.bmj.com/cgi/reprint/93/9/1017.pdf},
  owner = {euHeart},
  timestamp = {2008.09.10},
  url = {http://heart.bmj.com/cgi/content/abstract/93/9/1017}
}

@INCOLLECTION{LEE99,
  author = {Lee, Aaron W. F. and Dobkin, David and Sweldens, Wim and Schr\"oder,
	Peter},
  title = {Multiresolution mesh morphing},
  booktitle = {conference on Computer Graphics and Interactive Techniques},
  publisher = {ACM Press/Addison-Wesley Publishing Co.},
  year = {1999},
  pages = {343-350},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INCOLLECTION{LEE98,
  author = {Lee, Aaron W. F. and Sweldens, Wim and Schr\"oder, Peter and Cowsar,
	Lawrence and Dobkin, David},
  title = {MAPS: multiresolution adaptive parameterization of surfaces},
  booktitle = {Proceedings of the 25th annual conference on Computer graphics and
	interactive techniques},
  publisher = {ACM Press},
  year = {1998},
  pages = {95-104},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{LEE07,
  author = {Jack Lee and Patricia Beighley and Erik Ritman and Nicolas Smith},
  title = {Automatic segmentation of 3D micro-CT coronary vascular images.},
  journal = {Med Image Anal},
  year = {2007},
  volume = {11},
  pages = {630--647},
  number = {6},
  month = {Dec},
  abstract = {Although there are many algorithms available in the literature aimed
	at segmentation and model reconstruction of 3D angiographic images,
	many are focused on characterizing only a part of the vascular network.
	This study is motivated by the recent emerging prospects of whole-organ
	simulations in coronary hemodynamics, autoregulation and tissue oxygen
	delivery for which anatomically accurate vascular meshes of extended
	scale are highly desirable. The key requirements of a reconstruction
	technique for this purpose are automation of processing and sub-voxel
	accuracy. We have designed a vascular reconstruction algorithm which
	satisfies these two criteria. It combines automatic seeding and tracking
	of vessels with radius detection based on active contours. The method
	was first examined through a series of tests on synthetic data, for
	accuracy in reproduced topology and morphology of the network and
	was shown to exhibit errors of less than 0.5 voxel for centerline
	and radius detections, and 3 degrees for initial seed directions.
	The algorithm was then applied on real-world data of full rat coronary
	structure acquired using a micro-CT scanner at 20 microm voxel size.
	For this, a further validation of radius quantification was carried
	out against a partially rescanned portion of the network at 8 microm
	voxel size, which estimated less than 10\% radius error in vessels
	larger than 2 voxels in radius.},
  doi = {10.1016/j.media.2007.06.012},
  institution = {Bioengineering Institute, Faculty of Engineering, The University
	of Auckland, Private Bag 92019, Auckland, New Zealand. cj.lee@auckland.ac.nz},
  keywords = {Algorithms; Animals; Computer Simulation; Coronary Angiography; Coronary
	Vessels, anatomy /&/ histology; Imaging, Three-Dimensional; Radiographic
	Image Interpretation, Computer-Assisted, methods; Rats; Tomography,
	X-Ray Computed},
  language = {eng},
  medline-pst = {ppublish},
  owner = {euHeart},
  pii = {S1361-8415(07)00064-3},
  pmid = {17827050},
  timestamp = {2009.10.16},
  url = {http://dx.doi.org/10.1016/j.media.2007.06.012}
}

@INPROCEEDINGS{LEI91,
  author = {Leitner, F. and Cinquin, P.},
  title = {Dynamic segmentation: Detecting complex topology 3D-object},
  booktitle = {EMBC},
  year = {1991},
  volume = {13},
  pages = {295-296},
  address = {Orlando, FL, USA},
  note = {TY - CONF U1 - 92080561657 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc. U2 - Physiological Imaging
	Image Segmentation Three Dimensional Displays Object Recognition},
  abstract = {The continuous model of image segmentation is used to consider the
	segmenting problem as the evolution of a 3D surface (modeled by spline
	tensorial products) under strengths comparable to those used in 'active
	contours' or 'snakes'. The advantage of this method is that this
	evolution can be modeled with a simple differential system, the variables
	of which are the coefficients of the B-splines representing the frontier.
	Moreover, this method is by nature adaptive: it is easy to control
	the number of B-splines used to represent the frontier. A priori
	knowledge can easily be taken into account if provided under the
	form of a CAD model of the object to be segmented.},
  keywords = {Computer Graphics - Three Dimensional Graphics Computer Vision - Medical
	Applications Image Processing -- Image Analysis},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{LES09,
  author = {David Lesage and Elsa D Angelini and Isabelle Bloch and Gareth Funka-Lea},
  title = {A review of 3D vessel lumen segmentation techniques: models, features
	and extraction schemes.},
  journal = {Med Image Anal},
  year = {2009},
  volume = {13},
  pages = {819--845},
  number = {6},
  month = {Dec},
  abstract = {Vascular diseases are among the most important public health problems
	in developed countries. Given the size and complexity of modern angiographic
	acquisitions, segmentation is a key step toward the accurate visualization,
	diagnosis and quantification of vascular pathologies. Despite the
	tremendous amount of past and on-going dedicated research, vascular
	segmentation remains a challenging task. In this paper, we review
	state-of-the-art literature on vascular segmentation, with a particular
	focus on 3D contrast-enhanced imaging modalities (MRA and CTA). We
	structure our analysis along three axes: models, features and extraction
	schemes. We first detail model-based assumptions on the vessel appearance
	and geometry which can embedded in a segmentation approach. We then
	review the image features that can be extracted to evaluate these
	models. Finally, we discuss how existing extraction schemes combine
	model and feature information to perform the segmentation task. Each
	component (model, feature and extraction scheme) plays a crucial
	role toward the efficient, robust and accurate segmentation of vessels
	of interest. Along each axis of study, we discuss the theoretical
	and practical properties of recent approaches and highlight the most
	advanced and promising ones.},
  doi = {10.1016/j.media.2009.07.011},
  institution = {Siemens Corporate Research, Imaging and Visualization Dept., Princeton,
	NJ, USA.},
  keywords = {Algorithms; Angiography, methods; Artificial Intelligence; Humans;
	Image Enhancement, methods; Image Interpretation, Computer-Assisted,
	methods; Imaging, Three-Dimensional, methods; Pattern Recognition,
	Automated, methods; Reproducibility of Results; Sensitivity and Specificity;
	Subtraction Technique},
  language = {eng},
  medline-pst = {ppublish},
  owner = {euHeart},
  pii = {S1361-8415(09)00067-X},
  pmid = {19818675},
  timestamp = {2010.04.23},
  url = {http://dx.doi.org/10.1016/j.media.2009.07.011}
}

@INPROCEEDINGS{LEU04,
  author = {Leung, C.C. and Chan, C.H. and Chan, F.H.Y. and Tsui, W.K.},
  title = {B-spline snakes in two stages},
  booktitle = {Proceedings of the 17th International Conference on Pattern Recognition,
	ICPR 2004, Aug 23-26 2004},
  year = {2004},
  volume = {1},
  series = {Proceedings - International Conference on Pattern Recognition},
  pages = {568-571},
  address = {Cambridge, United Kingdom},
  publisher = {Institute of Electrical and Electronics Engineers Inc., Piscataway,
	NJ 08855-1331, United States},
  note = {TY - CONF U1 - 04518733061 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc. U2 - B-Spline Snakes
	Minimization curve Elasticity splines Optimal snakes},
  abstract = {In using Snake algorithms, the slow convergence speed is due to the
	large number of control points to be selected, as well as difficulties
	in setting the weighting factors that comprise the internal energies
	of the curve. Even in using the B-Spline snakes, splines cannot be
	fitted into the corner of the object completely. In this paper, a
	novel two-stage method based on B-Spline Snakes is proposed. It is
	superior both in accuracy and fast convergence speed over previous
	B-Spline Snakes. The first stage reduces the number of control points
	using potential function V(x,y) minimization. Hence, it allows the
	spline to quickly approach the minimum energy state. The second stage
	is designed to refine the B-Spline snakes based on the node points
	of the polynomials without knots. In other words, an elasticity spline
	is controlled by node points where knots are fixed. Simulation and
	validation of results are presented. Compared to the traditional
	B-Spline snakes, better performance was achieved using the method
	proposed in this paper.},
  keywords = {Splines Functions Approximation theory Spurious signal noise Polynomials
	Convergence of numerical methods Image analysis},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{LEV00,
  author = {Levy, Bruno and Mallet, Jean-Laurent},
  title = {Param�risation des surfaces triangul�s},
  journal = {International Journal of CADCAM and Computer Graphics},
  year = {2000},
  abstract = {Cet article introduit de nouvelles techniques pour texturer sous contraintes
	des surfaces triangul�s. Les coordonn�s de texture sont affect�s
	aux sommets de la triangulation gr�e �un algorithme it�atif g��al
	permettant de minimiser sous contraintes une fonctionnelle de lissage.
	L'introduction de contraintes permet de minimiser les d�ormations
	de la texture en contr�ant le tenseur m�rique fondamental de la param�risation.
	Il est �alement possible de faire correspondre des d�ails de la texture
	�des ��ents caract�istiques du mod�e �texturer. De plus, notre m�hode
	permet de regrouper sous un formalisme commun certaines m�hodes de
	param�risation connues. En conclusion, l'article pr�ente certaines
	applications possibles dans un contexte autre que celui du plaquage
	de textures, comme le probl�e de l'optimisation de maillages, ou
	encore celui de la conversion de surfaces triangul�s en surfaces
	polyn�iales.},
  keywords = {param�risation, surfaces triangul�s, texture, optimisation de maillages
	parameterization, triangulated Surfaces, texture mapping, mesh optimization},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{LEY93,
  author = {Leymarie, Frederic and Levine, Martin D.},
  title = {Tracking deformable objects in the plane using an active contour
	model},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {1993},
  volume = {15},
  pages = {617-634},
  number = {6},
  note = {TY - JOUR U1 - 93071032467 L2 - http://dx.doi.org/10.1109/34.216733
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Active contour model Vision algorithms Visual
	tracking Object recognition Deformable objects},
  abstract = {In this paper, we consider the problems of segmenting a noisy intensity
	image and tracking a nonrigid object in the plane. In evaluating
	these problems, we explore a new technique based on an active contour
	model commonly called a 'snake.' We have applied this technique to
	cell locomotion and tracking studies. The snake permits us to simultaneously
	solve, in constrained cases, both the segmentation and tracking problems.
	We present a detailed analysis of the snake model, emphasizing its
	limitations and shortcomings, and propose improvements to the original
	description of the model. We discuss in detail how the various parameters
	and forces of the snake model can be selected. We also consider problems
	of convergence of the optimization scheme. In particular, we propose
	an improved terminating criterion for the optimization scheme on
	the basis of topographic features of the graph of the intensity image
	(or potential surface). We also address the problem of how to derive
	useful potential surfaces on which the snake can crawl. Hierarchical
	filtering methods, as well as a continuation method based on a discrete
	scale-space representation, are discussed in this context. Results
	for both segmentation and tracking are presented. Possible failures
	of the method are also discussed.},
  keywords = {Algorithms Image analysis Cell membranes Medical imaging Computer
	vision},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{LI95,
  author = {Li, Bingcheng},
  title = {High-order moment computation of gray-level images},
  journal = IEEE_J_IP,
  year = {1995},
  volume = {4},
  pages = {502--505},
  number = {4},
  abstract = {Describes an efficient approach to calculate geometric moments of
	a 2-D gray-level image. It is shown both theoretically and experimentally
	that the new method compares favorably with previous techniques,
	especially for high-order moments},
  doi = {10.1109/83.370680},
  issn = {1057-7149},
  keywords = {filtering theory, image processing, 2-D gray-level image, filtering,
	geometric moments, gray-level images, high-order moment computation},
  owner = {euHeart},
  timestamp = {2008.09.23}
}

@INPROCEEDINGS{LI06,
  author = {Bing Li and S.T. Acton},
  title = {Vector field convolution for image segmentation using snakes},
  booktitle = {ICIP},
  year = {2006},
  address = {Atlanta, GA, USA},
  abstract = {Snakes, or active contours, have been widely used in image processing
	applications. Typical roadblocks to consistent performance include
	limited capture range, noise sensitivity, and poor convergence to
	concavities. This paper proposes a new design for the snake external
	force, called vector field convolution (VFC), to address these problems.
	Qualitative and quantitative comparisons with the gradient vector
	flow (GVF) external force are presented in this paper to show the
	advantages of this innovation},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{LI07.1,
  author = {Li, Bing and Acton, Scott T.},
  title = {Active contour external force using vector field convolution for
	image segmentation},
  journal = {IEEE Transactions on Image Processing},
  year = {2007},
  volume = { 16},
  pages = {2096 - 2106},
  number = { 8},
  note = {Vector field convolution;Vector field kernel;Active contours;Gradient
	vector flow;},
  abstract = {Snakes, or active contours, have been widely used in image processing
	applications. Typical roadblocks to consistent performance include
	limited capture range, noise sensitivity, and poor convergence to
	concavities. This paper proposes a new external force for active
	contours, called vector field convolution (VFC), to address these
	problems. VFC is calculated by convolving the edge map generated
	from the image with the user-defined vector field kernel. We propose
	two structures for the magnitude function of the vector field kernel,
	and we provide an analytical method to estimate the parameter of
	the magnitude function. Mixed VFC is introduced to alleviate the
	possible leakage problem caused by choosing inappropriate parameters.
	We also demonstrate that the standard external force and the gradient
	vector flow (GVF) external force are special cases of VFC in certain
	scenarios. Examples and comparisons with GVF are presented in this
	paper to show the advantages of this innovation, including superior
	noise robustness, reduced computational cost, and the flexibility
	of tailoring the force field. &copy; 2007 IEEE.},
  copyright = {Compilation and indexing terms, Copyright 2008 Elsevier Inc.},
  issn = {1057-7149},
  key = {Image segmentation},
  keywords = {Image enhancement;Image quality;Numerical methods;Vector quantization;},
  language = {English},
  timestamp = {2008.01.23},
  url = {http://dx.doi.org/10.1109/TIP.2007.899601}
}

@INPROCEEDINGS{LI07.2,
  author = {Li, Jie and Regli, William C. and Sun, Wei},
  title = {Mathematical representation of the vascular structure and applications},
  booktitle = {SPM '07: Proceedings of the 2007 ACM symposium on Solid and physical
	modeling},
  year = {2007},
  pages = {373--378},
  address = {New York, NY, USA},
  publisher = {ACM},
  abstract = {The complexity and diversity of the vascular system makes it difficult
	to perform quantitative blood vessel modeling and analysis. Emerging
	applications in biomedical modeling, bio-mimetic manufacturing and
	tissue engineering necessitate the development of new approaches
	to computational modeling of vascular structures. In this paper,
	we address the computational representation of realistic vascular
	objects to enable deformable blood vessels. First, we construct a
	swept volume based vascular model from a physiological vascular model.
	Second, we construct the NURBS surface based on deformable circles,
	i.e., the primitive of swept volume, as the boundary of the swept
	volumes. The approach of NURBS surface construction can also be extended
	to reconstruct medical image based vasclar models. In the end, potential
	applications of the mathematical representation of vascular structure
	including blood flow simulation in scaffold are discussed. As a computational
	representation, our vascular model enables multiscale visualization
	of a realistic vascular system. The work in this paper presents the
	initial steps toward creating computational models suitable for high-fidelity
	vascular modeling, design and analysis. The authors believe that
	improved computational modeling for biological structures will enable
	new applications in tissues engineering and biological analysis.},
  doi = {http://doi.acm.org/10.1145/1236246.1236300},
  isbn = {978-1-59593-666-0},
  location = {Beijing, China},
  owner = {euHeart},
  timestamp = {2009.10.14}
}

@INPROCEEDINGS{LI99,
  author = {Li, Xiaobo and Wang, Jiankang},
  title = {Adaptive balloon models},
  booktitle = {Computer Vision and Pattern Recognition},
  year = {1999},
  volume = { 2},
  pages = {434 - 439},
  address = {Fort Collins, CO, USA},
  publisher = {IEEE},
  note = {Active contour models;Adaptive balloon models;},
  abstract = {The original snake models require a close initialization which in
	many situations are difficult to acquire. The balloon model presented
	by Cohen et al. to solve this problem suffers from the difficulty
	of choosing a constant inflating force due to variable internal shrinking
	forces and non-constant boundary intensity levels. Xu el al. on the
	other hand, proposed to use a pressure force to exactly offset the
	shrinking forces. The resulting model achieves better stability in
	terms of parameter insensitivity by sacrificing smoothness constraints,
	thus it would go through even small gaps on a boundary. We instead
	propose to compute an adaptive inflating force locally for each snaxel
	so that it is just enough to overcome the image force. A new smoothness
	constraint which can maintain smoothness without any shrinking side-effects
	is also presented, along with a new way to resample a balloon without
	significantly reducing its tension. The combined model is sensitive
	to weak and incomplete boundaries, and yet able to overcome noise
	edges. Experimental results are reported to support our statements.},
  copyright = {Compilation and indexing terms, Copyright 2007 Elsevier Inc. All rights
	reserved},
  issn = {1063-6919},
  journal = {Conference on Computer Vision and Pattern Recognition},
  key = {Feature extraction},
  keywords = {Image enhancement;Mathematical models;},
  language = {English},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{LIA99.1,
  author = {Liang, Jianming and McInerney, Tim and Terzopoulos, Demetri},
  title = {United Snakes},
  booktitle = {ICCV},
  year = {1999},
  volume = {2},
  pages = {933},
  publisher = {IEEE Computer Society},
  abstract = {Since their debut in 1987, snakes (active contour models) have become
	a standard image analysis technique with several variants now in
	common use. We present a portable, reusable, software package called
	“United Snakes�. The package unites the most popular snake variants,
	including finite difference, B-spline, and Hermite polynomial snakes
	within the mathematical framework of a general finite element formulation
	with a choice of shape functions. The package furthermore incorporates
	a recently proposed snake-like technique known as “livewire�. We
	integrate snakes and livewire by introducing an effective method
	for imposing hard constraints on snakes. Our experiments demonstrate
	that snakes and livewire have complementary strengths and that their
	union offers a more powerful tool for interactive image analysis,
	especially for medical imaging applications. United Snakes is implemented
	in Java as a JavaBean so that it can easily be integrated in end-user
	application systems.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{LIA99.2,
  author = {Liao, Chia-Wei and Medioni, Gerard},
  title = {Simultaneous surface approximation and segmentation of complex objects},
  journal = {Computer Vision and Image Understanding},
  year = {1999},
  volume = {73},
  pages = {43-63},
  number = {1},
  note = {TY - JOUR U1 - 99024554438 L2 - http://dx.doi.org/10.1006/cviu.1998.0694
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Bezier surface},
  abstract = {Deformable models represent a useful approach to approximate objects
	from collected data points. We propose to augment the basic approaches
	designed to handle mostly compact objects or objects of known topology.
	Our approach can fit simultaneously more than one curve or surface
	to approximate multiple topologically complex objects by using (1)
	the residual data points, (2) the badly fitting parts of the approximating
	surface, and (3) appropriate Boolean operations. In 2-D, B-snakes
	are used to approximate each object (pattern). In 3-D, an analytical
	surface representation, based on the elements detected, is presented.
	The global representation of a 3-D object, in terms of elements and
	their connection, takes the form of B-spline and Bezier surfaces.
	A Bezier surface is used to connect different elements, and the connecting
	surface itself conforms to the data points nearby through energy
	minimization. This way, a G<sup>1</sup> continuity surface is achieved
	for the underlying 3-D object. We present experiments on synthetic
	and real data in 2-D and 3-D. In these experiments, multiple complex
	patterns and objects with through boles are segmented. The system
	proceeds automatically without human interaction or any prior knowledge
	of the topology of the underlying object.},
  keywords = {Mathematical models Curve fitting Topology Approximation theory Boolean
	algebra Image segmentation},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{LIA95,
  author = {Liao, Chia-Wei and Medioni, Gerard},
  title = {Surface approximation of a cloud of 3D points},
  journal = {Graphical Models and Image Processing},
  year = {1995},
  volume = {57},
  pages = {67-74},
  number = {1},
  note = {TY - JOUR U1 - 95032635118 L2 - http://dx.doi.org/10.1006/gmip.1995.1007
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Powell algorithm Nonconvex optimization problem},
  abstract = {Deformable models are used to approximate a three-dimensional surface
	represented by a cloud of three dimensional points. The models are
	presented in the form of a nonconvex optimization problem by means
	of the Powell algorithm which ensures convergence and does not require
	gradient information. The number of control points processed by the
	algorithm at one time is controlled, leading to reasonable complexity,
	robustness, and good numerical stability. The proposed approach provides
	a compact representation of the approximated data and is suitable
	for such applications as motion tracking and object recognition.},
  keywords = {Pattern recognition Mathematical models Algorithms Optimization Convergence
	of numerical methods Three dimensional computer graphics Parameter
	estimation Iterative methods Data reduction Image understanding Image
	processing},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{LIT03,
  author = {Andrew Litvin},
  title = {Levelset Based Segmentation Using Data Driven Shape Prior on Feature
	Histograms},
  booktitle = {Workshop on Statistical Signal Processing},
  year = {2003},
  abstract = {We develop a new data driven shape prior for use in image segmentation.
	This prior is designed to penalize the differences between the distributions
	of contour features obtained from training data shapes and those
	of a segmenting curve. We incorporate this prior into a level set
	segmentation framework and present an efficient method for its implementation.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{LIU07,
  author = {Xin Liu and Xihai Zhao and Jie Huang and Christopher J Francois and
	David Tuite and Xiaoming Bi and Debiao Li and James C Carr},
  title = {Comparison of 3D free-breathing coronary MR angiography and 64-MDCT
	angiography for detection of coronary stenosis in patients with high
	calcium scores.},
  journal = {AJR Am J Roentgenol},
  year = {2007},
  volume = {189},
  pages = {1326--1332},
  number = {6},
  month = {Dec},
  abstract = {OBJECTIVE: The objective of our study was to compare the diagnostic
	performance of coronary MR angiography (MRA) and 64-MDCT angiography
	(MDCTA) for the detection of significant stenosis (> or = 50\%) in
	patients with high calcium scores. MATERIALS AND METHODS: Eighteen
	patients (12 men, six women; mean age, 56 y; age range, 38-77 y)
	who had at least one calcified plaque with a calcium score of > 100
	underwent coronary MRA and conventional coronary angiography (CAG)
	within 2 weeks of MDCTA. Coronary MRA image quality of the calcified
	segments was assessed by two observers in consensus on a 4-point
	scale (1 = not visible, 2 = poor, 3 = good, 4 = excellent) using
	a 10-segment model from the modified American Heart Association classification.
	Three experienced radiologists, unaware of the results of conventional
	CAG, independently assessed for the presence of significant stenosis
	on MDCTA images and the corresponding MRA images. Receiver operating
	characteristic (ROC) curves were calculated for each reader using
	conventional CAG as the gold standard. RESULTS: Thirty-three calcified
	plaques with a calcium score of > 100 were detected on MDCTA in the
	18 patients. The coronary segments with nodal calcification (n =
	17) showed a higher mean image quality score than the segments with
	diffuse calcification (n = 16) (3.47 +/- 0.62 vs 2.94 +/- 0.77, respectively;
	p < 0.05). Of the 33 coronary segments with calcification, 12 significant
	stenoses were identified on conventional CAG. The sensitivity, specificity,
	and area under the ROC curve (AUC) for MRA and MDCTA, respectively,
	were as follows: reader 1, 75\%, 81\%, 0.82 versus 75\%, 48\%, 0.68;
	reader 2, 83\%, 71\%, 0.82 versus 67\%, 52\%, 0.63; and reader 3,
	83\%, 71\%, 0.85 versus 83\%, 43\%, 0.65, respectively. The average
	AUC of MRA for the three readers was significantly higher than that
	of MDCTA (p = 0.030). CONCLUSION: Coronary MRA has higher image quality
	for coronary segments with nodal calcification than for coronary
	segments with diffuse calcification. Coronary MRA has better diagnostic
	performance than coronary MDCTA for the detection of significant
	stenosis in patients with high calcium scores.},
  doi = {10.2214/AJR.07.2805},
  institution = {Department of Radiology, Northwestern University, 448 E Ontario St.,
	Ste. 700, Chicago, IL 60611, USA.},
  keywords = {Adult; Aged; Angiography, methods; Artifacts; Calcinosis, complications/diagnosis;
	Coronary Stenosis, diagnosis/etiology; Female; Humans; Imaging, Three-Dimensional,
	methods; Magnetic Resonance Imaging, methods; Male; Middle Aged;
	Reproducibility of Results; Respiratory Mechanics; Sensitivity and
	Specificity; Tomography, X-Ray Computed, methods},
  owner = {euHeart},
  pii = {189/6/1326},
  pmid = {18029867},
  timestamp = {2009.02.16},
  url = {http://dx.doi.org/10.2214/AJR.07.2805}
}

@ARTICLE{LLO09,
  author = {Donald Lloyd-Jones and Robert Adams and Mercedes Carnethon and Giovanni
	De Simone and T. Bruce Ferguson and Katherine Flegal and Earl Ford
	and Karen Furie and Alan Go and Kurt Greenlund and Nancy Haase and
	Susan Hailpern and Michael Ho and Virginia Howard and Brett Kissela
	and Steven Kittner and Daniel Lackland and Lynda Lisabeth and Ariane
	Marelli and Mary McDermott and James Meigs and Dariush Mozaffarian
	and Graham Nichol and Christopher O'Donnell and Veronique Roger and
	Wayne Rosamond and Ralph Sacco and Paul Sorlie and Randall Stafford
	and Julia Steinberger and Thomas Thom and Sylvia Wasserthiel-Smoller
	and Nathan Wong and Judith Wylie-Rosett and Yuling Hong and American
	Heart Association Statistics Committee and Stroke Statistics Subcommittee},
  title = {Heart disease and stroke statistics -- 2009 update: a report from
	the American Heart Association Statistics Committee and Stroke Statistics
	Subcommittee.},
  journal = {Circulation},
  year = {2009},
  volume = {119},
  pages = {480--486},
  number = {3},
  month = {Jan},
  owner = {euHeart},
  pmid = {19171871},
  timestamp = {2009.02.16}
}

@INPROCEEDINGS{LOO94,
  author = {Loop, Charles},
  title = {Smooth Spline Surfaces over Irregular Meshes.},
  booktitle = {SIGGRAPH},
  year = {1994},
  pages = {303-310},
  abstract = {An algorithm for creating smooth spline surfaces over irregular meshes
	is presented. The algorithm is a generalization of quadratic B-splines;
	that is, if a mesh is (locally) regular, the resulting surface is
	equivalent to a B-spline. Otherwise, the resulting surface has a
	degree 3 or 4 parametric polynomial representation. A construction
	is given for representing the surface as a collection of tangent
	plane continuous triangular Bézier patches. The algorithm is simple,
	efficient, and generates aesthetically pleasing shapes.},
  keywords = {Spline Approximation, Computational Geometry and Object Modeling,
	Computer-Aided Design.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{LOR06,
  author = {Cristian Lorenz and Jens von Berg},
  title = {A comprehensive shape model of the heart.},
  journal = {Med Image Anal},
  year = {2006},
  volume = {10},
  pages = {657--670},
  number = {4},
  month = {Aug},
  abstract = {Domain knowledge about the geometrical properties of cardiac structures
	is an important ingredient for the segmentation of these structures
	in medical images or for the simulation of cardiac physiology. So
	far, a strong focus was put on the left ventricle due to its importance
	for the general pumping performance of the heart and related functional
	indices. However, other cardiac structures are of similar importance,
	e.g., the coronary arteries with respect to diagnosis and treatment
	of arteriosclerosis or the left atrium with respect to the treatment
	of atrial fibrillation. In this paper we describe the generation
	of a geometric cardiac model including the four cardiac chambers
	and the trunks of the connected vasculature, as well as the coronary
	arteries and a set of cardiac landmarks. A mean geometric model for
	the end-diastolic heart has been built based on 27 cardiac CT datasets
	and has been evaluated with respect to its capability to estimate
	the position of cardiac structures. Allowing a similarity transformation
	to adapt the model to image data, cardiac surface positions can be
	predicted with an accuracy of below 5mm.},
  doi = {10.1016/j.media.2006.03.004},
  institution = {Philips Research Laboratories, Sector Technical Systems, Röntgenstrasse
	24-26, P.O. Box 63 05 65, D-22335 Hamburg, Germany. Cristian.Lorenz@philips.com},
  keywords = {Computer Simulation; Heart, anatomy /&/ histology/radiography; Humans;
	Imaging, Three-Dimensional; Models, Anatomic; Models, Cardiovascular;
	Radiographic Image Interpretation, Computer-Assisted, methods},
  owner = {euHeart},
  pii = {S1361-8415(06)00016-8},
  pmid = {16709463},
  timestamp = {2009.05.19},
  url = {http://dx.doi.org/10.1016/j.media.2006.03.004}
}

@ARTICLE{LU02,
  author = {Lu, Aidong and Tang, Long and Xu, Yuhua and Tang, Zesheng},
  title = {Semiautomatic ultrasound image segmentation},
  journal = {Qinghua Daxue Xuebao/Journal of Tsinghua University},
  year = {2002},
  volume = {42},
  pages = {1385-1388},
  number = {10},
  note = {TY - JOUR U1 - 03037327838 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc. U2 - Spline snakes User
	interaction Force field},
  abstract = {This article presents an algorithm of semiautomatic segmentation of
	ultrasound images combining user intervention into the B-spline snake
	models using a new kind of force field and a new control point insertion
	strategy. The statistical parameters of the snake models are trained
	on-the-fly by studying the user intervention to achieve a satisfactory
	segmentation result. The resulting algorithm is especially useful
	for dealing with successive slices which need human interaction to
	provide fast, reliable and verifiable segmentation of ultrasound
	images.},
  keywords = {Ultrasonic imaging Mathematical models Image processing Image segmentation},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{LU01,
  author = {Lu, Ai-Dong and Tang, Long and Xu, Yu-Hua and Tang, Ze-Sheng},
  title = {Segmentation of ultrasound images with interactive B-spline snakes
	and its application},
  journal = {Ruan Jian Xue Bao/Journal of Software},
  year = {2001},
  volume = {12},
  pages = {1760-1768},
  number = {12},
  note = {TY - JOUR U1 - 02196945402 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc. U2 - Spline snakes B
	User interaction Surgical simulation Ultrasound images},
  abstract = {Due to the poor quality of ultrasound images, fully automatic segmentation
	methods are not feasible. This article describes a novel approach
	to the semiautomatic segmentation of ultrasound images. Although
	user interaction is not required much, it is used as an important
	factor and incorporated into the traditional B-spline snake models.
	The modified model is called an interactive B-snake model because
	the movement of the active contour is constrained through user interaction.
	By introducing a set of moving rules, B-spline segments are moved
	to the desired boundary directly. The statistical models are trained
	on-the-fly by observing boundaries accepted by the user. The resulting
	algorithm is especially useful when dealing with successive slices
	and provides fast, reliable and verifiable segmentation in ultrasound
	images. The algorithm has been used successfully on the Liver Tumor
	Surgical Simulation System.},
  keywords = {Ultrasonic imaging Mathematical models Image processing Edge detection
	Computer simulation Image segmentation},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{LUE07,
  author = {Luengo-Oroz, M. and Ledesma-Carbayo, M. and G\'omez-Diego, J. and
	Garc\'ia-Fern\'andez, M. and Desco, M. and Santos, A.},
  title = {Extraction of the Coronary Artery Tree in Cardiac Computer Tomographic
	Images Using Morphological Operators},
  booktitle = {FIMH},
  year = {2007},
  pages = {424--432},
  abstract = {Quantitative measurements of the coronary artery tree substructures
	from Cardiac Multislice-CT data sets is an important goal to improve
	the diagnosis and treatment of coronary artery disease. This paper
	presents an algorithm based on morphological grayscale reconstruction
	through 2D slice images devoted to the extraction of the 3D coronary
	artery tree. The proposed procedure is conceived as a first step
	prior to the segmentation and inspection of interesting substructures
	in the coronaries. The correct extraction of the left coronary arteries
	has been validated in 9 CT-datasets with satisfactory results, particularly
	concerning speed and robustness.},
  journal = {Functional Imaging and Modeling of the Heart},
  owner = {euHeart},
  timestamp = {2009.05.20},
  url = {http://dx.doi.org/10.1007/978-3-540-72907-5_43}
}

@ARTICLE{LUO93,
  author = {Luo, L. M. and Hamitouche, C. and Dillenseger, J. L. and Coatrieux,
	J. L.},
  title = {A moment-based three-dimensional edge operator},
  journal = {IEEE Transactions on Biomedical Engineering},
  year = {1993},
  volume = {40},
  pages = {693--703},
  number = {7},
  abstract = {A three-dimensional edge operator for detecting anatomical structures
	in medical imaging is presented. It uses the spatial moments of the
	gray-level surface, and operates in three dimensions with any window
	size. It allows the location and the contrast surface, as well as
	the surface orientation, to be estimated. The computation of the
	discrete version is reported. Bias and errors due to the spatial
	sampling and noise are analyzed at both a theoretical and experimental
	level. The moment-based operator is compared with other well-known
	edge operators using simple shaped primitives for which the analytical
	solution is known. The 3-D rendering of real data is then provided
	by merging the operator in a ray-tracing framework.<<ETX>>},
  doi = {10.1109/10.237699},
  issn = {0018-9294},
  keywords = {medical image processing, 3D rendering, anatomical structures detection,
	contrast surface, gray-level surface, medical imaging, moment-based
	3D edge operator, noise, ray-tracing framework, spatial moments,
	spatial sampling, surface orientation},
  owner = {euHeart},
  timestamp = {2008.09.15}
}

@INPROCEEDINGS{LUO03,
  author = {Luo, S. and Li, R. and Ourselin, S.},
  title = {A New Deformable Model Using Dynamic Gradient Vector Flow and Adaptive
	Balloon Forces},
  booktitle = {APRS Workshop on Digital Image Computing},
  year = {2003},
  editor = {Lovell, B.},
  address = {Brisbane, Australia},
  month = {February},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{LYN00,
  author = {Lynch, John A. and Zaim, Souhil and Zhao, Jenny and Stork, Alexander
	and Peterfy, Charles G. and Genant, Harry K.},
  title = {Cartilage segmentation of 3D MRI scans of the osteoarthritic knee
	combining user knowledge and active contours},
  booktitle = {Medical Imaging 2000},
  year = {2000},
  volume = {3979},
  pages = {925-935},
  address = {San Diego, CA, USA},
  note = {TY - CONF U1 - 00085260781 L2 - http://dx.doi.org/10.1117/12.387758
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Image registration},
  abstract = {A technique for segmentation of articular cartilage from 3D MRI scans
	of the knee has been developed. It overcomes the limitations of the
	conventionally used region growing techniques, which are prone to
	inter- and intra- observer variability, and which can require much
	manual intervention. We describe a hybrid segmentation method combining
	expert knowledge with directionally oriented Canny filters, cost
	functions and cubic splines. After manual initialization, the technique
	utilized 3 cost functions which aided automated detection of cartilage
	and its boundaries. Using the sign of the edge strength, and the
	local direction of the boundary, this technique is more reliable
	than conventional `snakes', and the user had little control over
	smoothness of boundaries. This means that the automatically detected
	boundary can conform to the true shape of the real boundary, also
	allowing reliable detection of subtle local lesions on the normally
	smooth cartilage surface. Manual corrections, with possible re-optimization
	were sometimes needed. When compared to the conventionally used region
	growing techniques, this newly described technique measured local
	cartilage volume with 3 times better reproducibility, and involved
	two thirds less human interaction. Combined with the use of 3D image
	registration, the new technique should also permit unbiased segmentation
	of followup scans by automated initialization from a baseline segmentation
	of an earlier scan of the same patient.},
  keywords = {Magnetic resonance imaging Image segmentation Cartilage Edge detection
	Medical imaging},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{MA03,
  author = {Ma, Bo and Zhang, Tianwen and Li, Peihua},
  title = {HMM-based Kalman snake for contour tracking},
  journal = {Jisuanji Fuzhu Sheji Yu Tuxingxue Xuebao/Journal of Computer-Aided
	Design and Computer Graphics},
  year = {2003},
  volume = {15},
  pages = {1236-1241},
  number = {10},
  note = {TY - JOUR U1 - 04037822708 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc. U2 - Kalman snake B spline
	curve Hidden Markov model Active contour model},
  abstract = {Hidden Markov model (HMM) provides a powerful probabilistic mechanism
	to incorporate multiple image cues, and can encode curve smoothness
	constraint in transition probabilities, therefore can be used to
	obtain more accurate measurement. Using HMM, the processing result
	is input into the Kalman snake filtering system as new measurement
	information, which can enhance anti-jamming capacity and tracking
	robustness of the filtering system. In the light of new inner product
	and norm definition of spline vectors, the normalization of shape
	matrix can furthermore improve the stability of filtering system
	and increase the system controllability of the model and parameters.},
  keywords = {Feature extraction Kalman filtering Markov processes Mathematical
	models Image processing Contour measurement},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{MAI04,
  author = {David Maintz and Franz C Aepfelbacher and Kraig V Kissinger and René
	M Botnar and Peter G Danias and Walter Heindel and Warren J Manning
	and Matthias Stuber},
  title = {Coronary MR angiography: comparison of quantitative and qualitative
	data from four techniques.},
  journal = {AJR Am J Roentgenol},
  year = {2004},
  volume = {182},
  pages = {515--521},
  number = {2},
  month = {Feb},
  abstract = {OBJECTIVE: The optimal coronary MR angiography sequence has yet to
	be determined. We sought to quantitatively and qualitatively compare
	four coronary MR angiography sequences. SUBJECTS AND METHODS. Free-breathing
	coronary MR angiography was performed in 12 patients using four imaging
	sequences (turbo field-echo, fast spin-echo, balanced fast field-echo,
	and spiral turbo field-echo). Quantitative comparisons, including
	signal-to-noise ratio, contrast-to-noise ratio, vessel diameter,
	and vessel sharpness, were performed using a semiautomated analysis
	tool. Accuracy for detection of hemodynamically significant disease
	(> 50\%) was assessed in comparison with radiographic coronary angiography.
	RESULTS: Signal-to-noise and contrast-to-noise ratios were markedly
	increased using the spiral (25.7 +/- 5.7 and 15.2 +/- 3.9) and balanced
	fast field-echo (23.5 +/- 11.7 and 14.4 +/- 8.1) sequences compared
	with the turbo field-echo (12.5 +/- 2.7 and 8.3 +/- 2.6) sequence
	(p < 0.05). Vessel diameter was smaller with the spiral sequence
	(2.6 +/- 0.5 mm) than with the other techniques (turbo field-echo,
	3.0 +/- 0.5 mm, p = 0.6; balanced fast field-echo, 3.1 +/- 0.5 mm,
	p < 0.01; fast spin-echo, 3.1 +/- 0.5 mm, p < 0.01). Vessel sharpness
	was highest with the balanced fast field-echo sequence (61.6\% +/-
	8.5\% compared with turbo field-echo, 44.0\% +/- 6.6\%; spiral, 44.7\%
	+/- 6.5\%; fast spin-echo, 18.4\% +/- 6.7\%; p < 0.001). The overall
	accuracies of the sequences were similar (range, 74\% for turbo field-echo,
	79\% for spiral). Scanning time for the fast spin-echo sequences
	was longest (10.5 +/- 0.6 min), and for the spiral acquisitions was
	shortest (5.2 +/- 0.3 min). CONCLUSION: Advantages in signal-to-noise
	and contrast-to-noise ratios, vessel sharpness, and the qualitative
	results appear to favor spiral and balanced fast field-echo coronary
	MR angiography sequences, although subjective accuracy for the detection
	of coronary artery disease was similar to that of other sequences.},
  institution = {Department of Medicine, Beth Israel Deaconess Medical Center, Harvard
	Medical School, Boston, MA, USA. maintz@uni-muenster.de},
  keywords = {Aged; Coronary Angiography, methods; Coronary Artery Disease, radiography;
	False Negative Reactions; False Positive Reactions; Female; Humans;
	Imaging, Three-Dimensional; Magnetic Resonance Angiography, methods;
	Male; Middle Aged; Prospective Studies; Radiographic Image Enhancement,
	methods; Reproducibility of Results; Signal Processing, Computer-Assisted},
  language = {eng},
  medline-pst = {ppublish},
  owner = {euHeart},
  pmid = {14736693},
  timestamp = {2009.09.14}
}

@INPROCEEDINGS{MAN06,
  author = {Manniesing, R. and Niessen, W. },
  title = {Shape constrained vessel centerline extraction by integrating surface
	evolution and topology analysis},
  booktitle = {Proc. 3rd IEEE International Symposium on Biomedical Imaging: Nano
	to Macro},
  year = {2006},
  pages = {165--168},
  abstract = {A novel approach for vessel axis tracking is presented based on surface
	evolution in 3D. The main idea is to guide the evolution by analyzing
	the topology of intermediate segmentation results, and in particular,
	to impose shape constraints on the topology. For example, the topology
	can be constrained to represent a bifurcation, which can be imposed
	by extracting three different connected paths with maximum length
	from the skeleton of intermediate segmentation results. The evolving
	surface is then re-initialized with the newly found topology. Re-initialization
	is a crucial step since it creates probing behavior of the evolving
	front and prevents the surface from leaking into the background.
	The method was evaluated in two CTA applications (i) extracting the
	internal carotid arteries including the region in which they traverse
	through the skull base, which is challenging due to the close proximity
	of bone structures and overlap in intensity values, and (ii) extracting
	the carotid bifurcation, some of them severely stenosed and most
	of them containing calcifications. Using only the image gradient
	as the image force in the surface evolution and a single seed point
	as initialization, the method was successful in 80% of ten internal
	carotids in five patients, and 80% of ten carotid bifurcations in
	five patients, respectively.},
  doi = {10.1109/ISBI.2006.1624878},
  keywords = {bifurcation, blood vessels, computerised tomography, diagnostic radiography,
	image segmentation, medical image processing, CTA, bone structures,
	calcifications, carotid bifurcation, image segmentation, internal
	carotid arteries, shape constrained vessel centerline extraction,
	shape constraints, skull base, surface evolution, topology analysis,
	vessel axis tracking},
  owner = {euHeart},
  timestamp = {2008.09.24}
}

@ARTICLE{MAN05,
  author = {Rashindra Manniesing and Wiro Niessen},
  title = {Multiscale vessel enhancing diffusion in CT angiography noise filtering.},
  journal = {Inf Process Med Imaging},
  year = {2005},
  volume = {19},
  pages = {138--149},
  abstract = {Filtering of vessel structures in medical images by analyzing the
	second order information or the Hessian of the image, is a well known
	technique. In this work we incorporate Frangi's multiscale vessel
	filter, which is based on a geometrical analysis of the Hessian'
	eigenvectors, into a nonlinear, anisotropic diffusion scheme, such
	that diffusion mainly takes place along the vessel axis while diffusion
	perpendicular to this axis is inhibited. The multiscale character
	of the vesselness filter ensures an equally good response for varying
	vessel radii. The first, theoretical contribution of this paper is
	the modification of the original formulation of this vessel filter,
	such that it becomes a smooth function on its domain which is a necessary
	condition imposed by the diffusion process to ensure well-posedness.
	The second contribution concerns the application of noise filtering
	of 3D synthetic, phantom computed tomography (CT) and patient CT
	data. It is shown that the method is very effective in noise filtering,
	illustrating its potential as a preprocessing step in the analysis
	of low dose CT angiography.},
  institution = {Image Sciences Institute, University Medical Center Utrecht, P.O.
	Box 85500, 3508 GA, Utrecht, The Netherlands. rashindra@isi.uu.nl},
  keywords = {Algorithms; Angiography, instrumentation/methods; Humans; Imaging,
	Three-Dimensional, methods; Models, Biological; Models, Statistical;
	Phantoms, Imaging; Radiographic Image Enhancement, methods; Radiographic
	Image Interpretation, Computer-Assisted, methods; Reproducibility
	of Results; Sensitivity and Specificity; Tomography, X-Ray Computed,
	instrumentation/methods},
  language = {eng},
  medline-pst = {ppublish},
  owner = {euHeart},
  pmid = {17354691},
  timestamp = {2009.10.14}
}

@ARTICLE{MAN07,
  author = {Manniesing, R. and Viergever, M. A. and Niessen, W. J.},
  title = {Vessel Axis Tracking Using Topology Constrained Surface Evolution},
  journal = {IEEE Transactions on Medical Imaging},
  year = {2007},
  volume = {26},
  pages = {309--316},
  number = {3},
  abstract = {An approach to 3-D vessel axis tracking based on surface evolution
	is presented. The main idea is to guide the evolution of the surface
	by analyzing its skeleton topology during evolution, and imposing
	shape constraints on the topology. For example, the intermediate
	topology can be processed such that it represents a single vessel
	segment, a bifurcation, or a more complex vascular topology. The
	evolving surface is then reinitialized with the newly found topology.
	Reinitialization is a crucial step since it creates probing behavior
	of the evolving front, encourages the segmentation process to extract
	the vascular structure of interest and reduces the risk on leaking
	of the curve into the background. The method was evaluated in two
	computed tomography angiography applications: 1) extracting the internal
	carotid arteries including the region in which they traverse through
	the skull base, which is challenging due to the proximity of bone
	structures and overlap in intensity values; 2) extracting the carotid
	bifurcations including many cases in which they are severely stenosed
	and contain calcifications. The vessel axis was found in 90% (18/20
	internal carotids in ten patients) and 70% (14/20 carotid bifurcations
	in a different set of ten patients) of the cases},
  doi = {10.1109/TMI.2006.891503},
  issn = {0278-0062},
  keywords = {bifurcation, blood vessels, computerised tomography, diagnostic radiography,
	feature extraction, image segmentation, medical image processing,
	bone structure proximity, carotid bifurcations, complex vascular
	topology, computed tomography angiography, image segmentation, internal
	carotid artery, reinitialization, shape constraints, single vessel
	segment, skeleton topology, topology constrained surface evolution,
	vessel axis tracking, Carotid artery, computed tomography angiography
	(CTA), skull base, surface evolution, topology, vessel tracking},
  owner = {euHeart},
  timestamp = {2008.09.24}
}

@ARTICLE{MAR02,
  author = {Martinez, J. and Thomas, F.},
  title = {Efficient computation of local geometric moments},
  journal = IEEE_J_IP,
  year = {2002},
  volume = {11},
  pages = {1102--1111},
  number = {9},
  abstract = {Local moments have attracted attention as local features in applications
	such as edge detection and texture segmentation. The main reason
	for this is that they are inherently integral-based features, so
	that their use reduces the effect of uncorrelated noise. The computation
	of local moments, when viewed as a neighborhood operation, can be
	interpreted as a convolution of the image with a set of masks. Nevertheless,
	moments computed inside overlapping windows are not independent and
	convolution does not take this fact into account. By introducing
	a matrix formulation and the concept of accumulation moments, this
	paper presents an algorithm which is computationally much more efficient
	than convolving and yet as simple.},
  doi = {10.1109/TIP.2002.802532},
  issn = {1057-7149},
  keywords = {convolution, edge detection, image processing, image segmentation,
	image texture, matrix algebra, accumulation moments, computationally
	efficient algorithm, edge detection, image analysis, image convolution,
	integral-based features, local features, local geometric moments
	computation, matrix formulation, neighborhood operation, overlapping
	windows, texture segmentation},
  owner = {euHeart},
  timestamp = {2008.09.23}
}

@INPROCEEDINGS{MCI03,
  author = {McInerney, Tira and Dehmeshki, Hoda},
  title = {User-defined B-spline template-snakes},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention, MICCAI
	2003 - 6th International Conference Proceedings, Nov 15-18 2003},
  year = {2003},
  volume = {2879},
  series = {Lecture Notes in Computer Science},
  pages = {746-753},
  address = {Montreal, Que., Canada},
  publisher = {Springer Verlag, Heidelberg, D-69121, Germany},
  note = {TY - CONF U1 - 05028787073 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc. U2 - B-spline template-snakes
	User initialization process Control polygons Noisy images},
  abstract = {We combine a new user initialization process with a B-spline snake
	to create a model with the properties of a deformable template. This
	'template' snake can be constrained by its control polygon and is
	initially extremely close to, and similar in shape to, the target
	anatomical structure, The initialization process acts as almost a
	pre-segmentation and labelling step, making the snake's task much
	simpler and hence more likely to succeed in noisy images without
	subsequent user editing. By imposing an order on the initialization
	process, the user is able to transfer knowledge of global shape,
	symmetry, landmark position etc. to the model. We apply our snake
	to the segmentation of 2D medical images.},
  keywords = {Image segmentation Computer vision Data processing Robustness (control
	systems) Image analysis Mechanisms Finite element method Graphic
	methods Optimization Medical imaging},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{MCI02,
  author = {Tim McInerney and Ghassan Hamarneh and Martha Shenton and Demetri
	Terzopoulos},
  title = {Deformable organisms for automatic medical image analysis.},
  journal = {Med Image Anal},
  year = {2002},
  volume = {6},
  pages = {251--266},
  number = {3},
  month = {Sep},
  abstract = {We introduce a new approach to medical image analysis that combines
	deformable model methodologies with concepts from the field of artificial
	life. In particular, we propose "deformable organisms", autonomous
	agents whose task is the automatic segmentation, labeling, and quantitative
	analysis of anatomical structures in medical images. Analogous to
	natural organisms capable of voluntary movement, our artificial organisms
	possess deformable bodies with distributed sensors, as well as (rudimentary)
	brains with motor, perception, behavior, and cognition centers. Deformable
	organisms are perceptually aware of the image analysis process. Their
	behaviors, which manifest themselves in voluntary movement and alteration
	of body shape, are based upon sensed image features, pre-stored anatomical
	knowledge, and a deliberate cognitive plan. We demonstrate several
	prototype deformable organisms based on a multiscale axisymmetric
	body morphology, including a "corpus callosum worm" that can overcome
	noise, incomplete edges, considerable anatomical variation, and interference
	from collateral structures to segment and label the corpus callosum
	in 2D mid-sagittal MR brain images.},
  institution = {School of Computer Science, Ryerson University, Toronto, Ontario
	M5B 2K3, Canada. tmcinern@scs.ryerson.ca},
  keywords = {Algorithms; Corpus Callosum, anatomy /&/ histology/physiology; Expert
	Systems; Humans; Image Interpretation, Computer-Assisted, methods;
	Image Processing, Computer-Assisted, methods; Magnetic Resonance
	Imaging; Models, Biological; Models, Statistical; Movement, physiology;
	Sensitivity and Specificity},
  owner = {euHeart},
  pii = {S136184150200083X},
  pmid = {12270230},
  timestamp = {2009.02.25}
}

@ARTICLE{MCI00,
  author = {McInerney, Tim and Terzopoulos, Demetri},
  title = {T-Snakes: Topology Adaptive Snakes},
  journal = {Medical Image Analysis.},
  year = {2000},
  volume = {4},
  pages = {73-91},
  number = {2},
  abstract = {We present a new class of deformable contours (snakes) and apply them
	to the segmentation of medical images. Our snakes are defined in
	terms of an affine cell image decomposition (ACID). The `snakes in
	ACID' framework significantly extends conventional snakes, enabling
	topological flexibility among other features. The resulting topology
	adaptive snakes, or `T-snakes', can be used to segment some of the
	most complex-shaped biological structures from medical images in
	an efficient and highly automated manner.},
  keywords = {Deformable contours (snakes), Medical images, Affine cell image decomposition
	(ACID), Topological flexibility, Complex biological structures.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{MCI99,
  author = {McInerney, T. and Terzopoulos, D.},
  title = {Topology Adaptive Deformable Surfaces for Medical Image Volume Segmentation.},
  journal = {IEEE Transactions on Medical Imaging.},
  year = {1999},
  volume = {18},
  pages = {840-850},
  number = {10},
  note = {http://www.scs.ryerson.ca/~tmcinern/papers.html},
  abstract = {Deformable models, which include deformable contours (the popular
	snakes) and deformable surfaces, are a powerful model-based medical
	image analysis technique. The authors develop a new class of deformable
	models by formulating deformable surfaces in terms of an affine cell
	image decomposition (ACID). The authors' approach significantly extends
	standard deformable surfaces, while retaining their interactivity
	and other desirable properties. In particular, the ACID induces an
	efficient reparameterization mechanism that enables parametric deformable
	surfaces to evolve into complex geometries, even modifying their
	topology as necessary. The authors demonstrate that their new ACID-based
	deformable surfaces, dubbed T-surfaces, can effectively segment complex
	anatomic structures from medical volume images},
  keywords = {Segmentation, deformable models, deformable surfaces.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{MCI96,
  author = {McInerney, Tim and Terzopoulos, Demetri},
  title = {Deformable Models in Medical Image Analysis : A Survey},
  journal = {Medical Image Analysis},
  year = {1996},
  volume = {1},
  pages = {91-108},
  number = {2},
  abstract = {This article surveys deformable models, a promising and vigorously
	researched computer-assisted medical image analysis technique. Among
	model-based techniques, deformable models offer a unique and powerful
	approach to image analysis that combines geometry, physics, and approximation
	theory. They have proven to be effective in segmenting, matching,
	and tracking anatomic structures by exploiting (bottom-up) constraints
	derived from the image data together with (top-down) a priori knowledge
	about the location, size, and shape of these structures. Deformable
	models are capable of accommodating the significant variability of
	biological structures over time and across different individuals.
	Furthermore, they support highly intuitive interaction mechanisms
	that, when necessary, allow medical scientists and practitioners
	to bring their expertise to bear on the model-based image interpretation
	task. This article reviews the rapidly expanding body of work on
	the development and application of deformable models to problems
	of fundamental importance in medical image analysis, including segmentation,
	shape representation, matching, and motion tracking.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{MCI95,
  author = {McInerney, Tim and Terzopoulos, Demetri},
  title = {Topologically Adaptable Snakes},
  booktitle = {International Conference on Computer Vision},
  year = {1995},
  pages = {840-845},
  address = {Cambridge},
  abstract = {This paper presents a topologically adaptable snakes model for image
	segmentation and object representation. The model is embedded in
	the framework of domain subdivision using simplicial decomposition.
	This framework extends the geometric and topological adaptability
	of snakes while retaining all of the features of traditional snakes,
	such as user interaction, and overcoming many of the limitations
	of traditional snakes. By superposing a simplicial grid over the
	image domain and using this grid to iteratively reparameterize the
	deforming snakes model, the model is able to flow into complex shapes,
	even shapes with significant protrusions or branches, and to dynamically
	change topology as necessitated by the data. Snakes can be created
	and can split into multiple parts or seamlessly merge into other
	snakes. The model can also be easily converted to and from the traditional
	parametric snakes model representation. We apply a 2D model to various
	synthetic and real images in order to segment objects with complicated
	shapes and topologies.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{MCI06,
  author = {McIntosh, C. and Hamarneh, G.},
  title = {Vessel Crawlers: 3D Physically-based Deformable Organisms for Vasculature
	Segmentation and Analysis},
  booktitle = {Proc. IEEE Computer Society Conference on Computer Vision and Pattern
	Recognition},
  year = {2006},
  volume = {1},
  pages = {1084--1091},
  abstract = {We present a novel approach to the segmentation and analysis of vasculature
	from volumetric medical image data. Our method is an adoption and
	significant extension of deformable organisms, an artificial life
	framework for medical image analysis that complements classical deformable
	models with high-level, anatomically-driven control mechanisms. We
	extend deformable organisms to 3D, model their bodies as tubular
	spring-mass systems, and equip them with a new repertoire of sensory
	modules, behavioral routines, and decision making strategies. The
	result is a new breed of robust deformable organisms, vessel crawlers,
	that crawl along vasculature in 3D images, accurately segmenting
	vessel boundaries, detecting and exploring bifurcations, and providing
	sophisticated, clinically-relevant structural analysis. We validate
	our method through the segmentation and analysis of vascular structures
	in both noisy synthetic and real medical image data.},
  doi = {10.1109/CVPR.2006.329},
  issn = {1063-6919},
  owner = {euHeart},
  timestamp = {2009.02.25}
}

@ARTICLE{MEE03,
  author = {Meegama, Ravinda G.N. and Rajapakse, Jagath C.},
  title = {NURBS snakes},
  journal = {Image and Vision Computing},
  year = {2003},
  volume = {21},
  pages = {551-562},
  number = {6},
  note = {TY - JOUR U1 - 03237498447 L2 - http://dx.doi.org/10.1016/S0262-8856(03)00066-0
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Boundary extraction},
  abstract = {A new formulation of energy minimising splines, based on Non-Uniform
	Rational B-Spline (NURBS) curves, is presented. The existing snake
	models do not facilitate the local control of their shapes without
	increasing the number of control points. The introduction of NURBS
	renders more flexibility into the snake model due to the weighting
	parameters that are made to adapt according to the curvature of the
	contour. The proposed snake model was tested on both synthetic and
	real images; the new model demonstrated more local flexibility than
	the previous models and is capable of attracting to complex boundaries
	of the objects with increased accuracy. &copy; 2003 Elsevier Science
	B.V. All rights reserved.},
  keywords = {Feature extraction Boundary conditions Curve fitting Computational
	complexity Medical imaging Object recognition},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{MEH03,
  author = {Mehta, S. and Barber, D.C. and van Beek, E. and Wild, J.M. and Hamdy
	F.C.},
  title = {Delineation of the prostate capsule in 3D Trans Rectal Ultrasound
	images using image registration},
  booktitle = {Medical Image Understanding and Analysis},
  year = {2003},
  owner = {euHeart},
  timestamp = {2008.09.23}
}

@ARTICLE{MEN90,
  author = {Menet, Sylvie and Saint-Marc, Philippe and Medioni, G\'erard},
  title = {B-Snakes : Implementation and Application to Stereo},
  journal = {Image Understanding Workshop},
  year = {1990},
  pages = {720-726},
  note = {Image Understanding Workshop},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{MET02,
  author = {Metaxas, Dimitris N. and Kakadiaris, Ioannis A.},
  title = {Elastically adaptive deformable models},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2002},
  volume = {24},
  pages = {1310-1321},
  number = {10},
  note = {TY - JOUR U1 - 02487238856 L2 - http://dx.doi.org/10.1109/TPAMI.2002.1039203
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Adaptive elastic parameters Deformable models
	Shape estimation Physics-based modeling},
  abstract = {We present a novel technique for the automatic adaptation of a deformable
	model's elastic parameters within a Kalman filter framework for shape
	estimation applications. The novelty of the technique is that the
	model's elastic parameters are not constant, but spatio-temporally
	varying. The variation of the elastic parameters depends on the distance
	of the model from the data and the rate of change of this distance.
	Each pass of the algorithm uses physics-based modeling techniques
	to iteratively adjust both the geometric and the elastic degrees
	of freedom of the model in response to forces that are computed from
	the discrepancy between the model and the data. By augmenting the
	state equations of an extended Kalman filter to incorporate these
	additional variables, we are able to significantly improve the quality
	of the shape estimation. Therefore, the model's elastic parameters
	are always initialized to the same value and they are subsequently
	modified depending on the data and the noise distribution. We present
	results demonstrating the effectiveness of our method for both two-dimensional
	and three-dimensional data.},
  keywords = {Kalman filtering Computer simulation Algorithms Parameter estimation
	Finite element method Magnetic resonance imaging Image quality Image
	reconstruction},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@MISC{MET08,
  author = {Metz, C. and Schaap, M. and Van Walsum, T. and Van der Giessen, A.
	and Weustink, A. and Mollet, N. and Krestin, G. and Niessen, W.},
  title = {3D Segmentation in the Clinic: A Grand Challenge II - Coronary Artery
	Tracking},
  howpublished = {MIDAS Journal},
  year = {2008},
  owner = {dje},
  timestamp = {2009.10.17},
  url = {http://hdl.handle.net/10380/1399}
}

@INPROCEEDINGS{MEY08,
  author = {Meyer, Carsten and Manzke, Robert, and Peters, Jochen and Ecabert,
	Olivier and Kneser, Reinhard and Reedy, Vivek Y. and Chan, Raymond
	C. and Weese, Jurgen},
  title = {Automatic intra-operative generation of geometric left atrium / pulmonary
	vein models from rotational X-ray angiography},
  booktitle = {MICCAI},
  year = {2008},
  editor = {Metaxas et al.},
  pages = {61--69},
  abstract = {Pre-procedural imaging with cardiac CT or MR has become popular for
	guiding complex electrophysiology procedures such as those used for
	atrial fibrillation ablation therapy. Electroanatomical mapping and
	ablation within the left atrium and pulmonary veins (LAPV) is facilitated
	using such data, however the pre-procedural anatomy can be quite
	different from that at the time of intervention. Recently, a method
	for intra-procedural LAPV imaging has been developed based on contrastenhanced
	3-D rotational X-ray angiography (3-D RA). These intraprocedural
	data now create a compelling need for rapid and automated extraction
	of the LAPV geometry for catheter guidance. We present a new approach
	to automatic intra-procedural generation of LAPV surfaces from 3-D
	RA volumes. Using model-based segmentation, our technique is robust
	to imaging noise and artifacts typical of 3-D RA imaging, strongly
	minimizes the user interaction time required for segmentation, and
	eliminates inter-subject variability. Our findings in 33 patients
	indicate that intra-procedural LAPV surface models accurately represent
	the anatomy at the time of intervention and are comparable to preprocedural
	models derived from CTA or MRA.},
  owner = {euHeart},
  timestamp = {2008.09.23}
}

@ARTICLE{MON05,
  author = {Montagnat, Johan and Delingette, Herv{\'e}},
  title = {4D deformable models with temporal constraints: application to 4D
	cardiac image segmentation},
  journal = {Medical Image Analysis (MedIA)},
  year = {2005},
  volume = {9},
  pages = {87-100},
  number = {1},
  month = feb,
  lang = {english},
  owner = {euHeart},
  sorte = {revue},
  timestamp = {2009.02.24},
  url = {http://www.i3s.unice.fr/~johan/publis/MEDIA05.pdf}
}

@ARTICLE{MON98,
  author = {Montagnat, Johan and Delingette, Herv{\'e}},
  title = {Globally constrained deformable models for 3D object reconstruction},
  journal = {Signal Processing},
  year = {1998},
  volume = {71},
  pages = {173-186},
  number = {2},
  lang = {english},
  owner = {euHeart},
  sorte = {revue},
  timestamp = {2009.02.24},
  url = {http://www.i3s.unice.fr/~johan/publis/SP98.ps.gz}
}

@ARTICLE{MON01,
  author = {Montagnat, Johan and Delingette, H. and Ayache, N.},
  title = {A Review of Deformable Surfaces : Topology, Geometry and Deformation},
  journal = {Image and Vision Computing.},
  year = {2001},
  volume = {19},
  pages = {1023-1040},
  note = {Image and Vision Computing},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{MOR03,
  author = {Morooka, K.
	
	Takagi, H.
	
	Nagahashi, H.},
  title = {Active balloon model based on 3D skeleton extraction by competitive
	learning},
  booktitle = {3-D Digital Imaging and Modeling, 2003. 3DIM 2003. Proceedings. Fourth
	International Conference on},
  year = {2003},
  pages = {87-94},
  abstract = {We focus on the polygonal representation of a 3D object model which
	is composed of a lot of points on the object surface. In the 3D animation,
	it is sometimes necessary to use multiresolution representation of
	a model to cope with various situations, and it is preferable to
	represent the models with different resolutions in a certain unified
	data structure. In order to meet these requirements, we present a
	new method which generates the approximated model of an original
	one by using multiple deformable models, called active balloon models
	(ABMs). We extract the three-dimensional skeleton of the object.
	The obtained skeleton comprises nodes and edges, and represents the
	structure of the object. Based on the skeleton, the approximated
	model is generated by deforming the ABMs. Some experimental works
	are made to verify the capability of our method.},
  keywords = {computational geometry
	
	computer animation
	
	data structures
	
	feature extraction
	
	image representation
	
	image resolution
	
	image thinning
	
	mesh generation
	
	solid modelling
	
	unsupervised learning
	
	3D animation
	
	3D object model
	
	3D skeleton extraction
	
	active balloon model
	
	competitive learning
	
	multiple deformable models
	
	multiresolution representation
	
	object surface
	
	polygonal representation
	
	unified data structure},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{MOR98,
  author = {Mortensen, Eric N. and Barrett, William A.},
  title = {Interactive segmentation with intelligent scissors},
  journal = {Graphical Models and Image Processing},
  year = {1998},
  volume = { 60},
  pages = {349 - 384},
  number = { 5},
  note = {Intelligent scissors;Boundary cooling;On-the-fly training;},
  abstract = {We present a new, interactive tool called Intelligent Scissors which
	we use for image segmentation. Fully automated segmentation is an
	unsolved problem, while manual tracing is inaccurate and laboriously
	unacceptable. However, Intelligent Scissors allow objects within
	digital images to be extracted quickly and accurately using simple
	gesture motions with a mouse. When the gestured mouse position comes
	in proximity to an object edge, a live-wire boundary `snaps' to,
	and wraps around the object of interest. Live-wire boundary detection
	formulates boundary detection as an optimal path search in a weighted
	graph. Optimal graph searching provides mathematically piece-wise
	optimal boundaries while greatly reducing sensitivity to local noise
	or other intervening structures. Robustness is further enhanced with
	on-the-fly training which causes the boundary to adhere to the specific
	type of edge currently being followed, rather than simply the strongest
	edge in the neighborhood. Boundary cooling automatically freezes
	unchanging segments and automates input of additional seed points.
	Cooling also allows the user to be much more free with the gesture
	path, thereby increasing the efficiency and finesse with which boundaries
	can be extracted.},
  copyright = {Compilation and indexing terms, Copyright 2007 Elsevier Inc. All rights
	reserved},
  issn = {1077-3169},
  key = {Image segmentation},
  keywords = {Image analysis;Interactive computer systems;Feature extraction;Edge
	detection;Graph theory;Sensitivity analysis;Interactive computer
	graphics;},
  language = {English},
  owner = {euHeart},
  timestamp = {2009.02.24},
  url = {http://dx.doi.org/10.1006/gmip.1998.0480}
}

@BOOK{MUK98,
  title = {Moment Functions in Image Analysis, Theory and Applications},
  publisher = {World Scientific},
  year = {1998},
  author = {R. Mukundan and K.~R.~ Ramakrishnan},
  isbn = {978-981-02-3524-6}
}

@ARTICLE{NAT06,
  author = {Natarajan, Vijay and Wang, Yusu and Bremer, Peer-Timo and Pascucci,
	Valerio and Hamann, Bernd},
  title = {Segmenting molecular surfaces},
  journal = {Computer Aided Geometric Design},
  year = {2006},
  volume = {23},
  pages = {495--509},
  number = {6},
  month = aug,
  abstract = {This paper presents a new method for segmentation of molecular surfaces.
	Topological analysis of a scalar function defined on the surface
	and its associated gradient field reveals the relationship between
	the features of interest and critical points of the scalar function.
	The segmentation is obtained by associating segments with local minima/maxima.
	Controlled simplification of the function merges segments resulting
	in a hierarchical segmentation of the molecular surface. This segmentation
	is used to identify rigid components of protein molecules and to
	study the role of cavities and protrusions in protein-protein interactions.},
  booktitle = {Applications of Geometric Modeling in the Life Sciences},
  keywords = {Segmentation, Morse theory, Morse-Smale complex, Multiresolution data
	structure, Visualization, Structural biology, Molecular graphics},
  timestamp = {2008.02.01},
  url = {http://www.sciencedirect.com/science/article/B6TYN-4JTR8SF-1/2/eff7f28d47428cd55643613bbd13249e}
}

@ARTICLE{NEU98,
  author = {Neumaier, Arnold},
  title = {Solving ill-conditioned and singular linear systems: A tutorial on
	regularization},
  journal = {SIAM Review},
  year = {1998},
  volume = { 40},
  pages = {636 - 666},
  number = { 3},
  note = {Tikhonov regularization;Generalized cross validation (GCV);Generalized
	maximum likelihood (GML);},
  abstract = {It is shown that the basic regularization procedures for finding meaningful
	approximate solutions of ill-conditioned or singular linear systems
	can be phrased and analyzed in terms of classical linear algebra
	that can be taught in any numerical analysis course. Apart from rewriting
	many known results in a more elementary form, we also derive a new
	two-parameter family of merit functions for the determination of
	the regularization parameter. The traditional merit functions from
	generalized cross validation (GCV) and generalized maximum likelihood
	(GML) are recovered as special cases.},
  copyright = {Compilation and indexing terms, Copyright 2006 Elsevier Inc. All rights
	reserved},
  issn = {0036-1445},
  key = {Approximation theory},
  keywords = {Linear algebra;Numerical analysis;Functions;Maximum likelihood estimation;Error
	analysis;},
  language = {English},
  owner = {euHeart},
  timestamp = {2009.02.24},
  url = {http://dx.doi.org/10.1137/S0036144597321909}
}

@ARTICLE{NEZ07,
  author = {Reza Nezafat and Yuchi Han and Dana C Peters and Daniel A Herzka
	and John V Wylie and Beth Goddu and Kraig K Kissinger and Susan B
	Yeon and Peter J Zimetbaum and Warren J Manning},
  title = {Coronary magnetic resonance vein imaging: imaging contrast, sequence,
	and timing.},
  journal = {Magn Reson Med},
  year = {2007},
  volume = {58},
  pages = {1196--1206},
  number = {6},
  month = {Dec},
  abstract = {Recently, there has been increased interest in imaging the coronary
	vein anatomy to guide interventional cardiovascular procedures such
	as cardiac resynchronization therapy (CRT), a device therapy for
	congestive heart failure (CHF). With CRT the lateral wall of the
	left ventricle is electrically paced using a transvenous coronary
	sinus lead or surgically placed epicardial lead. Proper transvenous
	lead placement is facilitated by the knowledge of the coronary vein
	anatomy. Cardiovascular MR (CMR) has the potential to image the coronary
	veins. In this study we propose and test CMR techniques and protocols
	for imaging the coronary venous anatomy. Three aspects of design
	of imaging sequence were studied: magnetization preparation schemes
	(T(2) preparation and magnetization transfer), imaging sequences
	(gradient-echo (GRE) and steady-state free precession (SSFP)), and
	imaging time during the cardiac cycle. Numerical and in vivo studies
	both in healthy and CHF subjects were performed to optimize and demonstrate
	the utility of CMR for coronary vein imaging. Magnetization transfer
	was superior to T(2) preparation for contrast enhancement. Both GRE
	and SSFP were viable imaging sequences, although GRE provided more
	robust results with better contrast. Imaging during the end-systolic
	quiescent period was preferable as it coincided with the maximum
	size of the coronary veins.},
  doi = {10.1002/mrm.21395},
  institution = {Department of Medicine (Cardiovascular Division), Beth Israel Deaconess
	Medical Center, Boston, Massachusetts 02215, USA. rnezafat@bidmc.harvard.edu},
  keywords = {Adult; Algorithms; Coronary Angiography, methods; Coronary Vessels,
	pathology; Female; Heart Failure, pathology; Humans; Image Enhancement,
	methods; Image Interpretation, Computer-Assisted, methods; Magnetic
	Resonance Angiography, methods; Male; Middle Aged; Reproducibility
	of Results; Sensitivity and Specificity; Veins, pathology},
  owner = {euHeart},
  pmid = {17969081},
  timestamp = {2009.02.16},
  url = {http://dx.doi.org/10.1002/mrm.21395}
}

@ARTICLE{NG99,
  author = {Ng, Kit-Cheng and Newman, Timothy S.},
  title = {Elastic registration of MRA brain images using salient blood vessel
	features},
  journal = {Proceedings of SPIE - The International Society for Optical Engineering
	Proceedings of the 1999 Medical Imaging - Image Processing, Feb 22-Feb
	25 1999},
  year = {1999},
  volume = {3661},
  pages = {782-793},
  number = {II},
  note = {TY - JOUR U1 - 99104861442 L2 - http://dx.doi.org/10.1117/12.348635
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Elastic image registration Curve tracing B
	splines Volume warping},
  abstract = {In this paper, a new local registration method is introduced. The
	method enables regional refinements to an initial coarse global alignment
	of two magnetic resonance angiography (MRA) volume datasets through
	the use of snake-based local elastic deformations on blood vessel
	curves. The curves are deformed based on corresponding salient features
	(points of bifurcation and high curvature) that are extracted using
	a multi-stage method. The extraction involves first tracing blood
	vessel curves from depth-enhanced maximal intensity projections of
	the original volume data and then fitting B-splines to the traced
	structures. The framework for a new approach to volume warping which
	completes the refinement process for all points in the datasets is
	also introduced. The local registration method offers the promise
	of registering data collected over time from a patient.},
  keywords = {Angiography Magnetic resonance imaging Feature extraction Image analysis
	Blood vessel prostheses Interpolation Curve fitting Correlation methods
	Neurophysiology Brain Medical imaging},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{NI04,
  author = {Xinlai Ni and Michael Garland and John C. Hart},
  title = {Fair morse functions for extracting the topological structure of
	a surface mesh},
  booktitle = {SIGGRAPH},
  year = {2004},
  pages = {613--622},
  publisher = {ACM Press},
  abstract = {Morse theory reveals the topological structure of a shape based on
	the critical points of a real function over the shape. A poor choice
	of this real function can lead to a complex configuration of an unnecessarily
	high number of critical points. This paper solves a relaxed form
	of Laplace's equation to find a "fair" Morse function with a user-controlled
	number and configuration of critical points. When the number is minimal,
	the resulting Morse complex cuts the shape into a disk. Specifying
	additional critical points at surface features yields a base domain
	that better represents the geometry and shares the same topology
	as the original mesh, and can also cluster a mesh into approximately
	developable patches. We make Morse theory on meshes more robust with
	teflon saddles and flat edge collapses, and devise a new "intermediate
	value propagation" multigrid solver for finding fair Morse functions
	that runs in provably linear time.},
  doi = {http://doi.acm.org/10.1145/1186562.1015769},
  location = {Los Angeles, California},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{OCO09,
  author = {Aonghus O'Connor and Kieran F. Mulchrone and Patrick A. Meere},
  title = {WinDICOM: A program for determining inclusion shape and orientation},
  journal = {Computers \& Geosciences},
  year = {2009},
  volume = {35},
  pages = {1358 - 1368},
  number = {6},
  abstract = {Digital images obtained from X-ray computed tomography scans are analysed
	for the estimation of inclusion shape and orientation. Three-dimensional
	computer imagery and segmentation algorithms are used to visualise
	and isolate the regions of interest. These regions are then approximated
	by best-fit ellipsoids and the mean best-fit ellipsoid is used as
	a measure of preferred inclusion orientation. A Windows program is
	developed to implement these procedures and results found from both
	manufactured and natural data are presented. These results show that
	the radiodensity contrast plays a major role in the ability of the
	software to isolate inclusions from their matrix and hence determine
	rock fabric.},
  doi = {DOI: 10.1016/j.cageo.2008.08.015},
  issn = {0098-3004},
  keywords = {Non-invasive object inspection},
  owner = {euHeart},
  timestamp = {2009.05.22},
  url = {http://www.sciencedirect.com/science/article/B6V7D-4VFC7YV-1/2/bd8ed96a35c2ce61b038cf3a2eaf63d0}
}

@ARTICLE{OHT01,
  author = {Ohtake, Y. and Belyaev, A. and Bogaevski, I.},
  title = {Mesh regularization and adaptive smoothing},
  journal = {CAD Computer Aided Design},
  year = {2001},
  volume = { 33},
  pages = {789 - 800},
  number = { 11},
  note = {Mesh regularizations;Mesh smoothing;},
  abstract = {The paper presents a set of mesh smoothing tools developed to increase
	mesh regularity, reduce oversmoothing, and enhance crease lines.
	Mesh smoothing with simultaneous increasing mesh regularity and reducing
	oversmoothing is achieved by combining together the Laplacian flow
	and a mesh evolution by a function of the mean curvature. To enhance
	salient ridge and ravine structures we use a coupled nonlinear diffusion
	of the mesh normals and vertices. &copy; 2001 Elsevier Science Ltd.
	All rights reserved.},
  copyright = {Compilation and indexing terms, Copyright 2007 Elsevier Inc. All rights
	reserved},
  issn = {0010-4485},
  key = {Computer aided design},
  keywords = {Computer graphics;Computational geometry;Surfaces;Contour measurement;Computer
	simulation;Mathematical models;},
  language = {English},
  owner = {euHeart},
  timestamp = {2009.02.24},
  url = {http://dx.doi.org/10.1016/S0010-4485(01)00095-1}
}

@ARTICLE{OLS96,
  author = {Olstad, Bjorn and Torp, Anders H.},
  title = {Encoding of a priori information in active contour models},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {1996},
  volume = {18},
  pages = {863-872},
  number = {9},
  note = {TY - JOUR U1 - 96103372215 L2 - http://dx.doi.org/10.1109/34.537341
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Active contours Grammatical encodings String
	matching},
  abstract = {The theory of active contours models the problem of contour recovery
	as an energy minimization process. The computational solutions based
	on dynamic programming require that the energy associated with a
	contour candidate can be decomposed into an integral of local energy
	contributions. In this paper we propose a grammatical framework that
	can model different local energy models and a set of allowable transitions
	between these models. The grammatical encodings are utilized to represent
	a priori knowledge about the shape of the object and the associated
	signatures in the underlying images. The variability encountered
	in numerical experiments is addressed with the energy minimization
	procedure which is embedded in the grammatical framework. We propose
	an algorithmic solution that combines a nondeterministic version
	of the Knuth-Morris-Pratt algorithm for string matching with a time-delayed
	discrete dynamic programming algorithm for energy minimization. The
	numerical experiments address practical problems encountered in contour
	recovery such as noise robustness and occlusion.},
  keywords = {Encoding (symbols) Computational grammars Data structures Computer
	simulation Dynamic programming Knowledge representation Algorithms
	Edge detection},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{OZG05,
  author = {Ozgun, Murat and Hoffmeier, Andreas and Kouwenhoven, Marc and Botnar,
	Rene M. and Stuber, Matthias and Scheld, Hans Heinrich and Manning,
	Warren J. and Heindel, Walter and Maintz, David},
  title = {{Comparison of 3D Segmented Gradient-Echo and Steady-State Free Precession
	Coronary MRI Sequences in Patients with Coronary Artery Disease}},
  journal = {Am. J. Roentgenol.},
  year = {2005},
  volume = {185},
  pages = {103-109},
  number = {1},
  abstract = {OBJECTIVE. Our objective was to compare two state-of-the-art coronary
	MRI (CMRI) sequences with regard to image quality and diagnostic
	accuracy for the detection of coronary artery disease (CAD). SUBJECTS
	AND METHODS. Twenty patients with known CAD were examined with a
	navigator-gated and corrected free-breathing 3D segmented gradient-echo
	(turbo field-echo) CMRI sequence and a steady-state free precession
	sequence (balanced turbo field-echo). CMRI was performed in a transverse
	plane for the left coronary artery and a double-oblique plane for
	the right coronary artery system. Subjective image quality (1- to
	4-point scale, with 1 indicating excellent quality) and objective
	image quality parameters were independently determined for both sequences.
	Sensitivity, specificity, and accuracy for the detection of significant
	([&ge;] 50% diameter) coronary artery stenoses were determined as
	defined in invasive catheter X-ray coronary angiography. RESULTS.
	Subjective image quality was superior for the balanced turbo field-echo
	approach (1.8 {+/-} 0.9 vs 2.3 {+/-} 1.0 for turbo field-echo; p
	< 0.001). Vessel sharpness, signal-to-noise ratio, and contrast-to-noise
	ratio were all superior for the balanced turbo field-echo approach
	(p < 0.01 for signal-to-noise ratio and contrast-to-noise ratio).
	Of the 103 segments, 18% of turbo field-echo segments and 9% of balanced
	turbo field-echo segments had to be excluded from disease evaluation
	because of insufficient image quality. Sensitivity, specificity,
	and accuracy for the detection of significant coronary artery stenoses
	in the evaluated segments were 92%, 67%, 85%, respectively, for turbo
	field-echo and 82%, 82%, 81%, respectively, for balanced turbo field-echo.
	CONCLUSION. Balanced turbo field-echo offers improved image quality
	with significantly fewer nondiagnostic segments when compared with
	turbo field-echo. For the detection of CAD, both sequences showed
	comparable accuracy for the visualized segments.},
  eprint = {http://www.ajronline.org/cgi/reprint/185/1/103.pdf},
  owner = {euHeart},
  timestamp = {2009.01.12},
  url = {http://www.ajronline.org/cgi/content/abstract/185/1/103}
}

@INPROCEEDINGS{PAR99,
  author = {Park, Joo-Young and McInerney, T. and Terzopoulos, D. and Kim, Myoung-Hee},
  title = {A multiscale deformable model for extracting complex surfaces from
	volume images},
  booktitle = {Proc. Seventh Pacific Conference on Computer Graphics and Applications},
  year = {1999},
  pages = {208--215},
  abstract = {Deformable surface models are an attractive method for segmenting
	the three-dimensional shapes of complex anatomic structures in volumetric
	medical images. Despite the success of this approach, several problems
	remain. In this paper, we propose a multiscale deformable surface
	model with non-intersection constraint forces that successfully addresses
	three significant problems-sensitivity to model initialization, difficulties
	in dealing with severe object concavities, and model self-intersection.
	The first two problems are addressed by the multiscale scheme, which
	progressively resamples the triangulated, deformable surface model
	both globally and locally, matching its resolution to the levels
	of a volume image pyramid. We address the third problem by including
	a non-intersection constraint force among the customary internal
	and external forces in the physics-based formulation. We apply our
	new deformable surface model to the challenging task of extracting
	brain cortical surfaces},
  doi = {10.1109/PCCGA.1999.803364},
  keywords = {brain, medical image processing, solid modelling, brain cortical surfaces,
	complex anatomic structures, complex surfaces extraction, deformable
	surface model, model initialization, model self-intersection, multiscale
	deformable model, multiscale deformable surface model, severe object
	concavities, volume image pyramid, volume images, volumetric medical
	images},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{PAU01,
  author = {Mark Pauly and Markus Gross},
  title = {Spectral processing of point-sampled geometry},
  booktitle = {SIGGRAPH '01: Proceedings of the 28th annual conference on Computer
	graphics and interactive techniques},
  year = {2001},
  pages = {379--386},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  doi = {http://doi.acm.org/10.1145/383259.383301},
  isbn = {1-58113-374-X},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{PET99,
  author = {Peters, Jorg},
  title = {Curvature continuous spline surfaces over irregular meshes},
  journal = {Computer Aided Geometric Design.},
  year = {1996},
  volume = {13},
  pages = {101-131},
  number = {2},
  abstract = {Concepts and techniques for the construction of smooth surfaces over
	irregular meshes are developed and made concrete by defining curvature
	continuous splines based on 3-sided patches. These splines extend
	the B-spline paradigm for the construction of parametric piecewise
	polynomial surfaces to control meshes with nonquadrilateral cells
	and more or fewer than four cells meeting at a point. Mesh points
	serve as control points and are locally averaged to obtain a Bernstein-Bézier
	representation which in turn defines surface points as averages.},
  keywords = {C2 surface, Corner cutting, Box splines, Blending, Geometric continuity,
	Spline mesh, Free-form surface modeling, Symbolic generation of constraints.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@CONFERENCE{PET08,
  author = {J. Peters and O. Ecabert and C. Lorenz and J. von Berg and M. J.
	Walker and T. B. Ivanc and M. Vembar and M. E. Olszewski and J. Weese},
  title = {Segmentation of the heart and major vascular structures in cardiovascular
	CT images},
  booktitle = {Medical Imaging},
  year = {2008},
  editor = {Joseph M. Reinhardt and Josien P. W. Pluim},
  volume = {6914},
  number = {1},
  pages = {691417},
  publisher = {SPIE},
  doi = {10.1117/12.768494},
  eid = {691417},
  journal = {Medical Imaging 2008: Image Processing},
  location = {San Diego, CA, USA},
  numpages = {12},
  owner = {euHeart},
  timestamp = {2008.09.23},
  url = {http://link.aip.org/link/?PSI/6914/691417/1}
}

@INPROCEEDINGS{PET07,
  author = {Peters, Jochen and Ecabert, Olivier and Meyer, Carsten and Schramm,
	Hauke and Kneser, Reinhard and Groth, Alexandra and Weese, Jürgen},
  title = {Automatic Whole Heart Segmentation in Static Magnetic Resonance Image
	Volumes},
  booktitle = {MICCAI},
  year = {2007},
  pages = {402--410},
  abstract = {We present a fully automatic segmentation algorithm for the whole
	heart (four chambers, left ventricular myocardium and trunks of the
	aorta, the pulmonary artery and the pulmonary veins) in cardiac MR
	image volumes with nearly isotropic voxel resolution, based on shape-constrained
	deformable models. After automatic model initialization and reorientation
	to the cardiac axes, we apply a multi-stage adaptation scheme with
	progressively increasing degrees of freedom. Particular attention
	is paid to the calibration of the MR image intensities. Detailed
	evaluation results for the various anatomical heart regions are presented
	on a database of 42 patients. On calibrated images, we obtain an
	average segmentation error of 0.76mm.},
  owner = {euHeart},
  timestamp = {2008.09.10},
  url = {http://dx.doi.org/10.1007/978-3-540-75759-7_49}
}

@ARTICLE{PET05,
  author = {Peters, Jochen and Ecabert, Olivier and Weese, Jürgen},
  title = {Feature optimization via simulated search for model-based heart segmentation},
  journal = {International Congress Series},
  year = {2005},
  volume = {1281},
  pages = {33--38},
  month = may,
  abstract = {Medical images contain numerous different structures and organs. When
	segmenting a complex organ such as the heart, it is a typical problem
	of deformable models and related techniques that the image features
	defining the wanted object boundaries are not discriminative enough
	to distinguish between borders of different sub-organs. As a result,
	the model is often locally attracted by some wrong structures. We
	present a method to automatically learn from a representative set
	of 3D images which features are most appropriate at each position
	of the surface of the deformable model. The basic idea is to simulate
	the boundary detection for the given 3D images and to select those
	features that minimize the distance between the detected position
	and the desired object boundary. In addition, we present examples
	for heart segmentation showing that attraction by false edges is
	almost completely eliminated.},
  booktitle = {CARS 2005: Computer Assisted Radiology and Surgery},
  keywords = {Model-based segmentation, Image features, Discriminative training,
	Cardiac CT},
  owner = {euHeart},
  timestamp = {2008.10.22},
  url = {http://www.sciencedirect.com/science/article/B7581-4GFTP32-H/2/d5fdbeca7283d1468c32a92761dea206}
}

@INCOLLECTION{PET96,
  author = {Peters, Jorg and Wittman, Michael},
  title = {Smooth Blending of Basic Surfaces Using Trivariate Box Splines},
  booktitle = {Ima 96, The Mathematics of Surfaces},
  year = {1996},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{PFE96,
  author = {Pfeifle, Ron and Seidel, Hans-Peter},
  title = {Fitting Triangular B-Splines to Functional Scattered Data},
  journal = {Computer Graphics Forum.},
  year = {1996},
  volume = {15},
  pages = {15-23},
  number = {1},
  abstract = {Scattered data is, by definition, irregularly spaced. Uniform surface
	schemes are not well adapted to the locally varying nature of such
	data. Conversely, Triangular B-Spline surfaces2 are more flexible
	in that they can be built over arbitrary triangulations and thus
	can be adapted to the scattered data. This paper discusses the use
	of DMS spline surfaces for approximation of scattered data. A method
	is provided for automatically triangulating the domain containing
	the points and generating basis functions over this triangulation.
	A surface approximating the data is then found by a combination of
	least squares and bending energy minimization. This combination serves
	both to generate a smooth surface and to accommodate for gaps in
	the data. Examples are presented which demonstrate the eftectiveness
	of the technique for mathematical, geographical and other data sets.},
  keywords = {Scattered data approximation, triangular B-splines, DMS splines, simplex
	splines, quadtrees.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{PIE91,
  author = {Piegl, Les},
  title = {On NURBS: a Survey},
  journal = {IEEE Computer Graphics and Applications archive},
  year = {1991},
  volume = {11},
  pages = {55 - 71},
  number = {1},
  abstract = {Nonuniform rational B-spline (NURBS) curves and surfaces, which are
	based on rational and B-splines, are defined. The important characteristics
	of NURBS that have contributed to their wide acceptance as standard
	tools for geometry representation and design are summarized. Their
	application to representing conic sections and commonly used surfaces,
	designing curves and surfaces, and modifying shapes is examined.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INCOLLECTION{PIT05,
  author = {A. Pitiot and H. Delingette and P.M. Thompson},
  title = {Automated Image Segmentation: Issues and Applications},
  booktitle = {Medical Imaging Systems Technology},
  publisher = {World Scientific},
  year = {2005},
  editor = {Cornelius T. Leondes},
  volume = {3},
  address = {London},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{PRA06,
  author = {Niek Hendrik Jan Prakken and Evert-Jan P A Vonken and Birgitta K
	Velthuis and Pieter A F M Doevendans and Maarten-Jan M Cramer},
  title = {3D MR coronary angiography: optimization of the technique and preliminary
	results.},
  journal = {Int J Cardiovasc Imaging},
  year = {2006},
  volume = {22},
  pages = {477--487},
  number = {3-4},
  abstract = {OBJECTIVE: Current clinical full MR angiography with multiple breathhold
	multiple thin slab acquisition (MTS) is difficult and arduous. This
	study describes the optimisation of the whole heart free - breathing
	balanced turbo field echo (B-TFE) protocol. A high-resolution image
	of the whole heart is produced in less or comparable time to MTS
	acquisition and allows for reconstruction afterwards to visualise
	the individual coronary arteries. The scan is easily performed because
	the volume has to be targeted only once. DESIGN AND SETTING: Eighteen
	healthy adults without a history of cardiovascular disease underwent
	free-breathing 3D MR angiography with the B-TFE protocol. The whole-heart
	data set was reformatted in identical orientations in all subjects
	to visualise the major coronary arteries. MAIN OUTCOME MEASURES:
	Vessel length, signal and contrast to noise ratio were determined
	and compared for each vessel. RESULTS: Mean visible vessel lengths
	were 116 mm for the right, 102 mm for the left main and left descending
	and 76 mm for the left circumflex coronary artery. The average signal
	to noise ratio was 7.5 and contrast to noise ratio was 4.9. Because
	of the need for synchronised cardiac and respiratory triggering the
	coronaries could not be judged in 25\% of the subjects. CONCLUSIONS:
	The optimised B-TFE protocol had equal judgeability and vessels could
	be judged over longer contiguous distances compared to earlier implementations
	of the B-TFE protocol. We conclude whole heart free breathing navigator-gated
	and slice-tracked 3D coronary MR angiography with use of the adjusted
	B-TFE protocol is possible, but still suboptimal for clinical use.},
  doi = {10.1007/s10554-005-9053-8},
  institution = {Department of Radiology, University Medical Centre, Utrecht, The
	Netherlands. NHJPrakken@GMail.com},
  keywords = {Adult; Coronary Angiography, methods; Coronary Artery Disease, diagnosis/pathology;
	Coronary Vessels, anatomy /&/ histology; Feasibility Studies; Female;
	Humans; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional;
	Magnetic Resonance Angiography; Male; Middle Aged; Reproducibility
	of Results; Sensitivity and Specificity},
  language = {eng},
  medline-pst = {ppublish},
  owner = {euHeart},
  pmid = {16538433},
  timestamp = {2009.09.14},
  url = {http://dx.doi.org/10.1007/s10554-005-9053-8}
}

@INCOLLECTION{PRA02,
  author = {Prautzsch, Hartmut and Boehm, Wolfgang},
  title = {Box Splines},
  booktitle = {Handbook of Computer Aided Geometric Design.},
  year = {2002},
  editor = {Farin, Hoschek, Kim, eds.},
  pages = {ch10},
  abstract = {This chapter provides a brief introduction to box and half-box splines
	with particular focus on triangular splines and surface design. A
	particular example of box splines are the B-splines with equidistant
	knots. In general, box splines consist of regularly arranged polynomial
	pieces and they have a useful geometric interpretation. Namely they
	can be viewed as density functions of the shadows of higher dimensional
	boxes and half-boxes. Of particular interest for Geometric Design
	are box spline surfaces that consist of triangular polynomial pieces.
	These box spline surfaces have planar domains, but it is quite simple
	to construct arbitrary two-dimensional surfaces, i.e., manifolds,
	with these box splines.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{PRE03,
  author = {Precioso, F. and Barland, M. and Blu, T. and Unser, M.},
  title = {Smoothing B-spline active contour for fast and robust image and video
	segmentation},
  booktitle = {Proceedings: 2003 International Conference on Image Processing, ICIP-2003,
	Sep 14-17 2003},
  year = {2003},
  volume = {1},
  series = {IEEE International Conference on Image Processing},
  pages = {137-140},
  address = {Barcelona, Spain},
  publisher = {Institute of Electrical and Electronics Engineers Computer Society},
  note = {TY - CONF U1 - 03517785265 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc. U2 - Video segmentation
	Active contours},
  abstract = {This paper deals with fast image and video segmentation using active
	contours. Region based active contours using level-sets are powerful
	techniques for video segmentation but they suffer from large computational
	cost. A parametric active contour method based on B-Spline interpolation
	has been proposed in [1] to highly reduce the computational cost
	but this method is sensitive to noise. Here, we choose to relax the
	rigid interpolation constraint in order to robustify our method in
	the presence of noise: by using smoothing splines, we trade a tunable
	amount of interpolation error for a smoother spline curve. We show
	by experiments on natural sequences that this new flexibility yields
	segmentation results of higher quality at. no additional computational
	cost. Hence real time processing for moving objects segmentation
	is preserved.},
  keywords = {Real time systems Interpolation Statistical tests Algorithms Image
	segmentation},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{PRE02,
  author = {Precioso, Frederic and Barlaud, M.},
  title = {Regular Spatial B-Spline Active Contour For Fast Video Segmentation},
  booktitle = {ICIP},
  year = {2002},
  address = {New York},
  publisher = {IEEE},
  abstract = {This paper deals with fast video segmentation using active contours.
	Region-based active contours is a powerful technique for video segmentation.
	However most of these methods are implemented using level-sets. Although
	level-set methods provide accurate segmentation, they suffer from
	large computational cost. The proposed method uses BSpline parametric
	method to highly improve the computation cost. Our method removes
	irregular sampling assumption. It combines multi-resolution regular
	sampling and length penalty.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{PRE05,
  author = {Precioso, Frederic and Barlaud, Michel and Blu, Thierry and Unser,
	Michael},
  title = {Robust real-time segmentation of images and videos using a smooth-spline
	snake-based algorithm},
  journal = {IEEE Transactions on Image Processing},
  year = {2005},
  volume = {14},
  pages = {910-924},
  number = {7},
  note = {TY - JOUR Compilation and indexing terms, Copyright 2006 Elsevier
	Inc. All rights reserved U1 - 05289199644 U2 - Cubic spline interpolation
	Cubic smoothing splines Region based active contours L2 - http://dx.doi.org/10.1109/TIP.2005.849307},
  abstract = {This paper deals with fast image and video segmentation using active
	contours. Region-based active contours using level sets are powerful
	techniques for video segmentation, but they suffer from large computational
	cost. A parametric active contour method based on B-Spline interpolation
	has been proposed in [26] to highly reduce the computational cost,
	but this method is sensitive to noise. Here, we choose to relax the
	rigid interpolation constraint in order to robustify our method in
	the presence of noise: by using smoothing splines, we trade a tunable
	amount of interpolation error for a smoother spline curve. We show
	by experiments on natural sequences that this new flexibility yields
	segmentation results of higher quality at no additional computational
	cost. Hence, real-time processing for moving objects segmentation
	is preserved. &copy; 2005 IEEE.},
  keywords = {Image segmentation Algorithms Interpolation Image quality Spurious
	signal noise Computational complexity},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{PRO92,
  author = {Richard J. Prokop and Anthony P. Reeves},
  title = {A survey of moment-based techniques for unoccluded object representation
	and recognition},
  journal = {CVGIP: Graph. Models Image Process.},
  year = {1992},
  volume = {54},
  pages = {438--460},
  number = {5},
  address = {Orlando, FL, USA},
  doi = {http://dx.doi.org/10.1016/1049-9652(92)90027-U},
  issn = {1049-9652},
  owner = {euHeart},
  publisher = {Academic Press, Inc.},
  timestamp = {2008.09.22}
}

@ARTICLE{QIN97,
  author = {Qin, Hong and Terzopoulos, Demetri},
  title = {Triangular NURBS and their dynamic generalizations},
  journal = {Computer Aided Geometric Design},
  year = {1997},
  volume = {14},
  pages = {325-347},
  number = {4},
  note = {TY - JOUR U1 - 97063684021 L2 - http://dx.doi.org/10.1016/S0167-8396(96)00062-3
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Triangular B splines Geometric design Lagrangian
	mechanics},
  abstract = {Triangular NURBS is proposed to enhance the power of triangular B-splines.
	A new model based on the elegant triangular NURBS geometry and principles
	of physical dynamics is developed to ameliorate the design process.
	The model combines the geometric features of triangular NURBS with
	the demonstrated conveniences of interaction within a physics-based
	framework. Several applications of the model are demonstrated including
	direct manipulation and interactive sculpting through force-based
	tools, the fitting of unorganized data, and solid rounding with geometric
	and physical constraints.},
  keywords = {Computational geometry Computer simulation Integration Differential
	equations Equations of motion Constraint theory Finite element method
	Algorithms Computer aided design},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{RAD97,
  author = {Radeva, Petia and Amini, Amir A. and Huang, Jiantao},
  title = {Deformable B-solids and Implicit Snakes for 3D Localization and tracking
	of SPAMM MRI Data},
  journal = {Computer Vision and Image Understanding.},
  year = {1997},
  volume = {66},
  pages = {163-178},
  number = {2},
  abstract = {To date, MRI-SPAMM data from different image slices have been analyzed
	independently. In this paper, we propose an approach for 3D tag localization
	and tracking of SPAMM data by a novel deformable B-solid. The solid
	is defined in terms of a 3D tensor product B-spline. The isoparametric
	curves of the B-spline solid have special importance. These are termed
	implicit snakes as they deform under image forces from tag lines
	in different image slices. The localization and tracking of tag lines
	is performed under constraints of continuity and smoothness of the
	B-solid. To track motion from boundaries and restrict image forces
	to the myocardium, a volumetric model is employed as a pair of coupled
	endocardial and epicardial B-spline surfaces. To recover deformations
	of the left ventricle (LV), an energy-minimization problem is posed
	where both tag and LV boundary data are used. The framework has been
	implemented on tag data from short axis (SA) cardiac images, as well
	as SA LV boundaries, and is currently being extended to include long
	axis data.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{RAD95,
  author = {Radeva, Petia and Serrat, Joan and Marti, Enric},
  title = {Snake for model-based segmentation},
  booktitle = {ICCV},
  year = {1995},
  pages = {816 - 821},
  address = {Cambridge, MA, USA},
  publisher = {IEEE},
  note = {Snake techniques;Spurious edge points;Deformation convergence;Model
	based segmentation;Elastic matching;},
  abstract = {Despite the promising results of numerous applications, the hitherto
	proposed snake techniques share some common problems: snake attraction
	by spurious edge points, snake degeneration (shrinking and flattening),
	convergence and stability of the deformation process, snake initialization
	and local determination of the parameters of elasticity. We argue
	here that these problems can be solved only when all the snake aspects
	are considered. The snakes proposed here implement a new potential
	field and external force in order to provide a deformation convergence,
	attraction by both near and far edges as well as snake behaviour
	selective according to the edge orientation. Furthermore, we conclude
	that in the case of model-based segmentation, the internal force
	should include structural information about the expected snake shape.
	Experiments using this kind of snakes for segmenting bones in complex
	hand radiographs show a significant improvement.},
  copyright = {Compilation and indexing terms, Copyright 2007 Elsevier Inc. All rights
	reserved},
  key = {Computational geometry},
  keywords = {Image segmentation;Computer vision;Mathematical models;Problem solving;Hierarchical
	systems;Edge detection;Bone;Radiography;Medical computing;},
  language = {English},
  owner = {euHeart},
  timestamp = {2009.02.24},
  url = {http://dx.doi.org/10.1109/ICCV.1995.466854}
}

@ARTICLE{RAM99,
  author = {Ramponi, Giovanni},
  title = {Warped Distance for Space-Variant Linear Image Interpolation},
  journal = {IEEE Trans. on Image Processing.},
  year = {1999},
  volume = {8},
  number = {5},
  abstract = {The problem of image interpolation using linear techniques is dealt
	with in this paper. Conventional spaceinvariant methods are revisited
	and changed into spacevariant ones, by introducing the concept of
	the Warped Distance among the pixels of an image. A better perceptual
	rendition of the image details is obtained in this way; this effect
	is proved both via the evaluation of the response to an idealized
	sigmoidal edge model and with experiments on real world images. The
	computational costs of the proposed approach are very small when
	compared to those of state-of-the-art nonlinear interpolation operators.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{RAS07,
  author = {Volker Rasche and Ludwig Binner and Friedrich Cavagna and Vinzenz
	Hombach and Markus Kunze and Jochen Spiess and Matthias Stuber and
	Nico Merkle},
  title = {Whole-heart coronary vein imaging: a comparison between non-contrast-agent-
	and contrast-agent-enhanced visualization of the coronary venous
	system.},
  journal = {Magn Reson Med},
  year = {2007},
  volume = {57},
  pages = {1019--1026},
  number = {6},
  month = {Jun},
  abstract = {The feasibility of three-dimensional (3D) whole-heart imaging of the
	coronary venous (CV) system was investigated. The hypothesis that
	coronary magnetic resonance venography (CMRV) can be improved by
	using an intravascular contrast agent (CA) was tested. A simplified
	model of the contrast in T(2)-prepared steady-state free precession
	(SSFP) imaging was applied to calculate optimal T(2)-preparation
	durations for the various deoxygenation levels expected in venous
	blood. Non-contrast-agent (nCA)- and CA-enhanced images were compared
	for the delineation of the coronary sinus (CS) and its main tributaries.
	A quantitative analysis of the resulting contrast-to-noise ratio
	(CNR) and signal-to-noise ratio (SNR) in both approaches was performed.
	Precontrast visualization of the CV system was limited by the poor
	CNR between large portions of the venous blood and the surrounding
	tissue. Postcontrast, a significant increase in CNR between the venous
	blood and the myocardium (Myo) resulted in a clear delineation of
	the target vessels. The CNR improvement was 347\% (P < 0.05) for
	the CS, 260\% (P < 0.01) for the mid cardiac vein (MCV), and 430\%
	(P < 0.05) for the great cardiac vein (GCV). The improvement in SNR
	was on average 155\%, but was not statistically significant for the
	CS and the MCV. The signal of the Myo could be significantly reduced
	to about 25\% (P < 0.001).},
  doi = {10.1002/mrm.21228},
  institution = {Department of Internal Medicine II, University Hospital of Ulm, Ulm,
	Germany. volker.rasche@uniklinik-ulm.de},
  keywords = {Adult; Contrast Media, administration /&/ dosage; Coronary Circulation;
	Feasibility Studies; Humans; Imaging, Three-Dimensional; Magnetic
	Resonance Imaging, methods; Male; Middle Aged; Organometallic Compounds,
	administration /&/ dosage; Veins},
  owner = {euHeart},
  pmid = {17534908},
  timestamp = {2009.02.16},
  url = {http://dx.doi.org/10.1002/mrm.21228}
}

@ARTICLE{RAY06,
  author = {Ray, Nicolas and Li, Wan Chiu and Levy, Bruno and Sheffer, Alla and
	Alliez, Pierre},
  title = {Periodic global parameterization},
  journal = {ACM Transactions on Graphics},
  year = {2006},
  volume = { 25},
  pages = {1460 - 1485},
  number = { 4},
  note = {Mesh processing;Parameterization;Geometry processing;},
  abstract = {We present a new globally smooth parameterization method for the triangulated
	surfaces of arbitrary topology. Given two orthogonal piecewise linear
	vector fields defined over the input mesh (typically the estimated
	principal curvature directions), our method computes two piecewise
	linear periodic functions, aligned with the input vector fields,
	by minimizing an objective function. The bivariate function they
	define is a smooth parameterization almost everywhere on the surface,
	except in the vicinity of singular vertices, edges, and triangles,
	where the derivatives of the parameterization vanish. We extract
	a quadrilateral chart layout from the parameterization function and
	propose an automatic procedure to detect the singularities, and fix
	them by splitting and reparameterizing the containing charts. Our
	method can construct both quasiconformal (angle preserving) and quasi-isometric
	(angle and area preserving) parameterizations. The more restrictive
	class of quasi-isometric parameterizations is constructed at the
	expense of introducing more singularities. The constructed parameterizations
	can be used for a variety of geometry processing applications. Since
	we can align the parameterization with the principal curvature directions,
	our result is particularly suitable for surface fitting and remeshing.
	&copy; 2006 ACM.},
  copyright = {Compilation and indexing terms, Copyright 2007 Elsevier Inc. All rights
	reserved},
  issn = {0730-0301},
  key = {Computer graphics},
  keywords = {Topology;Computational geometry;Curve fitting;Surfaces;},
  language = {English},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{REI67,
  author = {Reinsch, Christian H.},
  title = {Smoothing by Spline Functions},
  journal = {Numerisch Mathematik},
  year = {1967},
  volume = {10},
  pages = {177-183},
  note = {Numerisch Mathematik},
  keywords = {smoothing spline.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{REN08,
  author = {Renard, F. and Yang, Yongyi},
  title = {Coronary artery extraction and analysis for detection of soft plaques
	in MDCT images},
  booktitle = {Proc. 15th IEEE International Conference on Image Processing ICIP
	2008},
  year = {2008},
  pages = {2248--2251},
  abstract = {In this paper we aim to develop a computationally-efficient image-segmentation
	procedure for detection and quantification of soft plaques in coronary
	arteries from multidetector CT images. The proposed method consists
	of three steps: extraction of the arterial lumen centerline, segmentation
	of the lumen and arterial wall based on a locally-adaptive mixture-model
	using the expectation- maximization algorithm, and detection of soft
	plaques based on effective cross-sectional areas of the lumen and
	of the wall. Preliminary results using clinical acquisitions are
	presented to demonstrate the effectiveness of the proposed method.},
  doi = {10.1109/ICIP.2008.4712238},
  issn = {1522-4880},
  keywords = {cardiovascular system, computerised tomography, diseases, expectation-maximisation
	algorithm, feature extraction, image segmentation, medical image
	processing, MDCT image, adaptive mixture-model, arterial lumen centerline
	extraction, coronary artery extraction analysis, expectation-maximization
	algorithm, image-segmentation procedure, multidetector CT images,
	soft plaques detection, MDCT images, Vulnerable plaque, image segmentation,
	vessel tracking},
  owner = {euHeart},
  timestamp = {2009.05.04}
}

@INPROCEEDINGS{REU93.1,
  author = {Reuz\'e, P. and Coatrieux, J. L. and Luo, L. M. and Dillenseger,
	J. L.},
  title = {3-D vessel tracking and quantitation in angio MRI},
  booktitle = {Proc. IEEE Nineteenth Annual Northeast Bioengineering Conference},
  year = {1993},
  pages = {43--44},
  abstract = {A method for the three-dimensional (3-D) tracking and the quantitation
	of blood vessels from magnetic resonance imaging (MRI) is discussed.
	The approach is based on the 3-D geometrical moments and consists
	of the following steps: (1) interactive selection of 3-D seed points,
	(2) automatic tracking of the vessels, (3) local computation of both
	diameter and orientation, and (4) rendering of the vessel approximation
	model. The results on simulated and real data depict a good behavior
	of the detection and estimation scheme},
  doi = {10.1109/NEBC.1993.404423},
  keywords = {biomedical NMR, tracking, 3D geometrical moments, 3D seed points,
	3D vessel tracking, diameter, interactive selection, local computation,
	magnetic resonance imaging, medical diagnostic imaging, orientation,
	simulated data, vessel approximation model rendering},
  owner = {euHeart},
  timestamp = {2008.09.15}
}

@ARTICLE{REU93.2,
  author = {Reuz\'e, P. and Coatrieux, J.-L. and Luo, L. and Dillenseger, J.-L.},
  title = {A 3-D moment based approach for blood vessel detection and quantification
	in MRA},
  journal = {Technology and health care},
  year = {1993},
  volume = {1},
  pages = {181--188},
  number = {2},
  abstract = {This paper describes a new method for the three-dimensional (3-D)
	tracking and the quantification of blood vessels from Magnetic Resonance
	Angiography (MRA). The approach is based on the 3D geometrical moments
	and consists of the following steps : (1) interactive selection of
	3-D seed points ; (2) automatic tracking of the vessels ; (3) local
	computation of both diameter and orientation ; (4) rendering of the
	vessels. This detection and estimation scheme has been validated
	on simulated and real data.},
  owner = {euHeart},
  timestamp = {2008.09.24},
  url = {http://www.hal.inserm.fr/inserm-00135664/fr/}
}

@ARTICLE{RIO05,
  author = {Rioual, Kristell and Unanua, Edurne and Laguitton, Soizic and Garreau,
	Mireille and Boulmier, Dominique and Haigron, Pascal and Leclercq,
	Christophe and Coatrieux, Jean-Louis},
  title = {MSCT labelling for pre-operative planning in cardiac resynchronization
	therapy},
  journal = {Comput Med Imaging Graph},
  year = {2005},
  volume = {29},
  pages = {431--439},
  number = {6},
  month = sep,
  abstract = {The objective of this paper is twofold: (i) to show how multislice
	computed tomography (MSCT) data sets bring the information required
	for cardiac resynchronisation therapy (CRT) planning; (ii) to demonstrate
	the feasibility of 3D navigation into the veins where left ventricular
	leads have to be placed. The former has been achieved by exploring
	and labelling the cardiac structures of concern, the latter has been
	performed by using the concept of virtual navigation with high resolution
	surface detection and estimation algorithms.},
  issn = {0895-6111},
  keywords = {CRT, Pre-operative planning, Heart labelling, 3-D rendering},
  owner = {euHeart},
  publisher = {Elsevier Science},
  refid = {S0895-6111(05)00057-1},
  timestamp = {2008.09.09},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0895611105000571}
}

@ARTICLE{ROL08,
  author = {Marijn P Rolf and Rene ter Wee and Ton G van Leeuwen and Jos A E
	Spaan and Geert J Streekstra},
  title = {Diameter measurement from images of fluorescent cylinders embedded
	in tissue.},
  journal = {Med Biol Eng Comput},
  year = {2008},
  volume = {46},
  pages = {589--596},
  number = {6},
  month = {Jun},
  abstract = {Absorption and scattering of light by tissue as well as limitations
	in the resolution of the optical system influence the appearance
	of tissue embedded objects in fluorescence images and may reduce
	the accuracy of measurements from these images. Although the principles
	of light scattering in tissue and optical resolution are well known,
	the interplay between the two in fluorescence imaging in an imaging
	cryomicrotome is not well understood. In this paper we present and
	investigate an image formation model in a reflection geometry, like
	an imaging cryomicrotome, that takes both light scattering by tissue
	as well as the point spread function of the lens into account. The
	validity of the model was investigated by comparison of diameter
	estimates of fluorescent cylinders as obtained from images acquired
	in a reflection geometry with those estimated from simulated images.
	The results reveal that diameter values estimated from the simulated
	images are in excellent agreement with the experimental estimates.
	Our approach in modeling the image formation process of embedded
	fluorescent structures allows for the prediction of accuracy of quantitative
	estimates from fluorescence images. The relationship between imaging
	parameters and bias can be applied to arrive at accurate diameter
	estimates of near cylindrical structures like blood vessels.},
  doi = {10.1007/s11517-008-0328-9},
  institution = {Department of Medical Physics, Academic Medical Center, Meibergdreef
	9, 1105 AZ, Amsterdam, The Netherlands.},
  keywords = {Computer Simulation; Diagnostic Imaging, instrumentation/methods;
	Fluorescence; Humans; Image Interpretation, Computer-Assisted, methods;
	Imaging, Three-Dimensional, instrumentation/methods; Optics; Phantoms,
	Imaging; Scattering, Radiation},
  owner = {euHeart},
  pmid = {18365263},
  timestamp = {2008.09.26},
  url = {http://dx.doi.org/10.1007/s11517-008-0328-9}
}

@ARTICLE{RON94,
  author = {Ronfard, Remi},
  title = {Region-based strategies for active contour models},
  journal = {International Journal of Computer Vision},
  year = {1994},
  volume = {13},
  pages = {229--251},
  number = {2},
  month = oct,
  abstract = {The variational method has been introduced by Kass et al. (1987) in
	the field of object contour modeling, as an alternative to the more
	traditional edge detection-edge thinning-edge sorting sequence. since
	the method is based on a pre-processing of the image to yield an
	edge map, it shares the limitations of the edge detectors it uses.
	in this paper, we propose a modified variational scheme for contour
	modeling, which uses no edge detection step, but local computations
	insteadâ€”only around contour neighborhoodsâ€”as well as an â€œanticipatingâ€�
	strategy that enhances the modeling activity of deformable contour
	curves. many of the concepts used were originally introduced to study
	the local structure of discontinuity, in a theoretical and formal
	statement by leclerc & zucker (1987), but never in a practical situation
	such as this one. the first part of the paper introduces a region-based
	energy criterion for active contours, and gives an examination of
	its implications, as compared to the gradient edge map energy of
	snakes. then, a simplified optimization scheme is presented, accounting
	for internal and external energy in separate steps. this leads to
	a complete treatment, which is described in the last sections of
	the paper (4 and 5). the optimization technique used here is mostly
	heuristic, and is thus presented without a formal proof, but is believed
	to fill a gap between snakes and other useful image representations,
	such as split-and-merge regions or mixed line-labels image fields.},
  owner = {euHeart},
  timestamp = {2009.02.24},
  url = {http://dx.doi.org/10.1007/BF01427153}
}

@INPROCEEDINGS{RUE95,
  author = {Rueckert, Daniel and Burger, Peter},
  title = {Contour Fitting Using an Adaptive Spline Model},
  booktitle = {British Machine Vision},
  year = {1995},
  volume = {1},
  pages = {207-216},
  address = {Birmingham},
  abstract = {This paper presents a new segmentation algorithm by fitting active
	contour models (or snakes) to objects using adaptive splines. The
	adaptive spline model describes the contour of an object by a set
	of piecewisely interpolating C2 polynomial spline patches which are
	locally controlled. Thus the resulting description of the object
	contour is continuous and smooth. Polynomial splines provide a fast
	and efficient way for interpolating the object contour and allow
	us to compute its internal energy due to bending and elasticity deformations
	analytically. The adaptive spline model can be represented by its
	spline control points. The accuracy of the model is gradually increased
	during the segmentation process by inserting new control points.
	For estimating the optimal position of the control points, two different
	relaxation techniques based on Markov Random Fields (MRFs) have been
	combined and evaluated: Simulated Annealing (SA), which is a stochastic
	relaxation technique, and Iterated Conditional Modes (ICM), which
	is a probabilistic relaxation technique. We have studied convergence
	behavior and performance on artificial and medical images. The results
	show that the combination of both relaxation techniques provides
	very robust and initialization independent segmentation results.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{SAK05,
  author = {Hajime Sakuma and Yasutaka Ichikawa and Naohisa Suzawa and Tadanori
	Hirano and Katsutoshi Makino and Nozomu Koyama and Marc Van Cauteren
	and Kan Takeda},
  title = {Assessment of coronary arteries with total study time of less than
	30 minutes by using whole-heart coronary MR angiography.},
  journal = {Radiology},
  year = {2005},
  volume = {237},
  pages = {316--321},
  number = {1},
  month = {Oct},
  abstract = {This study had institutional review board approval, and all patients
	gave informed consent. The purpose of this study was to prospectively
	evaluate the use of whole-heart three-dimensional (3D) coronary magnetic
	resonance (MR) angiography in patients suspected of having coronary
	artery disease. Whole-heart coronary MR angiography was performed
	in 39 patients (30 men and nine women; mean age, 63.9 years +/- 15.6
	[standard deviation]) by using a steady-state free precession sequence
	with free breathing. Twenty patients (16 men and four women; mean
	age, 64.9 years +/- 11.7) also underwent conventional coronary angiography.
	MR angiography was successfully completed in 34 of 39 patients (87\%);
	the average imaging time was 13.8 minutes +/- 3.8. Sensitivity and
	specificity of MR angiography for detecting significant stenosis
	were 82\% (14 of 17 arteries) and 91\% (39 of 43 arteries), respectively.
	Whole-heart coronary MR angiography with a navigator-gated steady-state
	sequence can enable reliable 3D visualization of the coronary arteries
	in patients suspected of having coronary artery disease.},
  doi = {10.1148/radiol.2371040830},
  institution = {Department of Radiology, Mie University Hospital, 2-174 Edobashi,
	Tsu, Mie 514-8507, Japan. Sakuma@clin.medic.mie-u.ac.jp},
  keywords = {Coronary Angiography; Coronary Disease, diagnosis; Coronary Vessels,
	pathology; Female; Humans; Magnetic Resonance Angiography, methods;
	Male; Middle Aged; Prospective Studies; Sensitivity and Specificity;
	Time Factors},
  language = {eng},
  medline-pst = {ppublish},
  owner = {euHeart},
  pii = {2371040830},
  pmid = {16126921},
  timestamp = {2010.01.07},
  url = {http://dx.doi.org/10.1148/radiol.2371040830}
}

@INPROCEEDINGS{SAN95,
  author = {Sanson, Henri},
  title = {Reconstruction-model-based snake applied to optimal motion contour
	positioning},
  booktitle = {Visual Communications and Image Processing '95, May 24-26 95},
  year = {1995},
  volume = {2501/2},
  series = {Proceedings of SPIE - The International Society for Optical Engineering},
  pages = {846-856},
  address = {Taipei, Taiwan},
  publisher = {Society of Photo-Optical Instrumentation Engineers, Bellingham, WA,
	USA},
  note = {TY - CONF U1 - 95052696199 L2 - http://dx.doi.org/10.1117/12.206790
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Motion contour positioning Snake concepts
	Parametric modeling Curve evolution Motion boundary localization
	Spline curves},
  abstract = {This paper addresses the problem of optimal positioning of a contour
	separating two moving regions using snake concepts. After a brief
	recall of classical snake methodology, an alternative approach is
	proposed, based on a reconstruction criterion for the regions delimited
	by the curve, and the use of parametric modeling of both the region
	textures and boundaries. A generic adaptive step gradient algorithm
	is formulated for solving the curve evolution problem, independently
	of the models used. The method is then more specifically applied
	to motion boundary localization, where the texture of mobile regions
	is reconstructed by motion compensation, using polynomial motion
	models. The generic optimization algorithm is applied to motion frontiers
	defined by B- spline curves. Detailed implementation of this method
	in this particular case is described, and considerations about its
	behavior are given. Some experimental results are finally reported,
	attesting the interest of the proposed approach.},
  keywords = {Mathematical models Adaptive algorithms Textures Polynomials Optimization
	Mathematical techniques Image reconstruction},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{SCA99,
  author = {P. J. Scanlon and D. P. Faxon and A. M. Audet and B. Carabello and
	G. J. Dehmer and K. A. Eagle and R. D. Legako and D. F. Leon and
	J. A. Murray and S. E. Nissen and C. J. Pepine and R. M. Watson and
	J. L. Ritchie and R. J. Gibbons and M. D. Cheitlin and T. J. Gardner
	and A. Garson and R. O. Russell and T. J. Ryan and S. C. Smith},
  title = {ACC/AHA guidelines for coronary angiography. A report of the American
	College of Cardiology/American Heart Association Task Force on practice
	guidelines (Committee on Coronary Angiography). Developed in collaboration
	with the Society for Cardiac Angiography and Interventions.},
  journal = {J Am Coll Cardiol},
  year = {1999},
  volume = {33},
  pages = {1756--1824},
  number = {6},
  month = {May},
  keywords = {Coronary Angiography; Coronary Disease, mortality/radiography/therapy;
	Humans; Prognosis},
  owner = {euHeart},
  pii = {S0735-1097(99)00126-6},
  pmid = {10334456},
  timestamp = {2009.05.06}
}

@ARTICLE{SCH09,
  author = {Michiel Schaap and Coert T Metz and Theo van Walsum and Alina G van
	der Giessen and Annick C Weustink and Nico R Mollet and Christian
	Bauer and Hrvoje Bogunović and Carlos Castro and Xiang Deng and Engin
	Dikici and Thomas O'Donnell and Michel Frenay and Ola Friman and
	Marcela Hernández Hoyos and Pieter H Kitslaar and Karl Krissian and
	Caroline Kühnel and Miguel A Luengo-Oroz and Maciej Orkisz and Orjan
	Smedby and Martin Styner and Andrzej Szymczak and Hüseyin Tek and
	Chunliang Wang and Simon K Warfield and Sebastian Zambal and Yong
	Zhang and Gabriel P Krestin and Wiro J Niessen},
  title = {Standardized evaluation methodology and reference database for evaluating
	coronary artery centerline extraction algorithms.},
  journal = {Med Image Anal},
  year = {2009},
  volume = {13},
  pages = {701--714},
  number = {5},
  month = {Oct},
  __markedentry = {[dje]},
  abstract = {Efficiently obtaining a reliable coronary artery centerline from computed
	tomography angiography data is relevant in clinical practice. Whereas
	numerous methods have been presented for this purpose, up to now
	no standardized evaluation methodology has been published to reliably
	evaluate and compare the performance of the existing or newly developed
	coronary artery centerline extraction algorithms. This paper describes
	a standardized evaluation methodology and reference database for
	the quantitative evaluation of coronary artery centerline extraction
	algorithms. The contribution of this work is fourfold: (1) a method
	is described to create a consensus centerline with multiple observers,
	(2) well-defined measures are presented for the evaluation of coronary
	artery centerline extraction algorithms, (3) a database containing
	32 cardiac CTA datasets with corresponding reference standard is
	described and made available, and (4) 13 coronary artery centerline
	extraction algorithms, implemented by different research groups,
	are quantitatively evaluated and compared. The presented evaluation
	framework is made available to the medical imaging community for
	benchmarking existing or newly developed coronary centerline extraction
	algorithms.},
  doi = {10.1016/j.media.2009.06.003},
  institution = {Biomedical Imaging Group Rotterdam, Dept. of Radiology and Med. Informatics,
	Erasmus MC, Rotterdam, The Netherlands. michiel.schaap@erasmusmc.nl},
  keywords = {Algorithms; Coronary Angiography, standards; Humans; Netherlands;
	Pattern Recognition, Automated, standards; Radiographic Image Enhancement,
	methods/standards; Radiographic Image Interpretation, Computer-Assisted,
	methods/standards; Reference Values; Reproducibility of Results;
	Sensitivity and Specificity; Software Validation; Software, standards;
	Tomography, X-Ray Computed, standards},
  language = {eng},
  medline-pst = {ppublish},
  owner = {dje},
  pii = {S1361-8415(09)00047-4},
  pmid = {19632885},
  timestamp = {2009.12.12},
  url = {http://dx.doi.org/10.1016/j.media.2009.06.003}
}

@BOOK{SCH05,
  title = {CT of the heart : principles and applications},
  publisher = {Humana Press},
  year = {2005},
  author = {Schoepf, U. J},
  address = {Totowa, N.J},
  isbn = {1588293033},
  owner = {euHeart},
  timestamp = {2009.05.05},
  url = {http://millenium.itesm.mx/record=i2709774\&searchscope=0}
}

@ARTICLE{SCH002,
  author = {S. Schroeder and A. F. Kopp and B. Ohnesorge and H. Loke-Gie and
	A. Kuettner and A. Baumbach and C. Herdeg and C. D. Claussen and
	K. R. Karsch},
  title = {Virtual coronary angioscopy using multislice computed tomography.},
  journal = {Heart},
  year = {2002},
  volume = {87},
  pages = {205--209},
  number = {3},
  month = {Mar},
  abstract = {BACKGROUND: With faster image acquisition times and thinner slice
	widths, multislice detector computed tomography (MSCT) allows visualisation
	of human coronary arteries with diagnostic image quality. In addition
	to conventional axial slices, virtual coronary angioscopies (VCA)
	can be reconstructed using MSCT datasets. OBJECTIVE: To evaluate
	the feasibility of reconstructing VCA and to determine the clinical
	value of this new application in detecting atherosclerotic coronary
	artery lesions. METHODS: Datasets obtained by contrast enhanced non-invasive
	coronary angiography using MSCT (Somatom VZ) were analysed from 14
	consecutive patients. VCA were simulated in 14 coronary arteries
	(left anterior descending, n = 7; right coronary, n = 7). Lesion
	detection was undertaken on conventional contrast enhanced axial
	slices, as well as by VCA. Intracoronary ultrasound (ICUS) was used
	as the gold standard for in vivo plaque detection. RESULTS: 38 lesions
	were detected both on ICUS and on axial slices: 14 severe target
	lesions of > 75\% area stenosis (11 calcified, three non-calcified),
	and 24 intermediate lesions of < or = 75\% area stenosis (seven calcified,
	17 non-calcified). Using VCA, all severe lesions (n = 14) and all
	calcified intermediate plaques (n = 7) could clearly be identified.
	However, non-calcified intermediate lesions (n = 17) could not be
	accurately distinguished from the vessel wall; they were recognised
	as vessel wall alterations without significant luminal narrowing.
	CONCLUSIONS: Current MSCT technology allows reconstruction of VCA
	with good image quality. Despite a more anatomical view of heart
	and coronary vessels on three dimensional reconstruction, conventional
	axial slices were found to be superior for detecting coronary lesions.
	Thus further technical innovations are required before VCA can become
	a useful technique in clinical cardiology.},
  institution = {Department of Internal Medicine, Division of Cardiology Eberhard-Karls-University,
	Tuebingen, Germany. stephen.schroeder@med.uni-tuebingen.de},
  keywords = {Adult; Aged; Angina Pectoris, radiography/ultrasonography; Computer
	Simulation, standards; Coronary Angiography, methods; Coronary Artery
	Disease, radiography/ultrasonography; Coronary Stenosis, radiography/ultrasonography;
	Diagnosis, Computer-Assisted; Female; Humans; Male; Middle Aged;
	Pilot Projects; Radiographic Image Interpretation, Computer-Assisted;
	Sensitivity and Specificity; Tomography, X-Ray Computed, methods},
  language = {eng},
  medline-pst = {ppublish},
  owner = {euHeart},
  pmid = {11847152},
  timestamp = {2010.01.06}
}

@BOOK{SCH87,
  title = {Infographie II : Transformation et d\'ecoupage, courbes et surfaces,
	normes, visibilit\'e, rendu r\'ealiste},
  publisher = {Presses Polytechniques Romandes},
  year = {1987},
  author = {Philippe Schweizer},
  address = {Lausanne},
  isbn = {978-2880741372},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{SHA85,
  author = {Shahraray, Behzad and Anderson, David J.},
  title = {Uniform Resampling of Digitized Contours},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {1985},
  volume = {PAMI-7},
  pages = {674-681},
  number = {6},
  note = {TY - JOUR U1 - 86060026402 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc. U2 - IMAGE SHAPE ANALYSIS},
  abstract = {The problem of the nonuniform intersample distance of digital curves
	obtained as a result of quantization of continuous contours on a
	square lattice is studied. A resampling algorithm based on variable-factor
	interpolation and decimation is presented, and its performance is
	evaluated analytically and by computer simulations assuming the grid-intersect
	quantization. It is shown that the output of the resampling algorithm
	outperforms the original digital curve in terms of the average and
	maximum error in the measurement of length by 50 and 73 percent,
	respectively.},
  keywords = {COMPUTER PROGRAMMING - Algorithms COMPUTER SIMULATION IMAGE PROCESSING
	-- Image Analysis},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{SHI04,
  author = {Shih, Frank Y. and Zhang, Kai},
  title = {Efficient contour detection based on improved snake model},
  journal = {International Journal of Pattern Recognition and Artificial Intelligence},
  year = {2004},
  volume = {18},
  pages = {197-209},
  number = {2},
  note = {TY - JOUR U1 - 04208160035 L2 - http://dx.doi.org/10.1142/S0218001404003149
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Active contour model Snake Edge preserving
	smoothing Gradient vector flow},
  abstract = {Active contour model, also called snake, adapts to edges in an image.
	A snake is defined as an energy minimizing spline - the snake's energy
	depends on its shape and location within the image. Problems associated
	with initialization and poor convergence to boundary concavities,
	however, have limited its utility. In this paper, we present a new
	external force field, named gravitation force field, for the snake
	model. We associate this force field with edge preserving smoothing
	to drive the snake for solving the problems. Our gravitation force
	field uses gradient values as particles to construct force field
	in the whole image. This force field will attract the active contour
	toward the edge boundary. The locations of the initial contour are
	very flexible, such that they can be very far away from the objects
	and can be inside, outside, or the mixture. The improved snake can
	converge toward the object boundary in a fast pace.},
  keywords = {Artificial intelligence Image processing Gravitation Constraint theory
	Image segmentation Convergence of numerical methods Vectors Functions
	Edge detection},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{SHU08,
  author = {Shu, Huazhong and Luo, Limin and Coatrieux, J. -L.},
  title = {Moment-based approaches in imaging part 3: computational considerations
	[A Look at...]},
  journal = IEEE_M_EMB,
  year = {2008},
  volume = {27},
  pages = {89--91},
  number = {3},
  abstract = {This paper discusses the computation of moments in imaging. Faster
	algorithms are needed to decrease the computational complexity for
	real-time environment applications.},
  doi = {10.1109/MEMB.2008.918690},
  issn = {0739-5175},
  keywords = {medical image processing, method of moments, computation of moments,
	computational complexity, fast algorithm, image analysis method,
	imaging, moment-based approach, real-time environment application},
  owner = {euHeart},
  timestamp = {2008.09.15}
}

@ARTICLE{SHU07,
  author = {Shu, Huazhong and Luo, Limin and Coatrieux, J. -L. },
  title = {Moment-Based Approaches in Imaging. 1. Basic Features [A Look At
	...]},
  journal = IEEE_M_EMB,
  year = {2007},
  volume = {26},
  pages = {70--74},
  number = {5},
  abstract = {This first article was aimed at providing the basic formulations of
	moments, a classification, and an introductory bibliography. This
	first part presents a classification of moments, and, rather than
	entering into theoretical details, it sketches their different expressions.
	The companion articles will review their properties and the potential
	contributions they already bring to imaging.},
  doi = {10.1109/EMB.2007.906026},
  issn = {0739-5175},
  keywords = {biomedical imaging, medical image processing, method of moments, biomedical
	imaging, image processing, moment-based approaches},
  owner = {euHeart},
  timestamp = {2008.09.15}
}

@BOOK{SIL80,
  title = {Dictionnaire encyclop\'edique de psychologie},
  publisher = {Bordas},
  year = {1980},
  author = {Norbert Sillamy},
  volume = {2},
  pages = {753},
  isbn = {2-04-008040-6},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{SIM05.1,
  author = {Simon, Antoine and Garreau, Mireille and Boulmier, Dominique and
	Coatrieux, Jean-Louis and Le Breton, Hervé},
  title = {A Surface-Volume Matching Process Using a Markov Random Field Model
	for Cardiac Motion Extraction in MSCT Imaging},
  booktitle = {Functional Imaging and Modeling of the Heart},
  year = {2005},
  pages = {457--466},
  abstract = {Multislice Computed Tomography (MSCT) scanners offers new perspectives
	for cardiac kinetics evaluation with 3D time image sequences of high
	contrast and spatio-temporal resolutions. A new method is proposed
	for cardiac motion extraction in Multislice CT. Based on a 3D surface-volume
	matching process, it provides the detection of the heart left cavities
	along the acquired sequence and the estimation of their 3D surface
	velocity fields. A 3D segmentation step and surface reconstruction
	process are first applied on only one image of the sequence to obtain
	a 3D mesh representation for one t time. A Markov Random Field model
	is defined to find best correspondences between 3D mesh nodes at
	t time and voxels in the next volume at t + 1 time. A simulated annealing
	is used to perform a global optimization of the correspondences.
	First results obtained on simulated and real data show the good behaviour
	of this method.},
  owner = {euHeart},
  timestamp = {2008.09.10},
  url = {http://dx.doi.org/10.1007/11494621_45}
}

@INPROCEEDINGS{SIM05.2,
  author = {Simon, A. and Garreau, M. and Boulmier, D. and Toumoulin, C. and
	Le Breton, H. },
  title = {Cardiac motion estimation in multislice computed tomography imaging
	using a 4D multiscale surface-volume matching process},
  booktitle = {Proc. Computers in Cardiology},
  year = {2005},
  pages = {219--222},
  owner = {euHeart},
  timestamp = {2008.09.10}
}

@ARTICLE{SIN05,
  author = {Jagmeet P Singh and Stuart Houser and E. Kevin Heist and Jeremy N
	Ruskin},
  title = {The coronary venous anatomy: a segmental approach to aid cardiac
	resynchronization therapy.},
  journal = {J Am Coll Cardiol},
  year = {2005},
  volume = {46},
  pages = {68--74},
  number = {1},
  month = {Jul},
  abstract = {The coronary sinus is the gateway for left ventricular (LV) epicardial
	lead placement for cardiac resynchronization therapy. The implanting
	electrophysiologist is usually challenged by a high degree of variability
	in the coronary venous anatomy, making it important to have a more
	consistent and uniform segmental approach to describe the coronary
	venous tree and its branches. Classifying the coronary sinus branches
	and tributaries by the segment of their location rather than by conventional
	anatomic names (i.e., middle cardiac vein, great cardiac vein, and
	so on), would provide more relevant anatomic and functional information
	at the time of LV lead placement. This would enable the implanting
	physician to proactively correlate the venous anatomy with the segmental
	wall motion abnormalities or dyssynchrony, as defined by echocardiography
	and other imaging modalities. The current viewpoint calls for a more
	systematic segmental approach for describing the coronary venous
	anatomy.},
  doi = {10.1016/j.jacc.2005.04.017},
  institution = {Cardiac Arrhythmia Service, Massachusetts General Hospital, Harvard
	Medical School, Boston, Massachusetts 02114, USA. jsingh@partners.org},
  keywords = {Cardiac Pacing, Artificial; Coronary Angiography; Coronary Vessels,
	anatomy /&/ histology; Heart Ventricles, anatomy /&/ histology/radiography;
	Humans; Phlebography; Terminology as Topic; Tomography, X-Ray Computed},
  owner = {euHeart},
  pii = {S0735-1097(05)00900-9},
  pmid = {15992638},
  timestamp = {2009.05.19},
  url = {http://dx.doi.org/10.1016/j.jacc.2005.04.017}
}

@ARTICLE{SPA05,
  author = {J. A E Spaan and R. ter Wee and J. W G E van Teeffelen and G. Streekstra
	and M. Siebes and C. Kolyva and H. Vink and D. S. Fokkema and E.
	VanBavel},
  title = {Visualisation of intramural coronary vasculature by an imaging cryomicrotome
	suggests compartmentalisation of myocardial perfusion areas.},
  journal = {Med Biol Eng Comput},
  year = {2005},
  volume = {43},
  pages = {431--435},
  number = {4},
  month = {Jul},
  abstract = {A technique is presented for the 3D visualisation of the coronary
	arterial tree using an imaging cryomicrotome. After the coronary
	circulation of the excised heart was filled with a fluorescent plastic,
	the heart was frozen and mounted in the cryomicrotome. The heart
	was then sliced serially, with a slice thickness of 40 microm, and
	digital images were taken from each cutting plane of the remaining
	bulk material using appropriate excitation and emission filters.
	Using maximum intensity projections over a series of images in the
	cutting plane and perpendicular plane, the structural organisation
	of intramural vessels was visualised in the present study. The branching
	end in the smallest visible vessels, which define tissue areas that
	are well delineated from each other by 1-2 mm wide bands populated
	only by vessels less than 40 microm in diameter. The technique presented
	here allows further quantification in the future of the 3D structure
	of the coronary arterial tree by image analysis techniques.},
  institution = {Department of Medical Physics, Cardiovascular Research Institute
	Amsterdam, The Netherlands. j.a.spaan@amc.uva.nl},
  keywords = {Animals; Coronary Circulation; Coronary Vessels, anatomy /&/ histology;
	Cryopreservation; Goats; Image Processing, Computer-Assisted, methods;
	Imaging, Three-Dimensional, methods; Specimen Handling, methods},
  owner = {euHeart},
  pmid = {16255423},
  timestamp = {2008.09.26}
}

@ARTICLE{SPA08,
  author = {Jos Spaan and Christina Kolyva and Jeroen van den Wijngaard and Rene
	ter Wee and Pepijn van Horssen and Jan Piek and Maria Siebes},
  title = {Coronary structure and perfusion in health and disease.},
  journal = {Philos Transact A Math Phys Eng Sci},
  year = {2008},
  volume = {366},
  pages = {3137--3153},
  number = {1878},
  month = {Sep},
  abstract = {Blood flow is distributed through the heart muscle via a system of
	vessels forming the coronary circulation. The perfusion of the myocardium
	can be hampered by atherosclerosis creating localized obstructions
	in the epicardial vessels or by microvascular disease. In early stages
	of the disease, these impediments to blood flow are offset by dilation
	of the resistance vessels, which normally compensates for a decrease
	in perfusion pressure or increased metabolism. However, this dilatory
	reserve can become exhausted, which in general occurs first at the
	deeper layers of the heart wall where intramural vessels are subjected
	to compressive forces related to heart contraction. In the catheterization
	laboratory, guide wires of 0.33 mm diameter are available that are
	equipped with a pressure and flow velocity sensor at the tip, which
	can be positioned distal to the stenosis. These signals provide information
	about the impediment of the stenosis on coronary flow and allow for
	the evaluation of the status of the microcirculation. However, the
	interpretation of these signals is strongly model-dependent and therefore
	it is of paramount importance to develop realistic models reflecting
	the anatomy and unique physiology of the coronary circulation.},
  doi = {10.1098/rsta.2008.0075},
  institution = {Department of Medical Physics, Academic Medical Center, University
	of Amsterdam, Meibergdreef 15, 1105 AZ Amsterdam, The Netherlands.
	j.a.spaan@amc.uva.nl},
  owner = {euHeart},
  pii = {Y54R4341862N6853},
  pmid = {18559321},
  timestamp = {2008.09.26},
  url = {http://dx.doi.org/10.1098/rsta.2008.0075}
}

@ARTICLE{STA92,
  author = {Staib, Lawrence H. and Duncan, James S.},
  title = {Boundary finding with parametrically deformable models},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {1992},
  volume = {14},
  pages = {1061-1075},
  number = {11},
  note = {TY - JOUR U1 - 93020702816 L2 - http://dx.doi.org/10.1109/34.166621
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Image segmentation Parametrically deofrmable
	models Boundary finding Global shape information Deformable model
	Maximum a posteriori objective functions},
  abstract = {Segmentation using boundary finding is enhanced both by considering
	the boundary as a whole and by using model-based global shape information.
	Previous boundary finding methods have either not used global shape
	or have designed individual shape models specific to particular shapes.
	The authors apply flexible constraints, in the form of a probabilistic
	deformable model, to the problem of segmenting natural 2-D objects
	whose diversity and irregularity of shape make them poorly represented
	in terms of fixed features or form. The parametric model is based
	on the elliptic Fourier decomposition of the boundary. Probability
	distributions on the parameters of the representation bias the model
	to a particular overall shape while allowing for deformations. Boundary
	finding is formulated as an optimization problem using a maximum
	a posteriori objective function. Results of the method applied to
	real and synthetic images are presented, including an evaluation
	of the dependence of the method on prior information and image quality.},
  keywords = {Probability Optimization Image analysis},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{STA03,
  author = {Jos Stam},
  title = {Flows on surfaces of arbitrary topology},
  booktitle = {SIGGRAPH '03: ACM SIGGRAPH 2003 Papers},
  year = {2003},
  pages = {724--731},
  address = {New York, NY, USA},
  publisher = {ACM},
  abstract = {In this paper we introduce a method to simulate fluid flows on smooth
	surfaces of arbitrary topology: an effect never seen before. We achieve
	this by combining a two-dimensional stable fluid solver with an atlas
	of parametrizations of a Catmull-Clark surface. The contributions
	of this paper are: (i) an extension of the Stable Fluids solver to
	arbitrary curvilinear coordinates, (ii) an elegant method to handle
	cross-patch boundary conditions and (iii) a set of new external forces
	custom tailored for surface flows. Our techniques can also be generalized
	to handle other types of processes on surfaces modeled by partial
	differential equations, such as reaction-diffusion. Some of our simulations
	allow a user to interactively place densities and apply forces to
	the surface, then watch their effects in real-time. We have also
	computed higher resolution animations of surface flows off-line.},
  doi = {http://doi.acm.org/10.1145/1201775.882338},
  isbn = {1-58113-709-5},
  location = {San Diego, California},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{STE06,
  author = {Steiner, D. and Fischer, A.},
  title = {Automatic remeshing by mapping a 2D grid on 3D genus-g meshes based
	on topological analysis},
  journal = {CAD Computer Aided Design},
  year = {2006},
  volume = {38},
  pages = {887 - 901},
  number = { 8},
  note = {3D genus-g meshes;Remeshing;Topological analysis;},
  abstract = {Remeshing of 3D meshes is important for many CAD, visualization and
	analysis applications. Current remeshing methods for closed manifold
	genus-g meshes usually involve 3D mesh operations, such as splitting
	and merging the mesh edges, in order to construct a new mesh that
	will satisfy given geometrical criteria. These 3D operations are
	local and usually do not lead to the desired new mesh. Indeed, the
	remeshed model usually does not satisfy regular criteria. Moreover,
	changing one of the criteria will lead to reactivating the remeshing
	process from scratch. In this paper, we propose a novel remeshing
	approach which overcomes the above problems. It can be applied on
	3D genus-g meshes (g &gt; 0) which are 2D manifold and triangular.
	The proposed approach is based on continuously mapping a 2D grid
	on the 3D genus-g meshes. The criteria of the new mesh can be defined
	directly on the 2D grid. The remeshing is invariant to the original
	triangular mesh. The mapping is based on a new parameterization technique,
	also developed by the authors, which utilizes the topologic analysis
	of the object. As a result, the mapping of the 2D grid into a 3D
	mesh minimizes distortion and guarantees continuity. In addition,
	the 2D grid, element shapes and density can be modified straightforwardly,
	thus making the approach modular. In the paper, the remeshing approach
	and the parameterization (mapping) technique are described in detail,
	and the feasibility of the remeshing method is demonstrated on complex
	3D genus-g objects. &copy; 2006 Elsevier Ltd. All rights reserved.},
  copyright = {Compilation and indexing terms, Copyright 2006 Elsevier Inc. All rights
	reserved},
  issn = {0010-4485},
  key = {Computational geometry},
  keywords = {Computer aided design;Topology;Large scale systems;Mathematical models;},
  language = {English},
  owner = {euHeart},
  timestamp = {2009.02.24},
  url = {http://dx.doi.org/10.1016/j.cad.2006.04.016}
}

@ARTICLE{STE04,
  author = {Steiner, Dvir and Fischer, Anath},
  title = {Finding and defining the generators of genus-n objects for constructing
	topological and cut graphs},
  journal = {Visual Computer},
  year = {2004},
  volume = {20},
  pages = {266 - 278},
  number = { 4},
  note = {Shape classification;Topological graph;Cut graph;Longitudes;Meridian;},
  abstract = {The topology of an object is commonly represented through a topological
	graph or a cut graph (polygonal scheme). Over the past few years,
	many studies have focused on extracting the topological and cut graphs
	of complex freeform objects that are represented by meshes. For an
	object with genus-n, the topological graph has n cycles, while the
	cut graph contains 2n cycles. These loops, however, do not always
	explicitly represent the holes in the objects. That is, a cycle in
	the graph can be a cycle around a solid (meridian), a cycle around
	a hole (longitude), or almost any combination of the two. The task
	of classifying the cycles (generators) as cycles around holes (longitude)
	and cycles around solids (meridians) on the mesh is not straightforward.
	Every closed orientable 2-manifold with genus-n can be seen as a
	collection of n toruses stitched together, so that each hole in the
	object can be referred to as a torus with two generators. This paper
	proposes a method that extracts the generators from which the longitudes
	and the meridians are found. The topological graph is defined by
	the longitudes and by a spanning tree constructed between them. The
	cut graph is constructed using the same concept. The advantage of
	the proposed method over other methods is that each loop in the topological
	graph explicitly represents a hole in the object.},
  copyright = {Compilation and indexing terms, Copyright 2006 Elsevier Inc. All rights
	reserved},
  issn = {0178-2789},
  key = {Computer graphics},
  keywords = {Image processing;Pattern matching;Vectors;Trees (mathematics);Mathematical
	models;Algorithms;},
  language = {English},
  owner = {euHeart},
  timestamp = {2009.02.24},
  url = {http://dx.doi.org/10.1007/s00371-003-0232-0}
}

@ARTICLE{STI03,
  author = {\'Eric Stindel},
  title = {Chirurgie Orthop\'edique Assist\'ee par Ordinateur : Utilisation
	Per-op\'eratoire des Mod\`eles Statistiques D\'eformables},
  journal = {Signal},
  year = {2003},
  volume = {98},
  pages = {17-22},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{STO95,
  author = {Stonick, V.L.},
  title = {Time-varying performance surfaces for adaptive IIR filters: geometric
	properties and implications for filter stability},
  journal = {IEEE Transactions on Signal Processing},
  year = {1995},
  volume = {43},
  pages = {29--42},
  number = {1},
  booktitle = {Signal Processing, IEEE Transactions on [see also Acoustics, Speech,
	and Signal Processing, IEEE Transactions on]},
  issn = {1053-587X},
  keywords = {IIR filters, adaptive filters, filtering theory, stability, time-varying
	filters, adaptive IIR filtering, adaptive IIR filters, adaptive algorithms,
	data-dependent descent directions, error surface, exact z-domain
	methods, filter coefficients, filter stability, geometric properties,
	gradients, iteration, on-line operation, on-line stability problems,
	stability performance, time-varying performance surfaces},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{STU07,
  author = {Matthias Stuber and Robert G Weiss},
  title = {Coronary magnetic resonance angiography.},
  journal = {J Magn Reson Imaging},
  year = {2007},
  volume = {26},
  pages = {219--234},
  number = {2},
  month = {Aug},
  abstract = {Coronary magnetic resonance angiography (MRA) is a powerful noninvasive
	technique with high soft-tissue contrast for the visualization of
	the coronary anatomy without X-ray exposure. Due to the small dimensions
	and tortuous nature of the coronary arteries, a high spatial resolution
	and sufficient volumetric coverage have to be obtained. However,
	this necessitates scanning times that are typically much longer than
	one cardiac cycle. By collecting image data during multiple RR intervals,
	one can successfully acquire coronary MR angiograms. However, constant
	cardiac contraction and relaxation, as well as respiratory motion,
	adversely affect image quality. Therefore, sophisticated motion-compensation
	strategies are needed. Furthermore, a high contrast between the coronary
	arteries and the surrounding tissue is mandatory. In the present
	article, challenges and solutions of coronary imaging are discussed,
	and results obtained in both healthy and diseased states are reviewed.
	This includes preliminary data obtained with state-of-the-art techniques
	such as steady-state free precession (SSFP), whole-heart imaging,
	intravascular contrast agents, coronary vessel wall imaging, and
	high-field imaging. Simultaneously, the utility of electron beam
	computed tomography (EBCT) and multidetector computed tomography
	(MDCT) for the visualization of the coronary arteries is discussed.},
  doi = {10.1002/jmri.20949},
  institution = {Department of Radiology, Division of Magnetic Resonance Research,
	Johns Hopkins University, Baltimore, Maryland, USA. mstuber@mri.jhu.edu},
  keywords = {Animals; Contrast Media; Coronary Angiography, instrumentation/methods;
	Coronary Vessels, pathology; Heart, physiology; Humans; Image Processing,
	Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance
	Angiography, instrumentation/methods; Movement; Myocardium, pathology;
	X-Rays},
  owner = {euHeart},
  pmid = {17610288},
  timestamp = {2009.02.16},
  url = {http://dx.doi.org/10.1002/jmri.20949}
}

@ARTICLE{SUH04,
  author = {Suhling, M. and Arigovindan, M. and Hunziker, P. and Unser, M.},
  title = {Multiresolution moment filters: theory and applications},
  journal = IEEE_J_IP,
  year = {2004},
  volume = {13},
  pages = {484--495},
  number = {4},
  abstract = {We introduce local weighted geometric moments that are computed from
	an image within a sliding window at multiple scales. When the window
	function satisfies a two-scale relation, we prove that lower order
	moments can be computed efficiently at dyadic scales by using a multiresolution
	wavelet-like algorithm. We show that B-splines are well-suited window
	functions because, in addition to being refinable, they are positive,
	symmetric, separable, and very nearly isotropic (Gaussian shape).
	We present three applications of these multiscale local moments.
	The first is a feature-extraction method for detecting and characterizing
	elongated structures in images. The second is a noise-reduction method
	which can be viewed as a multiscale extension of Savitzky-Golay filtering.
	The third is a multiscale optical-flow algorithm that uses a local
	affine model for the motion field, extending the Lucas-Kanade optical-flow
	method. The results obtained in all cases are promising.},
  doi = {10.1109/TIP.2003.819859},
  issn = {1057-7149},
  keywords = {filtering theory, image resolution, image sequences, method of moments,
	splines (mathematics), wavelet transforms, B-splines, Lucas-Kanade
	optical-flow method, Savitzky-Golay filtering, dyadic scales, feature
	extraction, image analysis, local weighted geometric moments, multiresolution
	moment filters, multiresolution wavelet-like algorithm, multiscale
	local moments, multiscale optical-flow algorithm, noise reduction,
	sliding window, structure detection, two-scale relation, window function},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{SZE93,
  author = {Szeliski, Richard and Tonnessen, David and Terzopoulos, Demetri},
  title = {Curvature and continuity control in particle-based surface models},
  booktitle = {Geometric Methods in Computer Vision II, Jul 12-13 1993},
  year = {1993},
  volume = {2031},
  series = {Proceedings of SPIE - The International Society for Optical Engineering},
  pages = {172-181},
  address = {San Diego, CA, USA},
  publisher = {Publ by Society of Photo-Optical Instrumentation Engineers, Bellingham,
	WA, USA},
  note = {TY - CONF U1 - 93081061649 L2 - http://dx.doi.org/10.1117/12.146623
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Particle based surface models 3D curves |s
	ThreeD curves},
  abstract = {This paper develops techniques to locally control curvature and continuity
	in particle-based surface models. Such models are a generalization
	of traditional spline surfaces built out of triangular patches. Traditional
	splines require the topology of the triangular mesh to be specified
	ahead of time. In contrast, particle-based surface models compute
	the topology dynamically as a function of the relative node positions,
	and can add or delete nodes as required. Such models are particularly
	important in computer vision and other inverse problems, where the
	topology of the surface being reconstructed is usually not known
	a priori. We develop techniques for both locally controlling the
	curvature of the surface (through additional state at each node),
	and for adapting the triangulation to surface curvature (by concentrating
	more particles in areas of high curvature). We show how the same
	ideas can also be applied to 3-D curves, which results in a flexible
	version of traditional dynamic contours (snakes).},
  keywords = {Surfaces Image reconstruction Computer vision},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{TAI94,
  author = {Taine, M.-C. and Herment, A. and Diebold, B. and Peronneau, P.},
  title = {Segmentation of cardiac and vascular ultrasound images with extension
	to border kinetics},
  booktitle = {Proceedings of the 1994 IEEE Ultrasonics Symposium. Part 3 (of 3),
	Nov 1-4 1994},
  year = {1994},
  volume = {3},
  series = {Proceedings of the IEEE Ultrasonics Symposium},
  pages = {1773-1776},
  address = {Cannes, Fr},
  publisher = {IEEE, Piscataway, NJ, USA},
  note = {TY - CONF U1 - 95042678542 L2 - http://dx.doi.org/10.1109/ULTSYM.1994.401934
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Ultrasonography Snakesplines Curvature criteria},
  abstract = {We developed a segmentation algorithm adapted to cardiac and vascular
	structures in ultrasound image sequences which tries to consider
	the specificities of ultrasonography (texture, dropouts, high frame
	rate,...). The problems of extraction of the left ventricle from
	apical and parasternal short axis views as well as the vascular lumen
	from intravascular views are notably addressed using a variety of
	deformable models called 'Snake-Splines' [1], coupled with specific
	knowledge of ultrasound image characteristics. However, the method
	can be applied to any structure. The research is currently being
	continued to quantitate the movement of the extracted elements through
	the sequences.},
  keywords = {Image segmentation Algorithms Cardiovascular system Feature extraction
	Blood vessels Computer workstations Ultrasonic imaging},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{TAN04,
  author = {Tang, Jinshan and Acton, Scott T.},
  title = {Vessel Boundary Tracking for Intravital Microscopy Via Multiscale
	Gradient Vector Flow Snakes},
  journal = {IEEE Transactions on Biomedical Engineering},
  year = {2004},
  volume = {51},
  pages = {316-324},
  number = {2},
  note = {TY - JOUR U1 - 04068010911 L2 - http://dx.doi.org/10.1109/TBME.2003.820374
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Vessel boundary tracking Vasodilation Gradient
	vector flow (GVF)},
  abstract = {Due to movement of the specimen, vasodilation, and intense clutter,
	the intravital location of a vessel boundary from video microscopy
	is a difficult but necessary task in analyzing the mechanics of inflammation
	and the structure of the microvasculature. This paper details an
	active contour model for vessel boundary detection and tracking.
	In developing the method, two innovations are introduced. First,
	the B-spline model is combined with the gradient vector flow (GVF)
	external force. Second, a multiscale gradient vector flow (MSGVF)
	is employed to elude clutter and to reliably localize the vessel
	boundaries. Using synthetic experiments and video microscopy obtained
	via transillumination of the mouse cremaster muscle, we demonstrate
	that the MSGVF approach is superior to the fixed-scale GVF approach
	in terms of boundary localization. In each experiment, the fixed
	scale approach yielded at least a 50% increase in root mean squared
	error over the multiscale approach. In addition to delineating the
	vessel boundary so that cells can be detected and tracked, we demonstrate
	the boundary location technique enables automatic blood flow velocity
	computation in vivo.},
  keywords = {Living systems studies Muscle Cells Blood Computational methods Biomedical
	engineering},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{TAU03,
  author = {Gabriel Taubin},
  title = {New Results in Signal Processing and Compression of Polygon Meshes},
  booktitle = {SMI},
  year = {2003},
  volume = {00},
  pages = {45},
  address = {Los Alamitos, CA, USA},
  publisher = {IEEE Computer Society},
  abstract = {Polygon meshes, which are used in most graphics applications, require
	considerable amounts of storage, even when they only approximate
	precise shapes with limited accuracy. To support internet access
	to 3D models of complex virtual environments or assemblies for electronic
	shopping, collaborative CAD, multi-player video games, and scientific
	visualization, representations of 3D shapes must be compressed by
	several orders of magnitude. Furthermore, several closely related
	methods have been proposed in recent years to smooth, de-noise, edit,
	compress, transmit, and animate very large polygon meshes, based
	on topological and combinatorial methods, signal processing techniques,
	constrained energy minimization, and the solution of diffusion differential
	equations. This talk is an overview of some of my recent results
	in this area: Linear Anisotropic Mesh Filtering, Bi-Level Isosurface
	Compression, Space-Optimized Texture Maps, and Volume Warping for
	Adaptive Isosurface Extraction.},
  doi = {http://doi.ieeecomputersociety.org/10.1109/SMI.2003.1199600},
  isbn = {0-7695-1909-1},
  journal = {smi},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{TAU02,
  author = {Taubin, Gabriel},
  title = {Dual mesh resampling},
  journal = {Graphical Models},
  year = {2002},
  volume = {64},
  pages = {94-113},
  number = {2},
  note = {TY - JOUR U1 - 03037322214 L2 - http://dx.doi.org/10.1006/gmod.2002.0571
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Mesh resampling},
  abstract = {The dual of a 2-manifold polygonal mesh without boundary is commonly
	defined as another mesh with the same topology (genus) but different
	connectivity (vertexface incidence), in which faces and vertices
	occupy complementary locations and the position of each dual vertex
	is computed as the center of mass (barycenter or centroid) of the
	vertices that support the corresponding face. This barycenter dual
	mesh operator is connectivity idempotent but not geometrically idempotent
	for any choice of vertex positions, other than constants. In this
	paper we construct a new resampling dual mesh operator that is geometrically
	idempotent for the largest possible linear subspace of vertex positions.
	We look at the primal and dual mesh connectivities as irregular sampling
	spaces and at the rules to determine dual vertex positions as the
	result of a resampling process that minimizes signal loss. Our formulation,
	motivated by the duality of Platonic solids, requires the solution
	of a simple least-squares problem. We introduce a simple and efficient
	iterative algorithm closely related to Laplacian smoothing and with
	the same computational cost. We also characterize the configurations
	of vertex positions where signal loss does and does not occur during
	dual mesh resampling and the asymptotic behavior of iterative dual
	mesh resampling in the general case. Finally, we describe the close
	relation existing with discrete fairing and variational subdivision,
	and define a new primal-dual interpolatory recursive subdivision
	scheme. &copy; 2002 Elsevier Science (USA).},
  keywords = {Signal processing Algorithms Computational geometry Iterative methods
	Least squares approximations Laplace transforms Interpolation Computer
	graphics},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{TAU95,
  author = {Taubin, Gabriel},
  title = {A signal Processing Approach to Fair Surface Design},
  booktitle = {SIGGRAPH},
  year = {1995},
  pages = {351-358},
  abstract = {In this paper we describe a new tool for interactive free-form fair
	surface design. By generalizing classical discrete Fourier analysis
	to two-dimensional discrete surface signals functions defined on
	polyhedral surfaces of arbitrary topology, we reduce the problem
	of surface smoothing, or fairing, to low-pass filtering. We describe
	a very simple surface signal low-pass filter algorithm that applies
	to surfaces of arbitrary topology. As opposed to other existing optimization-based
	fairing methods,which are computationally more expensive, this is
	a linear time and space complexity algorithm. With this algorithm,
	fairing very large surfaces, such as those obtained from volumetric
	medical data, becomes affordable. By combining this algorithm with
	surface subdivision methods we obtain a very effective fair surface
	design technique. We then extend the analysis, and modify the algorithm
	accordingly, to accommodate different types of constraints. Some
	constraints can be imposed without anymodification of the algorithm,
	while others require the solution of a small associated linear system
	of equations. In particular, vertex location constraints, vertex
	normal constraints, and surface normal discontinuities across curves
	embedded in the surface, can be imposed with this technique.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{TEH88,
  author = {Teh, C. -H. and Chin, R. T. },
  title = {On image analysis by the methods of moments},
  journal = IEEE_J_PAMI,
  year = {1988},
  volume = {10},
  pages = {496--513},
  number = {4},
  abstract = {Various types of moments have been used to recognize image patterns
	in a number of applications. A number of moments are evaluated and
	some fundamental questions are addressed, such as image-representation
	ability, noise sensitivity, and information redundancy. Moments considered
	include regular moments, Legendre moments, Zernike moments, pseudo-Zernike
	moments, rotational moments, and complex moments. Properties of these
	moments are examined in detail and the interrelationships among them
	are discussed. Both theoretical and experimental results are presented},
  doi = {10.1109/34.3913},
  issn = {0162-8828},
  keywords = {pattern recognition, picture processing, Legendre moments, Zernike
	moments, complex moments, image analysis, image-representation, information
	redundancy, noise sensitivity, pattern recognition, picture processing,
	regular moments, rotational moments},
  owner = {euHeart},
  timestamp = {2008.09.16}
}

@INPROCEEDINGS{TEJ04,
  author = {Tejos, Cristian and Hall, Laurance D. and Cardenas-Blanco, Arturo},
  title = {Segmentation of articular cartilage using active contours and prior
	knowledge},
  booktitle = {EMBS},
  year = {2004},
  volume = {26 III},
  pages = {1648-1651},
  address = {San Francisco, CA, United States},
  publisher = {IEEE},
  note = {TY - CONF U1 - 05028780150 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc. U2 - Active contours
	Cartilage segmentation Diffusion snakes Prior knowledge},
  abstract = {A Diffusion Snake segmentation algorithm was evaluated on synthetic
	and real MR images of articular cartilage. The algorithm proved to
	be robust to missing boundaries and the initial contour converges
	over large distances. Compared with a standard B-Spline Snake, more
	accurate and reproducible segmentations were obtained, with less
	effort during initialisation of the algorithm.},
  keywords = {Image segmentation Magnetic resonance imaging Fast Fourier transforms
	Algorithms Set theory Cartilage},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{TER88.2,
  author = {Terzopoulos, Demetri},
  title = {Computation of Visible-Surface Representations.},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {1988},
  volume = {10},
  pages = {417-438},
  number = {4},
  abstract = {A computational theory of visible-surface representations is developed.
	The visible-surface reconstruction process that computes these quantitative
	representations unifies formal solutions to the key problems of (1)
	integrating multiscale constraints on surface depth and orientation
	from multiple-visual sources, (2) interpolating dense, piecewise-smooth
	surfaces from these constraints, (3) detecting surface depth and
	orientation discontinuities to apply boundary conditions on interpolation,
	and (4) structuring large-scale, distributed-surface representations
	to achieve computational efficiency. Visible-surface reconstruction
	is an inverse problem. A well-posed variational formulation results
	from the use of a controlled-continuity surface model. Discontinuity
	detection amounts to the identification of this generic model's distributed
	parameters from the data. Finite-element shape primitives yield a
	local discretization of the variational principle. The result is
	an efficient algorithm for visible-surface reconstruction. The algorithm
	deploys numerical relaxation in multigrid hierarchies and is suited
	to implementation on massively parallel networks of locally interconnected
	processors. Several applications contribute to an empirical evaluation
	of the framework.},
  keywords = {SURFACES VISION - Artificial MATHEMATICAL TECHNIQUES - Finite Element
	Method COMPUTER SYSTEMS, DIGITAL - Parallel Processing IMAGE PROCESSING
	-- Reconstruction},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{TER88.1,
  author = {Terzopoulos, Demetri and Fleischer, Kurt},
  title = {Deformable Models},
  journal = {The Visual Computer},
  year = {1988},
  volume = {4},
  pages = {306-331},
  owner = {Dje},
  timestamp = {2007.09.17}
}

@INPROCEEDINGS{TER87,
  author = {Demetri Terzopoulos and John Platt and Alan Barr and Kurt Fleischer},
  title = {Elastically deformable models},
  booktitle = {SIGGRAPH},
  year = {1987},
  pages = {205--214},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  doi = {http://doi.acm.org/10.1145/37401.37427},
  isbn = {0-89791-227-6},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{TER88.3,
  author = {Terzopoulos, Demetri and Witkin, Andrew},
  title = {Physically based models with rigid and deformable components},
  journal = {IEEE Computer Graphics and Applications},
  year = {1988},
  volume = { 8},
  pages = {41 - 51},
  number = { 6},
  note = {Geometric modeling;Deformable Models;Nonrigid Motion;Rigid Body Dynamics;Elastic
	Deformation;},
  abstract = {A class of physically based models suitable for animating flexible
	objects in simulated physical environments was proposed earlier by
	the authors. The original formulation works well in practice for
	models whose shapes are moderately to highly deformable, but it tends
	to become numerically ill conditioned as the rigidity of the models
	is increased. An alternative formulation of deformable models is
	presented in which deformations are decomposed into a reference component,
	which may represent an arbitrary shape, and a displacement component,
	allowing deformation away from this reference shape. The application
	of the deformable models to a physically based computer animation
	project is illustrated.},
  address = {Edmonton, Alberta, Can},
  copyright = {Compilation and indexing terms, Copyright 2006 Elsevier Inc. All rights
	reserved},
  issn = {0272-1716},
  key = {Computer Graphics -- Animation},
  keywords = {Equations of Motion;Kinematics;Elasticity;},
  language = {English},
  owner = {euHeart},
  timestamp = {2009.02.24},
  url = {http://dx.doi.org/10.1109/38.20317}
}

@INPROCEEDINGS{THE06,
  author = {Th\'evenaz, P. and Unser, M.},
  title = {The Snakuscule},
  booktitle = {ICIP},
  year = {2006},
  pages = {1633--1636},
  address = {Atlanta GA, USA},
  month = {October 8-11,},
  publisher = {IEEE},
  abstract = {Traditional snakes, or active contours, are planar parametric curves.
	Their parameters are determined by optimizing the weighted sum of
	three energy terms: one depending on the data (typically on the integral
	of its gradient under the curve, or on its integral over the area
	enclosed by the curve), one monitoring the shape of the curve (typically
	promoting its smoothness, or regularizing ambiguous solutions), and
	one incorporating prior knowledge (typically favoring a given shape).
	We present in this paper a snake that we designed to be as simple
	as possible without losing too many of the characteristics of more
	complicated, fuller versions. It retains an area data term and requires
	regularization to avoid an ill-posed optimization problem. It is
	parameterized by just two points, thus further easing requirements
	on the optimizer. Despite its extreme simplicity, this active contour
	can efficiently solve a variety of problems such as cell counting
	and segmentation of approximately circular features.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INCOLLECTION{THE00,
  author = {Th{\'{e}}venaz, P. and Blu, T. and Unser, M.},
  title = {Image Interpolation and Resampling},
  booktitle = {Handbook of Medical Imaging, Processing and Analysis},
  publisher = {Academic Press},
  year = {2000},
  editor = {Bankman, I.N.},
  chapter = {25},
  pages = {393--420},
  address = {San Diego CA, USA},
  abstract = {This chapter presents a survey of interpolation and resampling techniques
	in the context of exact, separable interpolation of regularly sampled
	data. In this context, the traditional view of interpolation is to
	represent an arbitrary continuous function as a discrete sum of weighted
	and shifted synthesis functions - in other words, a mixed convolution
	equation. An important issue is the choice of adequate synthesis
	functions that satisfy interpolation properties. Examples of finite-support
	ones are the square pulse (nearest-neighbor interpolation), the hat
	function (linear interpolation), the cubic Keys' function, and various
	truncated or windowed versions of the sinc function. On the other
	hand, splines provide examples of infinite-support interpolation
	functions that can be realized exactly at a finite, surprisingly
	small computational cost. We discuss implementation issues and illustrate
	the performance of each synthesis function. We also highlight several
	artifacts that may arise when performing interpolation, such as ringing,
	aliasing, blocking and blurring. We explain why the approximation
	order inherent in the synthesis function is important to limit these
	interpolation artifacts, which motivates the use of splines as a
	tunable way to keep them in check without any significant cost penalty.}
}

@INPROCEEDINGS{TON06,
  author = {Y. Tong and P. Alliez and D. Cohen-Steiner and M. Desbrun},
  title = {Designing quadrangulations with discrete harmonic forms},
  booktitle = {Symposium on Geometry Processing},
  year = {2006},
  pages = {201--210},
  address = {Aire-la-Ville, Switzerland, Switzerland},
  publisher = {Eurographics Association},
  abstract = {We introduce a framework for quadrangle meshing of discrete manifolds.
	Based on discrete differential forms, our method hinges on extending
	the discrete Laplacian operator (used extensively in modeling and
	animation) to allow for line singularities and singularities with
	fractional indices. When assembled into a singularity graph, these
	line singularities are shown to considerably increase the design
	flexibility of quad meshing. In particular, control over edge alignments
	and mesh sizing are unique features of our novel approach. Another
	appeal of our method is its robustness and scalability from a numerical
	viewpoint: we simply solve a sparse linear system to generate a pair
	of piecewise-smooth scalar fields whose isocontours form a pure quadrangle
	tiling, with no T-junctions.},
  isbn = {30905673-36-3},
  location = {Cagliari, Sardinia, Italy},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{TON03,
  author = {Yiying Tong and Santiago Lombeyda and Anil N. Hirani and Mathieu
	Desbrun},
  title = {Discrete multiscale vector field decomposition},
  journal = {ACM Trans. Graph.},
  year = {2003},
  volume = {22},
  pages = {445--452},
  number = {3},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/882262.882290},
  issn = {0730-0301},
  owner = {euHeart},
  publisher = {ACM Press},
  timestamp = {2009.02.24}
}

@ARTICLE{TOU01,
  author = {Toumoulin, C. and Boldak, C. and Dillenseger, J. L. and Coatrieux,
	J. L. and Rolland, Y.},
  title = {Fast detection and characterization of vessels in very large 3-D
	data sets using geometrical moments},
  journal = IEEE_J_BME,
  year = {2001},
  volume = {48},
  pages = {604--606},
  number = {5},
  abstract = {An improved and very fast algorithm dealing with the extraction of
	vessels in three-dimensional imaging is described. The approach is
	based on geometrical moments and a local cylindrical approximation.
	A robust estimation of vessel and background intensity levels, position,
	orientation, and diameter of the vessels with adaptive control of
	key parameters, is provided during vessel tracking. Experimental
	results are presented for lower limb arteries in multidetector computed
	tomography scanner.},
  doi = {10.1109/10.918601},
  issn = {0018-9294},
  keywords = {blood vessels, computational geometry, computerised tomography, image
	segmentation, medical image processing, method of moments, 3D segmentation,
	adaptive control, background intensity levels, geometrical moments,
	local cylindrical approximation, lower limb arteries, multidetector
	computed tomography scanner, robust estimation, three-dimensional
	imaging, very fast algorithm, very large 3D data sets, vessel diameter,
	vessel intensity levels, vessel tracking, vessels extraction, window
	centering},
  owner = {euHeart},
  timestamp = {2008.09.10}
}

@INPROCEEDINGS{TOU03,
  author = {Toumoulin, C. and Boldak, C. and Garreau, M. and Boulmier, D. },
  title = {Coronary characterization in multi-slice computed tomography},
  booktitle = {Proc. Computers in Cardiology},
  year = {2003},
  pages = {749--752},
  abstract = {We present a 3D extraction method of coronaries in MSCT, which aims
	at refining the delineating of the vascular inner wall and the calcified
	contours for quantification purposes. The proposed approach makes
	use of a two-step process: the first one performs a vessel central
	axis tracking by applying a semi-automatic 3D geometrical moment-based
	method. A refinement is then performed, based on a level set approach,
	to improve the detection accuracy of both contours and calcifications.
	The level sets were applied first in 2-D space, independently on
	each slice, then in 3-D to perform the extraction directly in the
	volume. A comparison between the 2-D and 3-D procedures is provided
	in term of quality of delineation.},
  doi = {10.1109/CIC.2003.1291264},
  issn = {0276-6547},
  keywords = {blood vessels, cardiovascular system, computerised tomography, edge
	detection, medical image processing, 3D extraction method, calcified
	contours, contour detection, coronary characterization, multi-slice
	computed tomography, semi-automatic 3D geometrical moment-based method,
	vascular inner wall, vessel central axis tracking},
  owner = {euHeart},
  timestamp = {2008.09.10}
}

@ARTICLE{TSA93,
  author = {Tsai, C. T. and Sun, Y. N. and Chung, P. C.},
  title = {Minimising the energy of active contour model using a Hopfield network},
  journal = {IEE Proceedings, Part E: Computers and Digital Techniques},
  year = {1993},
  volume = {140},
  pages = {297-303},
  number = {6},
  note = {TY - JOUR U1 - 94031231103 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc. U2 - Active contour models
	Hopfield networks Industrial pattern recognition Minimization method
	Constrained energy minimization Neural network application Closed
	spline curve},
  abstract = {Active contour models (snakes) are commonly used for locating the
	boundary of an object in computer vision applications. The minimisation
	procedure is the key problem to solve in the technique of active
	contour models. In this paper, a minimisation method for an active
	contour model using Hopfield networks is proposed. Due to its network
	structure, it lends itself admirably to parallel implementation and
	is potentially faster than conventional methods. In addition, it
	retains the stability of the snake model and the possibility for
	inclusion of hard constraints. Experimental results are given to
	demostrate the feasibility of the proposed method in applications
	of industrial pattern recognition and medical image processing.},
  keywords = {Pattern recognition Neural networks Computer networks},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{UNS00,
  author = {Unser, M.},
  title = {Sampling---50 {Y}ears After {S}hannon},
  journal = {Proceedings of the {IEEE}},
  year = {2000},
  volume = {88},
  pages = {569--587},
  number = {4},
  month = {April},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{UNS99,
  author = {Unser, Michael},
  title = {Splines: A perfect fit for signal and image processing},
  journal = {IEEE Signal Processing Magazine},
  year = {1999},
  volume = {16},
  pages = {22-38},
  number = {6},
  note = {TY - JOUR U1 - 99124952443 L2 - http://dx.doi.org/10.1109/79.799930
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Spline interpolation Cardinal splines B-spline
	functions Spline sampling theory Multiresolution spline processing
	Spline pyramids Fractional splines},
  abstract = {The primary applications of splines in signal and image processing
	are reviewed. Splines are piecewise polynomials that are smoothly
	connected together. Continuous representation of a discrete signal
	may be obtained via spline fitting in one or more dimensions which
	may be exact (interpolation), or approximate (least-squares or smoothing).
	Expressed as linear combinations of B-spline basis functions, they
	may be determined by digital filtering. Excellent approximation properties
	make available convergence rates and error estimates. While its multiresolution
	properties make them suitable for constructing wavelet bases and
	multi-scale processing. Its localization properties are suitable
	for time-frequency signal analysis.},
  keywords = {Polynomials Piecewise linear techniques Signal processing Interpolation
	Least squares approximations Functions Digital filters Estimation
	Variational techniques Mathematical models Analog to digital conversion
	Signal theory},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{UNS90,
  author = {Unser, Michael},
  title = {Recursive filters for fast B-spline interpolation and compression
	of digital images},
  booktitle = {Medical Imaging IV: Image Capture and Display, Feb 4-5 1990},
  year = {1990},
  volume = {1232},
  series = {Proceedings of SPIE - The International Society for Optical Engineering},
  pages = {337-347},
  address = {Newport Beach, CA, USA},
  publisher = {Publ by Int Soc for Optical Engineering, Bellingham, WA, USA},
  note = {TY - CONF U1 - 91040183914 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc. U2 - B-Spline Interpolation},
  abstract = {Signal processing concepts are used to study the problems of obtaining
	continuous image representations in terms of B-spline basis functions,
	and performing reconstructions at various magnifications. Filters
	that compute direct or indirect B-spline transformations are derived
	and characterized in terms of their z-transforms for polynomial splines
	of any order. Efficient recursive implementations of these operators
	are proposed. Recursive filters that efficiently solve the problem
	of approximating a signal through the use of least squares splines
	are also derived. This last procedure is analogous to the application
	of an anti-aliasing filter prior to decimation for the representation
	of a signal with fewer samples.},
  keywords = {Signal Processing - Digital Techniques Signal Filtering and Prediction
	Information Theory - Data Compression Mathematical Transformations
	- Z Transforms Computer Graphics - Anti-aliasing Image Processing
	-- Reconstruction},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{UNS93.1,
  author = {Unser, Michael and Aldroubi, Akram and Eden, Murray},
  title = {B-spline signal processing. Part I. Theory},
  journal = {IEEE Transactions on Signal Processing},
  year = {1993},
  volume = {41},
  pages = {821-833},
  number = {2},
  note = {TY - JOUR U1 - 93030742494 L2 - http://dx.doi.org/10.1109/78.193220
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - B-spline signal processing Indirect B-spline
	transform Recursive filters Anti-aliasing low-pass filter},
  abstract = {This paper describes a set of efficient filtering techniques for the
	processing and representation of signals in terms of continuous B-spline
	basis functions. We first consider the problem of determining the
	spline coefficients for an exact signal interpolation (direct B-spline
	transform). The reverse operation is the signal reconstruction from
	its spline coefficients with an optional zooming factor m (indirect
	B-spline transform). We derive general expressions for the z transforms
	and the equivalent continuous impulse responses of B-spline interpolators
	of order n. We present simple techniques for signal differentiation
	and filtering in the transformed domain. We then derive recursive
	filters that efficiently solve the problems of smoothing spline and
	least squares approximations. The smoothing spline technique approximates
	a signal with a complete set of coefficients subject to certain regularization
	or smoothness constraints. The least squares approach, on the other
	hand, uses a reduced number of B-spline coefficients with equally
	spaced nodes; this techniques is in many ways analogous to the application
	of antialiasing low-pass filter prior to decimation in order to represent
	a signal correctly with a reduced number of samples.},
  keywords = {Signal filtering and prediction Least squares approximations Mathematical
	transformations Interpolation Low pass filters Surfaces Computer
	vision Polynomials Image processing},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{UNS93.2,
  author = {Unser, Michael and Aldroubi, Akram and Eden, Murray},
  title = {B-spline signal processing. Part II. Efficient design and applications},
  journal = {IEEE Transactions on Signal Processing},
  year = {1993},
  volume = {41},
  pages = {834-848},
  number = {2},
  note = {TY - JOUR U1 - 93030742495 L2 - http://dx.doi.org/10.1109/78.193221
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - B-spline signal processing Recursive filtering
	B-spline interpolation Cubic spline signal processing Cubic spline
	image pyramid},
  abstract = {This paper describes a class of recursive filtering algorithms for
	the efficient implementation of B-spline interpolation and approximation
	techniques. In terms of simplicity of realization and reduction of
	computational complexity, these algorithms compare favorably with
	conventional matrix approaches. A filtering interpretation (low-pass
	filter followed by an exact polynomial spline interpolator) of smoothing
	spline and least squares approximation methods is proposed. These
	techniques are applied to the design of digital filters for cubic
	spline signal processing. An efficient implementation of a smoothing
	spline edge detector is proposed. It is also shown how to construct
	a cubic spline image pyramid that minimizes the loss of information
	in passage from one resolution level to the next. In terms of common
	measures of fidelity (e.g., visual quality, SNR), this data structure
	appears to be superior to the widely used Gaussian/Laplacian pyramid.},
  keywords = {Low pass filters Digital filters Least squares approximations Interpolation
	Signal filtering and prediction Data structures Signal to noise ratio
	Mathematical transformations Computer simulation Polynomials Optimization
	Image processing},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{UNS91.1,
  author = {Unser, Michael and Aldroubi, Akram and Eden, Murray},
  title = {Fast B-spline transforms for continuous image representation and
	interpolation},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {1991},
  volume = {13},
  pages = {277-285},
  number = {3},
  note = {TY - JOUR U1 - 91050196331 L2 - http://dx.doi.org/10.1109/34.75515
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - B-Splines Continuous Representation Polynomial
	Splines Recursive Filter},
  abstract = {Efficient algorithms for the continuous representation of a discrete
	signal in terms of B-splines (direct B-spline transform) and for
	interpolative signal reconstruction (indirect B-spline transform)
	with an expansion factor m are described. Expressions for the z-transforms
	of the sampled B-spline functions are determined and a convolution
	property of these kernels is established. It is shown that both the
	direct and indirect spline transforms involve linear operators that
	are space invariant and are implemented efficiently by linear filtering.
	Fast computational algorithms based on the recursive implementations
	of these filters are proposed. A B-spline interpolator can also be
	characterized in terms of its transfer function and its global impulse
	response (cardinal spline of order n). The case of the cubic spline
	is treated in greater detail. The present approach is compared with
	previous methods that are reexamined from a critical point of view.
	It is concluded that B-spline interpolation correctly applied does
	not result in a loss of image resolution and that this type of interpolation
	can be performed in a very efficient manner.},
  keywords = {Mathematical Transformations Mathematical Techniques - Interpolation
	Signal Filtering and Prediction Image Processing -- Reconstruction},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{UNS91.2,
  author = {Unser, Michael and Aldroubi, Akram and Eden, Murray},
  title = {Recursive Regularization Filters: Design, Properties, and Applications},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence.},
  year = {1991},
  volume = {13},
  pages = {272-277},
  number = {3},
  note = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  abstract = {Least squares approximation problems that are regularized with specified
	highpass stabilizing kernels are discussed. For each problem, there
	is a family of discrete regularization filters (R-filters) which
	allow an efficient determination of the solutions. These operators
	are stable symmetric lowpass filters with an adjustable scale factor.
	Two decomposition theorems for the z-transform of such systems are
	presented. One facilitates the determination of their impulse response,
	while the other allows an efficient implementation through successive
	causal and anticausal recursive filtering. A case of special interest
	is the design of R-filters for the first- and second-order difference
	operators. These results are extended for two-dimensional signals
	and, for illustration purposes, are applied to the problem of edge
	detection. This leads to a very efficient implementation (8 multiplies
	+ 10 adds per pixel) of the optimal Canny edge detector based on
	the use of a separable second-order R-filter.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{VAL00,
  author = {Valdes, R. and Yanez-Suarez, O. and Medina, V.},
  title = {Trachea segmentation in CT images using active contours},
  booktitle = {EMBC},
  year = {2000},
  volume = {4},
  pages = {3184-3187},
  address = {Chicago, IL},
  publisher = {IEEE},
  note = {TY - CONF U1 - 01276568282 L2 - http://dx.doi.org/10.1109/IEMBS.2000.901568
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Trachea segmentation Active contours Airway
	segmentation Snakes model Canny filter},
  abstract = {Tracheal stenosis is an uncommon pathology that in early stages is
	often confused with different respiratory affections by its signs
	and symptoms. An automatic characterization of the tracheal stenosis
	requires adequate medical images and efficient segmentation algorithms.
	In CT images, several algorithms of airway segmentation have been
	used, such as 3D region growing, thresholding and gray-level profile
	analysis. In this work a segmentation method for trachea extraction
	in CT images is proposed. The algorithm is based on an active contour
	model (SS) formulated by considering the explicit expression of the
	natural cubic splines and is compared with the original snakes model
	(OS). In both cases, an automatic definition of the initial contour
	based on a Canny filter is proposed. Eight images were processed
	with both algorithms and the results show that the SS model is less
	sensitive to initial conditions. For this image modality the Canny
	operator proved to be a good choice to obtain the initial contour.
	The SS method generates a smoothed version of the tracheal border.},
  keywords = {Diseases Computerized tomography Image segmentation Pathology Medical
	imaging Algorithms Three dimensional Image analysis Respiratory mechanics},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{VEI08,
  author = {Nico R Van de Veire and Joanne D Schuijf and Gabe B Bleeker and Martin
	J Schalij and Jeroen J Bax},
  title = {Magnetic resonance imaging and computed tomography in assessing cardiac
	veins and scar tissue.},
  journal = {Europace},
  year = {2008},
  volume = {10 Suppl 3},
  pages = {iii110--iii113},
  month = {Nov},
  abstract = {The success of cardiac resynchronization therapy is influenced by
	several issues including cardiac venous anatomy and myocardial scar
	tissue. This article discusses non-invasive imaging modalities that
	could contribute significantly to the selection process of cardiac
	resynchronization therapy (CRT) candidates: multi-slice computed
	tomography to depict the coronary sinus tributaries and magnetic
	resonance imaging to identify scar tissue.},
  doi = {10.1093/europace/eun236},
  institution = {erlands. N.R.L.van_de_Veire@lumc.nl},
  keywords = {Cardiac Pacing, Artificial; Cardiomyopathies, diagnosis; Cicatrix,
	diagnosis; Coronary Sinus, pathology/radiography; Humans; Magnetic
	Resonance Imaging, methods; Patient Selection; Tomography, X-Ray
	Computed, methods},
  owner = {euHeart},
  pii = {eun236},
  pmid = {18955391},
  timestamp = {2009.02.16},
  url = {http://dx.doi.org/10.1093/europace/eun236}
}

@PHDTHESIS{VEL07.2,
  author = {Velut, J\'er\^ome},
  title = {Segmentation par mod\`ele d\'eformable surfacique localement r\'egularis\'e
	par spline lissante},
  school = {INSA-Lyon},
  year = {2007},
  month = {december},
  abstract = {Image segmentation through deformable models is a method that localizes
	object boundaries. When difficult segmentation context are proposed
	because of noise or a lack of information, the use of prior knowledge
	in the deformation process increases segmentation accuracy. Medical
	imaging is often concerned by these context. Moreover, medical applications
	deal with large amounts of data. Then it is mandatory to use a robust
	and fast processing.
	
	This question lead us to a local regularisation of the deformable
	model. Highly based on the active contour framework, also known as
	\emph{snake}, we propose a new regularization scheme. This is done
	by filtering the displacements at each iteration. The filter is based
	on a smoothing spline kernel whose aim was to approximate a set of
	points rather than interpolating it.
	
	We point out the consistency of the regularization parameter in such
	a method. It deals with a real, positive λ value that tunes the cut-off
	frequency of a low-pass filter. As it is possible to link analytically
	the cut-off frequency and the sampling frequency, one may give a
	metric meaning to this cut-off frequency. Moreover a different λ
	value should be set at each point through a variation of the filter's
	coefficients. It induces a local cut-off frequency and a local regularization.
	
	This new regularization method should be applied to surfaces. The
	main difficulty concerns the connectivity of the surface mesh. It
	has to be 4-valenced everywhere due to the bidimensionnal fashion
	of the filter. Segmentation results are given for such meshes. A
	special process of singular points on spherical mesh is proposed.}
}

@INPROCEEDINGS{VEL08,
  author = {Velut, J. and Benoit-Cattin, H. and Odet, C.},
  title = {IIR filtering of surface meshes for the regularization of deformable
	models},
  booktitle = {Proc. 15th IEEE International Conference on Image Processing ICIP
	2008},
  year = {2008},
  pages = {1077--1080},
  abstract = {A new adaptive IIR filtering of surface meshes by smoothing B-spline
	is proposed. We define some ad hoc processing of borders and singular
	points for genus 0 meshes. By adapting the filter cut-off frequency
	to the local mesh sampling, it enables homogeneous smoothing of non-uniform
	meshes. We present genus 0 and genus 1 meshes filtering as well as
	volume segmentations. These segmentations have been obtained using
	the IIR filtering of deformation forces within a smoothing B-Spline
	active surface.},
  doi = {10.1109/ICIP.2008.4711945},
  issn = {1522-4880},
  keywords = {IIR filters, adaptive filters, image segmentation, mesh generation,
	splines (mathematics), ad hoc processing, adaptive IIR filtering,
	deformable models, deformation forces, filter cut-off frequency,
	homogeneous smoothing, local mesh sampling, nonuniform meshes, smoothing
	B-spline, surface meshes, volume segmentations, Adaptive signal processing,
	geometric modeling, image segmentation, surfaces},
  owner = {euHeart},
  timestamp = {2009.04.17}
}

@ARTICLE{VEL07.1,
  author = {Velut, J\'er\^ome and Benoit-Cattin, Hugues and Odet, Christophe},
  title = {Locally Regularized Smoothing B-Snake},
  journal = {EURASIP Journal on Advances in Signal Processing},
  year = {2007},
  volume = {2007},
  pages = {Article ID 76241, 12 pages},
  note = {doi:10.1155/2007/76241},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{VEL06.1,
  author = {Velut, J\'er\^ome and Benoit-Cattin, Hugues and Odet, Christophe},
  title = {Locally Regularized Snake Through Smoothing B-Spline Filtering},
  booktitle = {EUSIPCO},
  year = {2006},
  pages = {In proceedings},
  address = {Firenze},
  abstract = {In this paper we propose a locally regularized snake based on smoothing-spline
	filtering. The proposed algorithm associates this regularization
	process to a force equilibrium scheme leading the snakes deformation.
	The regularization level is controlled through a unique parameter
	that can vary along the contour: It provides a locally regularized
	smoothing B-snake that offers a powerful framework to introduce prior
	knowledge. We illustrate the snake behavior on MRI images, with global
	and local regularization.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{VEL06.2,
  author = {Velut, J\'er\^ome and Benoit-Cattin, Hugues and Odet, Christophe},
  title = {Segmentation by Smoothing B-Spline Active Surface},
  booktitle = {ICIP},
  year = {2006},
  pages = {209-212},
  address = {Atlanta GA},
  abstract = {In this paper, a new active surface based segmentation algorithm is
	proposed. The regularization process is done trough smoothing B-spline
	filtering of the mesh point displacements. Such a regularization
	is controlled by a unique parameter Lambda that tunes the cut-off
	frequency of the smoothing B-spline filter. This approach is fast
	due to the 1D digital filtering used. It is well adapted to quadrangular
	meshes homeomorphic to a plan (plan, torus, cylinder). Furthermore,
	by using a specific management of singular points, genus zero surfaces
	can be used in the segmentation process.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{VEL10.2,
  author = {Velut, J\'er\^ome and Lentz, Pierre-Axel and Philipot, Cl\'ement
	and Garcia, Marie-Paule and Toumoulin, Christine},
  title = {A qualitative and quantitative study of coronary artery MRA},
  booktitle = {EMBC},
  year = {2010},
  pages = {in--press},
  publisher = {IEEE},
  owner = {euHeart},
  timestamp = {2010.01.18}
}

@INPROCEEDINGS{VEL10.1,
  author = {Velut, J\'er\^ome and Toumoulin, Christine and Coatrieux, Jean-Louis},
  title = {3D coronary structure tracking algorithm with regularization and
	multiple hypotheses in MRI},
  booktitle = {ISBI},
  year = {2010},
  pages = {37--40},
  publisher = {IEEE},
  owner = {euHeart},
  timestamp = {2010.01.18}
}

@INPROCEEDINGS{VEM98,
  author = {Vemuri, B.C. and Guo, Y.},
  title = {Snake pedals: Geometric models with physics-based control},
  booktitle = {Proceedings of the 1998 IEEE 6th International Conference on Computer
	Vision, Jan 4-7 1998},
  year = {1998},
  series = {Journal of Engineering and Applied Science},
  pages = {427-432},
  address = {Bombay, India},
  publisher = {IEEE, Piscataway, NJ, USA},
  note = {TY - CONF U1 - 98114467659 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc. U2 - Geometric shape
	modeling Snake pedal model},
  abstract = {In this paper, we introduce a novel geometric shape modeling scheme
	which allows for representation of global and local shape characteristics
	of an object. Geometric models are traditionally well suited for
	representing global shapes but not the local details. However, in
	this paper we propose a powerful geometric shape modeling scheme
	which allows for the representation of global shapes with local detail
	and permits model shaping as well as topological changes via physics-based
	control. The proposed modeling scheme consists of representing shapes
	by pedal curves and surfaces - pedal curves/surfaces are the loci
	of the foot of perpendiculars to the tangents of a fixed curve/surface
	from a fixed point called the pedal point. By varying the location
	of the pedal point, one can synthesize a large class of shapes which
	exhibit both local and global deformations. We introduce physics-based
	control for shaping these geometric models by letting the pedal point
	vary and use a dynamic spline to represent the position of this varying
	pedal point. The model dubbed as a `snake pedal' allows for interactive
	manipulation via forces applied to the snake. We demonstrate the
	applicability of this modeling scheme via examples of shape synthesis
	and shape estimation from real image data.},
  keywords = {Mathematical models Computer vision Object recognition},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{WAN96,
  author = {Wang, Mao and Evans, Joyce and Hassebrook, Laurence and Knapp, Charles},
  title = {A Mutlistage, Optimal Active Contour Model},
  journal = {Transactions on Image Processing},
  year = {1996},
  volume = {5},
  pages = {1586-1591},
  number = {11},
  abstract = {Energy-minimizing active contour models or snakes can be used in many
	applications such as edge detection, motion tracking, image matching,
	computer vision, and three-dimensional (3-D) reconstruction. We present
	a novel snake that is superior both in accuracy and convergence speed
	over previous snake algorithms. High performance is achieved by using
	spline representation and dividing the energy-minimization process
	into multiple stages. The first stage is designed to optimize the
	convergence speed in order to allow the snake to quickly approach
	the minimum-energy state. The second stage is devoted to snake refinement
	and to local minimization of energy, thereby driving the snake to
	a quasiminimum-energy state. The third stage uses the Bellman (1957)
	optimality principle to fine-tune the snake to the global minimum-energy
	state. This three-stage scheme is optimized for both accuracy and
	speed},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{WAN05,
  author = {Wang, Y. and Shu, H. Z. and Zhou, Z. D. and Toumoulin, C. and Coatrieux,
	J. L.},
  title = {Vessel extraction in coronary X-ray Angiography},
  booktitle = {Proc. 27th Annual International Conference of the Engineering in
	Medicine and Biology Society IEEE-EMBS 2005},
  year = {2005},
  pages = {1584--1587},
  abstract = {This paper describes a method to extract the vascular centerlines
	and contours in coronary angiography. The proposed approach associates
	geometric moments for the estimation of a "cylinder-like model" and
	relies on a tracking process. The orientation of the cylinder axis
	and its local diameter are computed from the analytical expressions
	of the geometric moments of up to order 2. Experimental results are
	presented on several images of two sequences that show the efficiency
	of the method},
  doi = {10.1109/IEMBS.2005.1616739},
  keywords = {blood vessels, diagnostic radiography, feature extraction, image sequences,
	medical image processing, coronary X-ray angiography, cylinder-like
	model, geometric moments, image sequences, vessel extraction, 2D
	geometric moments, centerlines and contours extraction, coronary
	angiography, structure tracking},
  owner = {euHeart},
  timestamp = {2008.09.10}
}

@INPROCEEDINGS{WAN01,
  author = {Wang, Y. and Teoh, E.K. and Shen, D.},
  title = {Structure-adaptive B-snake for segmenting complex objects},
  booktitle = {ICIP},
  year = {2001},
  volume = {2},
  pages = {769-772},
  address = {Thessaloniki},
  publisher = {IEEE},
  note = {TY - CONF U1 - 01526773441 L2 - http://dx.doi.org/10.1109/ICIP.2001.958607
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Automatic control point insertion Minimum
	mean square energy},
  abstract = {In this paper, we presented a structure-adaptive B-snake model for
	segmenting the complex structures in medical images. A strategy of
	automatic control-point insertion, adaptive to the structure of the
	studied object, has been proposed. Furthermore, a method of minimum
	mean square energy (MMSE) is developed to iteratively estimate the
	position of those control points in the B-snake model. By applying
	the proposed structure-adaptive B-snake model to medical images,
	we show that our method is robust and accurate in object contour
	extraction.},
  keywords = {Object recognition Medical imaging Feature extraction Computational
	complexity Iterative methods Mathematical models Image segmentation},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{WAN03,
  author = {Wang, Yue and Teoh, Eam Khwang and Shen, dinggang},
  title = {Lane Detection and Tracking Using B-Snake},
  journal = {Image and Vision Computing.},
  year = {2003},
  volume = {22},
  pages = {269-280},
  abstract = {In this paper, we proposed a B-Snake based lane detection and tracking
	algorithm without any cameras’ parameters. Compared with other lane
	models, the B-Snake based lane model is able to describe a wider
	range of lane structures since B-Spline can form any arbitrary shape
	by a set of control points. The problems of detecting both sides
	of lane markings (or boundaries) have been merged here as the problem
	of detecting the mid-line of the lane, by using the knowledge of
	the perspective parallel lines. Furthermore, a robust algorithm,
	called CHEVP, is presented for providing a good initial position
	for the B-Snake. Also, a minimum error method by Minimum Mean Square
	Error (MMSE) is proposed to determine the control points of the B-Snake
	model by the overall image forces on two sides of lane. Experimental
	results show that the proposed method is robust against noise, shadows,
	and illumination variations in the captured road images. It is also
	applicable to the marked and the unmarked roads, as well as the dash
	and the solid paint line roads.},
  keywords = {Lane detection; B-Spline; Snake; Lane model; Machine vision; Intelligent
	vehicle},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{WEB03,
  author = {Oliver M Weber and Alastair J Martin and Charles B Higgins},
  title = {Whole-heart steady-state free precession coronary artery magnetic
	resonance angiography.},
  journal = {Magn Reson Med},
  year = {2003},
  volume = {50},
  pages = {1223--1228},
  number = {6},
  month = {Dec},
  abstract = {Current implementations of coronary artery magnetic resonance angiography
	(MRA) suffer from limited coverage of the coronary arterial system.
	Whole-heart coronary MRA was implemented based on a free-breathing
	steady-state free-precession (SSFP) technique with magnetization
	preparation. The technique was compared to a similar implementation
	of conventional, thin-slab coronary MRA in 12 normal volunteers.
	Three thin-slab volumes were prescribed: 1) a transverse slab, covering
	the left main (LM) artery and proximal segments of the left anterior
	ascending (LAD) and left circumflex (LCX) coronary arteries; 2) a
	double-oblique slab covering the right coronary artery (RCA); and
	3) a double-oblique slab covering the proximal and distal segments
	of the LCX. The whole-heart data set was reformatted in identical
	orientations. Visible vessel length, vessel sharpness, and vessel
	diameter were determined and compared separately for each vessel.
	Whole-heart coronary MRA visualized LM/LAD (11.7 +/- 3.4 cm) and
	LCX (6.9 +/- 3.6 cm) over a significantly longer distance than the
	transverse volume (LM/LAD, 6.1 +/- 1.1 cm, P < 0.001; LCX, 4.2 +/-
	1.2 cm, P < 0.05). Improvements in visible vessel length for RCA
	and LCX in the whole-heart approach vs. their respective targeted
	volumes were not significant. It is concluded that the whole-heart
	coronary MRA technique improves visible vessel length and facilitates
	high-quality coronary MRA of the complete coronary artery tree in
	a single measurement.},
  doi = {10.1002/mrm.10653},
  institution = {Department of Radiology, University of California-San Francisco,
	San Francisco, California 94143, USA. oliver.weber@radiology.ucsf.edu},
  keywords = {Adult; Arteries, anatomy /&/ histology; Coronary Disease, diagnosis;
	Coronary Vessels, anatomy /&/ histology; Female; Humans; Imaging,
	Three-Dimensional; Magnetic Resonance Angiography, methods; Male},
  owner = {euHeart},
  pmid = {14648570},
  timestamp = {2009.07.02},
  url = {http://dx.doi.org/10.1002/mrm.10653}
}

@ARTICLE{WEB04,
  author = {Oliver M Weber and Sandra Pujadas and Alastair J Martin and Charles
	B Higgins},
  title = {Free-breathing, three-dimensional coronary artery magnetic resonance
	angiography: comparison of sequences.},
  journal = {J Magn Reson Imaging},
  year = {2004},
  volume = {20},
  pages = {395--402},
  number = {3},
  month = {Sep},
  abstract = {PURPOSE: To compare six free-breathing, three-dimensional, magnetization-prepared
	coronary magnetic resonance angiography (MRA) sequences. MATERIALS
	AND METHODS: Six bright-blood sequences were evaluated: Cartesian
	segmented gradient echo (C-SGE), radial SGE (R-SGE), spiral SGE (S-SGE),
	spiral gradient echo (S-GE), Cartesian steady-state free precession
	(C-SSFP), and radial SSFP (R-SSFP). The right coronary artery (RCA)
	was imaged in 10 healthy volunteers using all six sequences in randomized
	order. Images were evaluated by two observers with respect to signal-to-noise
	ratio (SNR), contrast-to-noise ratio (CNR), visible vessel length,
	vessel edge sharpness, and vessel diameter. RESULTS: C-SSFP depicted
	RCA over the longest distance with high vessel sharpness, good SNR,
	and excellent background suppression. S-GE provided best SNR and
	CNR in proximal segments, but more vessel blurring and poorer background
	suppression, resulting in poor visualization of distal segments.
	R-SSFP images showed good background suppression and best vessel
	sharpness, but only moderate SNR. C-SGE provided good SNR and reasonable
	CNR, but lowest vessel sharpness. S-SGE and R-SGE visualized the
	RCA over the smallest distance, mostly due to vessel blurring and
	low SNR, respectively. CONCLUSION: Overall, Cartesian SSFP provided
	the best image quality with excellent vessel sharpness, visualization
	of long vessel segments, and good SNR and CNR.},
  doi = {10.1002/jmri.20141},
  institution = {Department of Radiology, University of California, San Francisco,
	USA. oliver.weber@radiology.ucsf.edu},
  keywords = {Adult; Analysis of Variance; Coronary Angiography, methods; Coronary
	Vessels, physiology; Female; Humans; Imaging, Three-Dimensional;
	Magnetic Resonance Angiography, methods; Male; Respiration},
  owner = {euHeart},
  pmid = {15332246},
  timestamp = {2009.07.02},
  url = {http://dx.doi.org/10.1002/jmri.20141}
}

@ARTICLE{WEE08,
  author = {Chong-Yaw Wee and Raveendran Paramesran and R. Mukundan},
  title = {Fast computation of geometric moments using a symmetric kernel},
  journal = {Pattern Recogn.},
  year = {2008},
  volume = {41},
  pages = {2369--2380},
  number = {7},
  abstract = {This paper presents a novel set of geometric moments with symmetric
	kernel (SGM) obtained using an appropriate transformation of image
	coordinates. By using this image transformation, the computational
	complexity of geometric moments (GM) is reduced significantly through
	the embedded symmetry and separability properties. In addition, it
	minimizes the numerical instability problem that occurs in high order
	GM computation. The novelty of the method proposed in this paper
	lies in the transformation of GM kernel from interval [0,~] to interval
	[-1,1]. The transformed GM monomials are symmetry at the origin of
	principal Cartesian coordinate axes and hence possess symmetrical
	property. The computational complexity of SGM is reduced significantly
	from order O(N^4) using the original form of computation to order
	O(N^3) for the proposed symmetry-separable approach. Experimental
	results show that the percentage of reduction in computation time
	of the proposed SGM over the original GM is very significant at about
	75.0% and 50.0% for square and non-square images, respectively. Furthermore,
	the invariant properties of translation, scaling and rotation in
	Hu's moment invariants are maintained. The advantages of applying
	SGM over GM in Zernike moments computation in terms of efficient
	representation and computation have been shown through experimental
	results.},
  address = {New York, NY, USA},
  doi = {http://dx.doi.org/10.1016/j.patcog.2007.12.012},
  issn = {0031-3203},
  owner = {euHeart},
  publisher = {Elsevier Science Inc.},
  timestamp = {2008.09.22}
}

@OTHER{WEE01,
  abstract = {To improve the robustness of segmentation methods, more and more methods
	use prior knowledge. We present an approach which embeds an active
	shape model into an elastically deformable surface model, and combines
	the advantages of both approaches. The shape model constrains the
	flexibility of the surface mesh representing the deformable model
	and maintains an optimal distribution of mesh vertices. A specific
	external energy which attracts the deformable model to locally detected
	surfaces, reduces the danger that the mesh is trapped by false object
	boundaries. Examples are shown, and furthermore a validation study
	for the segmentation of vertebrae in CT images is presented. With
	the exception of a few problematic areas, the algorithm leads reliably
	to a very good overall segmentation.},
  author = {Weese, JÃ¼rgen and Kaus, Michael and Lorenz, Christian and Lobregt,
	Steven and Truyen, Roel and Pekar, Vladimir},
  journal = {Information Processing in Medical Imaging},
  owner = {euHeart},
  pages = {380--387},
  timestamp = {2008.10.21},
  title = {Shape Constrained Deformable Models for 3D Medical Image Segmentation},
  url = {http://dx.doi.org/10.1007/3-540-45729-1_38},
  year = {2001}
}

@INPROCEEDINGS{WEL94,
  author = {William Welch and Andrew Witkin},
  title = {Free-form shape design using triangulated surfaces},
  booktitle = {SIGGRAPH '94: Proceedings of the 21st annual conference on Computer
	graphics and interactive techniques},
  year = {1994},
  pages = {247--256},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {We present an approach to modeling with truly mutable yet completely
	controllable free-form surfaces of arbitrary topology. Surfaces may
	be pinned down at points and along curves, cut up and smoothly welded
	back together, and faired and reshaped in the large. This style of
	control is formulated as a constrained shape optimization, with minimization
	of squared principal curvatures yielding graceful shapes that are
	free of the parameterization worries accompanying many patch-based
	approaches. Triangulated point sets are used to approximate these
	smooth variational surfaces, bridging the gap between patch-based
	and particle-based representations. Automatic refinement, mesh smoothing,
	and re-triangulation maintain a good computational mesh as the surface
	shape evolves, and give sample points and surface features much of
	the freedom to slide around in the surface that oriented particles
	enjoy. The resulting surface triangulations are constructed and maintained
	in real time.},
  doi = {http://doi.acm.org/10.1145/192161.192216},
  isbn = {0-89791-667-0},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{WER04,
  author = {Weruaga, Luis and Verdu, Rafael and Morales, Juan},
  title = {Frequency domain formulation of active parametric deformable models},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2004},
  volume = {26},
  pages = {1568-1578},
  number = {12},
  note = {TY - JOUR U1 - 04508712669 L2 - http://dx.doi.org/10.1109/TPAMI.2004.124
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Frequency domain formulation Active parametric
	deformable models Lagrange minimization Frequency based implementation},
  abstract = {Active deformable models are simple tools, very popular in computer
	vision and computer graphics, for solving ill-posed problems or mimic
	real physical systems. The classical formulation is given in the
	spatial domain the motor of the procedure is a second-order linear
	system, and rigidity and elasticity are the basic parameters for
	its characterization. This paper proposes a novel formulation based
	on a frequency-domain analysis: The internal energy functional and
	the Lagrange minimization are performed entirely in the frequency
	domain, which leads to a simple formulation and design. The frequency-based
	implementation offers important computational savings in comparison
	to the original one, a feature that is improved by the efficient
	hardware and software computation of the FFT algorithm. This new
	formulation focuses on the stiffness spectrum, allowing the possibility
	of constructing deformable models apart from the elasticity and rigidity-based
	original formulation. Simulation examples validate the theoretical
	results. &copy; 2004 IEEE.},
  keywords = {Object recognition Image segmentation Mathematical models Fast Fourier
	transforms Algorithms Lagrange multipliers Computer vision Computer
	graphics Problem solving Computer simulation Frequency domain analysis},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{WIL97,
  author = {James Williams and Lawrence Wolff},
  title = {Analysis of the Pulmonary Vascular Tree Using Differential Geometry
	Based Vector Fields},
  journal = {Computer Vision and Image Understanding},
  year = {1997},
  volume = {65},
  pages = {226 - 236},
  number = {2},
  abstract = {We extract topological and local structural information from X-ray-computed
	tomography (CT) volume images of the vascular tree of the lung. We
	then produce an abstract model of the topology and geometry of the
	tree which is suitable for generating physical models. Physiologists
	model the vascular tree as a connected network of tubes and, to date,
	the only accurate data available to produce such models has come
	from dissection and measurement. Modeling the dynamic behavior of
	the tree in a living organism has been, until now, impossible. Building
	models from CT scans will increase the volume and quality of information
	available for both the study of physiology and diagnosis. We present
	an efficient, accurate analysis technique for volume images of tube
	networks. Using local operations that link techniques based on morphology
	to differential geometry, we transform the volume into a vector field
	which resembles idealized, axis-parallel fluid flow through the tube
	network. This field provides information to condense the salient
	features of the image into an augmented Euclidean minimum spanning
	tree (EMST). This augmented EMST proves to be a convenient and logical
	abstract representation of the vascular tree.},
  doi = {DOI: 10.1006/cviu.1996.0571},
  issn = {1077-3142},
  owner = {euHeart},
  timestamp = {2009.11.06},
  url = {http://www.sciencedirect.com/science/article/B6WCX-45M8S80-10/2/63f8987db2404f322c74d84c424af2c3}
}

@ARTICLE{WIN02,
  author = {Onno Wink and Alejandro F Frangi and Bert Verdonck and Max A Viergever
	and Wiro J Niessen},
  title = {3D MRA coronary axis determination using a minimum cost path approach.},
  journal = {Magn Reson Med},
  year = {2002},
  volume = {47},
  pages = {1169--1175},
  number = {6},
  month = {Jun},
  abstract = {A method is introduced to automatically find the coronary axis based
	on two or more user-defined points, even in the presence of a severe
	stenosis. The coronary axis is determined by finding a minimum cost
	path (MCP) in a feature image in which the tubular-like structures
	are enhanced. The results of the proposed method were compared with
	manually drawn central axes to estimate the accuracy. In 32 3D TFE-EPI
	acquisitions of patients and volunteers, 14 right coronary arteries
	(RCAs), 15 left anterior descending arteries (LADs), and eight left
	circumflex arteries (LCXs) were manually tracked twice by two operators
	to determine a reference axis and to assess the inter- and intra-user
	variability. On average, the maximum distance to the reference axis,
	based on only two user-defined points, is less than 1.5 mm; the average
	distance is around 0.65 mm, which is less than the average in-plane
	resolution. The results of the method are comparable to those of
	the manual operators.},
  doi = {10.1002/mrm.10164},
  institution = {Image Sciences Institute, University Medical Center, Utrecht, The
	Netherlands. onno@isi.uu.nl},
  keywords = {Coronary Stenosis, diagnosis; Coronary Vessels, anatomy /&/ histology;
	Humans; Image Processing, Computer-Assisted, methods; Magnetic Resonance
	Angiography, methods},
  language = {eng},
  medline-pst = {ppublish},
  owner = {euHeart},
  pmid = {12111963},
  timestamp = {2009.10.28},
  url = {http://dx.doi.org/10.1002/mrm.10164}
}

@ARTICLE{WIN04,
  author = {Onno Wink and Wiro J Niessen and Max A Viergever},
  title = {Multiscale vessel tracking.},
  journal = {IEEE Trans Med Imaging},
  year = {2004},
  volume = {23},
  pages = {130--133},
  number = {1},
  month = {Jan},
  abstract = {A method is presented that uses a vectorial multiscale feature image
	for wave front propagation between two or more user defined points
	to retrieve the central axis of tubular objects in digital images.
	Its implicit scale selection mechanism makes the method more robust
	to overlap and to the presence of adjacent structures than conventional
	techniques that propagate a wave front over a scalar image representing
	the maximum of a range of filters. The method is shown to retain
	its potential to cope with severe stenoses or imaging artifacts and
	objects with varying widths in simulated and actual two-dimensional
	angiographic images.},
  keywords = {Algorithms; Anatomy, Cross-Sectional, methods; Angiography, methods;
	Blood Vessels, anatomy /&/ histology; Humans; Image Enhancement,
	methods; Image Interpretation, Computer-Assisted, methods; Imaging,
	Three-Dimensional, methods; Pattern Recognition, Automated; Subtraction
	Technique},
  language = {eng},
  medline-pst = {ppublish},
  owner = {euHeart},
  pmid = {14719694},
  timestamp = {2009.10.14}
}

@ARTICLE{WIS08,
  author = {Thomas Wischgoll and Jenny Susana Choy and Erik L Ritman and Ghassan
	S Kassab},
  title = {Validation of image-based method for extraction of coronary morphometry.},
  journal = {Ann Biomed Eng},
  year = {2008},
  volume = {36},
  pages = {356--368},
  number = {3},
  month = {Mar},
  abstract = {An accurate analysis of the spatial distribution of blood flow in
	any organ must be based on detailed morphometry (diameters, lengths,
	vessel numbers, and branching pattern) of the organ vasculature.
	Despite the significance of detailed morphometric data, there is
	relative scarcity of data on 3D vascular anatomy. One of the major
	reasons is that the process of morphometric data collection is labor
	intensive. The objective of this study is to validate a novel segmentation
	algorithm for semi-automation of morphometric data extraction. The
	utility of the method is demonstrated in porcine coronary arteries
	imaged by computerized tomography (CT). The coronary arteries of
	five porcine hearts were injected with a contrast-enhancing polymer.
	The coronary arterial tree proximal to 1 mm was extracted from the
	3D CT images. By determining the centerlines of the extracted vessels,
	the vessel radii and lengths were identified for various vessel segments.
	The extraction algorithm described in this paper is based on a topological
	analysis of a vector field generated by normal vectors of the extracted
	vessel wall. With this approach, special focus is placed on achieving
	the highest accuracy of the measured values. To validate the algorithm,
	the results were compared to optical measurements of the main trunk
	of the coronary arteries with microscopy. The agreement was found
	to be excellent with a root mean square deviation between computed
	vessel diameters and optical measurements of 0.16 mm (<10\% of the
	mean value) and an average deviation of 0.08 mm. The utility and
	future applications of the proposed method to speed up morphometric
	measurements of vascular trees are discussed.},
  doi = {10.1007/s10439-008-9443-x},
  institution = {Department of Computer Science and Engineering, Wright State University,
	Dayton, OH, USA.},
  keywords = { Computer-Assisted, methods; Algorithms; Animals; Artificial Intelligence;
	Coronary Angiography, methods; Female; Imaging, Three-Dimensional,
	methods; Male; Pattern Recognition, Automated, methods; Radiographic
	Image Enhancement, methods; Reproducibility of Results; Sensitivity
	and Specificity; Software; Swine; Tomography, X-Ray Computed, methods},
  owner = {euHeart},
  pmid = {18228141},
  timestamp = {2008.09.26},
  url = {http://dx.doi.org/10.1007/s10439-008-9443-x}
}

@ARTICLE{WIS07,
  author = {Thomas Wischgoll and Joerg Meyer and Benjamin Kaimovitz and Yoram
	Lanir and Ghassan S Kassab},
  title = {A novel method for visualization of entire coronary arterial tree.},
  journal = {Ann Biomed Eng},
  year = {2007},
  volume = {35},
  pages = {694--710},
  number = {5},
  month = {May},
  abstract = {The complexity of the coronary circulation especially in the deep
	layers largely evades experimental investigations. Hence, virtual/computational
	models depicting structure-function relation of the entire coronary
	vasculature including the deep layer are imperative. In order to
	interpret such anatomically based models, fast and efficient visualization
	algorithms are essential. The complexity of such models, which include
	vessels from the large proximal coronary arteries and veins down
	to the capillary level (3 orders of magnitude difference in diameter),
	is a challenging visualization problem since the resulting geometrical
	representation consists of millions of vessel segments. In this study,
	a novel method for rendering the entire porcine coronary arterial
	tree down to the first segments of capillaries interactively is described
	which employs geometry reduction and occlusion culling techniques.
	Due to the tree-shaped nature of the vasculature, these techniques
	exploit the geometrical topology of the object to achieve a faster
	rendering speed while still handling the full complexity of the data.
	We found a significant increase in performance combined with a more
	accurate, gap-less representation of the vessel segments resulting
	in a more interactive visualization and analysis tool for the entire
	coronary arterial tree. The proposed techniques can also be applied
	to similar data structures, such as neuronal trees, airway structures,
	bile ducts, and other tree-like structures. The utility and future
	applications of the proposed algorithms are explored.},
  doi = {10.1007/s10439-007-9278-x},
  institution = {Department of Computer Science and Engineering, Wright State University,
	Dayton, OH, USA.},
  keywords = {Animals; Capillaries, anatomy /&/ histology; Computer Graphics; Computer
	Simulation; Coronary Vessels, anatomy /&/ histology; Data Display;
	Image Enhancement, methods; Image Interpretation, Computer-Assisted,
	methods; Imaging, Three-Dimensional, methods; Models, Anatomic; Models,
	Cardiovascular; Swine; User-Computer Interface},
  owner = {euHeart},
  pmid = {17334680},
  timestamp = {2008.09.26},
  url = {http://dx.doi.org/10.1007/s10439-007-9278-x}
}

@ARTICLE{WOR07,
  author = {Worz, S. . and Rohr, K. .},
  title = {Segmentation and Quantification of Human Vessels Using a 3-D Cylindrical
	Intensity Model},
  journal = {IEEE Transactions on Image Processing},
  year = {2007},
  volume = {16},
  pages = {1994--2004},
  number = {8},
  abstract = {We introduce a new approach for 3-D segmentation and quantification
	of vessels. The approach is based on a 3-D cylindrical parametric
	intensity model, which is directly fitted to the image intensities
	through an incremental process based on a Kalman filter. Segmentation
	results are the vessel centerline and shape, i.e., we estimate the
	local vessel radius, the 3-D position and 3-D orientation, the contrast,
	as well as the fitting error. We carried out an extensive validation
	using 3-D synthetic images and also compared the new approach with
	an approach based on a Gaussian model. In addition, the new model
	has been successfully applied to segment vessels from 3-D MRA and
	computed tomography angiography image data. In particular, we compared
	our approach with an approach based on the randomized Hough transform.
	Moreover, a validation of the segmentation results based on ground
	truth provided by a radiologist confirms the accuracy of the new
	approach. Our experiments show that the new model yields superior
	results in estimating the vessel radius compared to previous approaches
	based on a Gaussian model as well as the Hough transform.},
  doi = {10.1109/TIP.2007.901204},
  issn = {1057-7149},
  keywords = {Kalman filters, biomedical MRI, blood vessels, cardiovascular system,
	computerised tomography, diagnostic radiography, diseases, image
	segmentation, medical image processing, 3D cylindrical parametric
	intensity model, Gaussian model, Kalman filter, computed tomography
	angiography, heart, human vessels, image quantification, image segmentation,
	magnetic resonance angiography, randomized Hough transform, vascular
	diseases, 3-D MRA data, 3-D cylindrical model, 3-D parametric intensity
	model, 3-D vessel segmentation, Kalman filtering},
  owner = {euHeart},
  timestamp = {2008.09.25}
}

@ARTICLE{XIA07,
  author = {Xiao, Di and Ng, Wan Sing and Tsang, Charles B. and Abeyratne, Udantha
	Ranjith},
  title = {A region and gradient based active contour model and its application
	in boundary tracking on anal canal ultrasound images},
  journal = {Pattern Recognition},
  year = {2007},
  volume = { 40},
  pages = {3522 - 3539},
  number = { 12},
  note = {Active contour models;Anal ultrasound image;Boundary tracking;Statistical
	feature computation;},
  abstract = {A novel Gaussian mixture model (GMM)-based region and gradient active
	contour model is proposed for general object boundary tracking and
	the purpose of boundary tracking for anal muscle layers. Motion information
	is extracted from adjacent slice and is used to guide the first step
	of boundary tracking procedure. The idea is that GMM is introduced
	into the statistical feature computation for object region and background
	region, thereby providing an accurate model for regional pixel intensity
	description. Expectation-maximization algorithm and K-means algorithm
	are used for parameter solutions of GMM. Based on the available region
	information, gradient information and the self-constraints of the
	contour, a unifying active contour model is proposed. The proposed
	active contour models and tracking algorithm were tested on synthetic
	images and simulated ultrasound images to evaluate some generic features
	of the model for boundary tracking. Furthermore, it was applied to
	perform boundary tracking of anal muscle layers. The tracking results
	were evaluated by three experts. The results showed that the proposed
	method has a good performance for boundary tracking on anal wall
	ultrasound image. &copy; 2007 Pattern Recognition Society.},
  copyright = {Compilation and indexing terms, Copyright 2007 Elsevier Inc. All rights
	reserved},
  issn = {0031-3203},
  key = {Target tracking},
  keywords = {Computation theory;Feature extraction;Statistical methods;Ultrasonic
	imaging;},
  language = {English},
  owner = {euHeart},
  timestamp = {2009.02.24},
  url = {http://dx.doi.org/10.1016/j.patcog.2007.03.024}
}

@ARTICLE{XIE04,
  author = {Xie, Zhiyong and Farin, Gerald E.},
  title = {Image registration using hierarchical B-splines},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  year = {2004},
  volume = {10},
  pages = {85-94},
  number = {1},
  note = {TY - JOUR U1 - 04368342525 L2 - http://dx.doi.org/10.1109/TVCG.2004.1260760
	Compilation and indexing terms, Copyright 2005 Elsevier Engineering
	Information, Inc. U2 - Image registration Free form deformation Hierarchical
	B-splines Scattered data approximation Iterative closest point Optical
	flow},
  abstract = {Hierarchical B-splines have been widely used for shape modeling since
	their discovery by Forsey and Bartels. In this paper, we present
	an application of this concept, in the form of free-form deformation,
	to image registration by matching two images at increasing levels
	of detail. Results using MRI brain data are presented that demonstrate
	high degrees of matching while unnecessary distortions are avoided.
	We compare our results with the nonlinear ICP (Iterative Closest
	Point) algorithm (used for landmark-based registration) and optical
	flow (used for intensity-based registration).},
  keywords = {Hierarchical systems Pattern matching Iterative methods Surface phenomena
	Deformation Nonlinear systems Algorithms Image analysis},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{XU98.1,
  author = {Xu, Chenyang and Prince, Jerry L.},
  title = {Generalized gradient vector flow external forces for active contours},
  journal = {Elsevier - Signal Processing.},
  year = {1998},
  volume = {71},
  pages = {131-139},
  number = {2},
  note = {Elsevier - Signal Processing},
  abstract = {Active contours, or snakes, are used extensively in computer vision
	and image processing applications, particularly to locate object
	boundaries. A new type of external force for active contours, called
	gradient vector Flow (GVF) was introduced recently to address problems
	associated with initialization and poor convergence to boundary concavities.
	GVF is computed as a diffusion of the gradient vectors of a gray-level
	or binary edge map derived from the image. In this paper, we generalize
	the GVF formulation to include two spatially varying weighting functions.
	This improves active contour convergence to long, thin boundary indentations,
	while maintaining other desirable properties of GVF, such as an extended
	capture range. The original GVF is a special case of this new generalized
	GVF (GGVF) model. An error analysis for active contour results on
	simulated test images is also presented.},
  keywords = {Edge detection, Image segmentation, Shape representation and recovery,
	Deformable models Active contour models, Gradient vector Flow.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{XU98.2,
  author = {Xu, Chenyang and Prince, Jerry L.},
  title = {Snakes, Shapes, and Gradient Vector Flow},
  journal = {IEEE Transactions on Image Processing.},
  year = {1998},
  volume = {7},
  pages = {359-369},
  number = {3},
  abstract = {Snakes, or active contours, are used extensively in computer vision
	and image processing applications, particularly to locate object
	boundaries. Problems associated with initialization and poor convergence
	to boundary concavities, however, have limited their utility. This
	paper presents a new external force for active contours, largely
	solving both problems. This external force, which we call gradient
	vector flow (GVF), is computed as a diffusion of the gradient vectors
	of a gray-level or binary edge map derived from the image. It differs
	fundamentally from traditional snake external forces in that it cannot
	be written as the negative gradient of a potential function, and
	the corresponding snake is formulated directly from a force balance
	condition rather than a variational formulation. Using several two-dimensional
	(2-D) examples and one three-dimensional (3-D) example, we show that
	GVF has a large capture range and is able to move snakes into boundary
	concavities},
  keywords = {Active contour models, deformable surface models, edge detection,
	gradient vector flow, image segmentation, shape representation and
	recovery, snakes.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{XU97,
  author = {Xu, Chenyang and Prince, Jerry L.},
  title = {Gradient Vector Flow : A New External Force for Snakes},
  booktitle = {Computer Vision Pattern Recognition},
  year = {1997},
  pages = {66-71},
  address = {Puerto Rico},
  publisher = {IEEE},
  abstract = {Snakes, or active contours, are used extensively in computer vision
	and image processing applications, particularly to locate object
	boundaries. Problems associated with initialization and poor convergence
	to concave boundaries, however, have limited their utility. This
	paper develops a new external force for active contours, largely
	solving both problems. This external force, which we call gradient
	vector flow (GVF), is computed as a diffusion of the gradient vectors
	of a gray-level or binary edge map derived from the image. The resultant
	field has a large capture range and forces active contours into concave
	regions. Examples on simulated images and one real image are presented.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{XU00,
  author = {Xu, C. and Yezzi A., Jr. and Prince, J.L.},
  title = {On the relationship between parametric and geometric active contours},
  booktitle = {34th Asilomar Conference, Oct 29-Nov 1 2000},
  year = {2000},
  volume = {1},
  series = {Conference Record of the Asilomar Conference on Signals, Systems
	and Computers},
  pages = {483-489},
  address = {Pacific Grove, CA},
  publisher = {Institute of Electrical and Electronics Engineers Computer Society},
  note = {TY - CONF U1 - 01236529887 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc. U2 - Geometric active
	contours Parametric active contours},
  abstract = {Geometric active contours have many advantages over parametric active
	contours, such as computational simplicity and the ability to change
	curve topology during deformation. While many of the capabilities
	of the older parametric active contours have been reproduced in geometric
	active contours, the relationship between the two has not always
	been clear. In this paper we develop a precise relationship between
	the two which includes spatially-varying coefficients, both tension
	and rigidity, and non-conservative external forces. The result is
	a very general geometric active contour formulation for which the
	intuitive design principles of parametric active contours can be
	applied. We demonstrate several novel applications in a series of
	simulations.},
  keywords = {Computational complexity Algorithms Mathematical models Lagrange multipliers
	Computer simulation Image processing},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{XU08,
  author = {Dong Xu and Hua Li},
  title = {Geometric moment invariants},
  journal = {Pattern Recognition},
  year = {2008},
  volume = {41},
  pages = {240 - 249},
  number = {1},
  abstract = {This paper derives moment invariants in an intuitive way by multiple
	integrals of invariant geometric primitives like distance, area and
	volume. Many existing 2-D and 3-D moment invariants are re-expressed
	to prove the correctness of this method. Furthermore, this construction
	method can be easily extended to higher dimensional space and get
	higher-order moment invariants. Explicit expressions of 3-D moment
	invariants are generated automatically with the help of symbolic
	computation software and can be regarded as a kind of shape descriptor
	for the representation of solid objects.},
  doi = {DOI: 10.1016/j.patcog.2007.05.001},
  issn = {0031-3203},
  keywords = {Geometric primitive},
  url = {http://www.sciencedirect.com/science/article/B6V14-4NR187M-5/2/1ac7cbccb84b4d09623e60fc0336d21b}
}

@INPROCEEDINGS{YAN06,
  author = {Yang, G. and Bousse, A. and Toumoulin, C. and Shu, H. },
  title = {A Multiscale Tracking Algorithm for the Coronary Extraction in MSCT
	Angiography},
  booktitle = {Proc. 28th Annual International Conference of the IEEE Engineering
	in Medicine and Biology Society EMBS '06},
  year = {2006},
  pages = {3066--3069},
  abstract = {This paper deals with the extraction of the coronary network on dynamic
	volume sequences, acquired in Multi-slice Spiral Computed Tomography
	(MSCT). The proposed approach makes use of a tracking algorithm of
	the vascular structure, combining a 3D geometric moment operator
	with a multiscale Hessian filter to estimate the vessel central axis
	location, its local diameter and orientation. The method performs
	at the same time, a bifurcation detection to reconstitute the structure
	of the coronary network. The mean computation time to extract a coronary
	network is about 3 minutes using a P4-2.4G PC. Preliminary encouraging
	results are presented on one volume of a sequence.},
  doi = {10.1109/IEMBS.2006.260712},
  issn = {1557-170X},
  keywords = {3-D geometric moment, 3-D tracking, Coronary extraction, MSCT Angiography,
	Multi-scale Hessian filter},
  owner = {euHeart},
  timestamp = {2008.09.10}
}

@INPROCEEDINGS{YAN05,
  author = {Yang, G. and Toumoulin, C. and Coatrieux, J. -L. and Shu, H. and
	Luo, L. and Boulmier, D.},
  title = {A 3D Static Heart Model From a MSCT Data Set},
  booktitle = {Proc. 27th Annual International Conference of the Engineering in
	Medicine and Biology Society IEEE-EMBS 2005},
  year = {2005},
  pages = {5499--5502},
  abstract = {Dynamic computed tomography (CT) imaging aims to access the kinetics
	of the moving organs. In cardiac imaging, the interest lies in the
	possibility of obtaining anatomic and functional information on the
	heart and the coronaries during the same examination. However, segmentation,
	reconstruction and registration algorithms need to be developed for
	diagnostic purposes. We propose thus to built a 3D heart model from
	multi-slice spiral computed tomography (MSCT) dynamic sequences to
	facilitate the evaluation of these algorithms. The model building
	relies on semi-automatic segmentation techniques based on deformable
	models such as fast marching and active contours. Shape-based interpolation
	and marching cube algorithms are then used for the 3D surface reconstruction},
  doi = {10.1109/IEMBS.2005.1615728},
  keywords = {cardiology, computerised tomography, image reconstruction, image registration,
	image segmentation, interpolation, medical image processing, 3D static
	heart model, 3D surface reconstruction, MSCT data set, active contours,
	cardiac imaging, coronaries, diagnostics, dynamic computed tomography,
	fast marching, heart, image reconstruction, image registration, image
	segmentation, marching cube algorithms, moving organs, multislice
	spiral computed tomography dynamic sequences, semi-automatic segmentation
	techniques, shape-based interpolation, 3D static heart model, Active
	contours, Deformable models, Fast Marching, Marching cube, Shape-based
	interpolation},
  owner = {euHeart},
  timestamp = {2008.09.10}
}

@INPROCEEDINGS{YAN97,
  author = {Luren Yang and Fritz Albregtsen},
  title = {Fast computation of 3-D geometric moments using a discrete divergence
	theorem and a generalization to higher dimensions},
  booktitle = {Graphical Models and Image Processing},
  year = {1997},
  pages = {97--108},
  abstract = {A discrete Gauss ' theorem is presented. Using a fast surface tracking
	algorithm and the discrete Gauss ' theorem, we design a new method
	to compute the Cartesian geometric moments of 3-D objects. Compared
	to previous methods to compute such moments, the new method reduces
	the computational complexity significantly.}
}

@INPROCEEDINGS{YAN07,
  author = {Yang, Yan and Tannenbaum, A. and Giddens, D. and Stillman, A. },
  title = {AUTOMATIC SEGMENTATION OF CORONARY ARTERIES USING BAYESIAN DRIVEN
	IMPLICIT SURFACES},
  booktitle = {Proc. 4th IEEE International Symposium on Biomedical Imaging: From
	Nano to Macro ISBI 2007},
  year = {2007},
  pages = {189--192},
  abstract = {In this paper, we propose a hybrid approach for the automatic three-dimensional
	segmentation of coronary arteries using multi-scale vessel filtering
	and a Bayesian probabilistic approach in a level set image segmentation
	framework. The initial surface of the coronaries is obtained from
	the multiscale vessel filter response, and the surface then evolves
	to capture the exact boundary of the coronaries according to an improved
	evolution model of implicit surfaces. In our model, the image force
	and the propagation terms are re-defined using posterior probabilities
	obtained via Bayes' rule in order for the surface to approach to
	the boundaries faster and stop at the boundaries more accurately.
	The proposed method is tested on seven CT angiography (CTA) data-sets
	of left and right coronary arteries, and the quantitative comparison
	of our result against manually delineated contours on two of the
	data-sets yields a mean error of 0.37 mm},
  doi = {10.1109/ISBI.2007.356820},
  keywords = {Bayes methods, angiocardiography, blood vessels, computerised tomography,
	image segmentation, medical image processing, probability, spatial
	filters, Bayes rule, Bayesian driven implicit surfaces, Bayesian
	probabilistic approach, CT angiography, automatic segmentation, computerized
	tomography, coronaries, coronary arteries, image force, image segmentation,
	multiscale vessel filtering, probabilities, three-dimensional segmentation},
  owner = {euHeart},
  timestamp = {2009.05.20}
}

@ARTICLE{YU08,
  author = {G. Yu and P. Li and Y. L. Miao and Z. Z. Bian},
  title = {Multiscale active contour model for vessel segmentation.},
  journal = {J Med Eng Technol},
  year = {2008},
  volume = {32},
  pages = {1--9},
  number = {1},
  abstract = {This paper presents a novel multiscale active contour model for vessel
	segmentation. The model is based on accurate analysis of the vessel
	structure in the image. According to different scale response of
	the eigenvalues of local second order derivative (Hessian matrix),
	a new vessel region information function, which shows a valid estimation
	of the vesselness measure, is defined. We introduce the posteriori
	probability estimation into the active contours framework and design
	a new objective function. The defined objective function is minimized
	using the variational method, and a new region-based external force
	is obtained, which is more accurate to the vessel structure and not
	sensitive to the initial condition. This active contour model combines
	the obtained region-based and conventional boundary-based force,
	which aims at finding more accurate vessel edges even when the vessel
	branches are low contrast or blurry. Furthermore, the proposed model
	is implemented by an implicit method of level set framework, the
	solution of which is steady and suitable for various topology changes.
	Moreover, two new speed functions for vessel segmentation in the
	level set method are presented, one for fast marching and the other
	for a narrow-band algorithm. The vessel segmentation experiments
	compared with previous geometric active contour models are shown
	on several medical images. The experimental results demonstrate the
	performance of our approach.},
  doi = {10.1080/03091900600700798},
  institution = {School of Life Science and Technology, Xi'an Jiaotong University,
	Xi'an, PR China. yugang@mailst.xjtu.edu.cn},
  keywords = {Algorithms; Animals; Humans; Image Enhancement, methods; Image Interpretation,
	Computer-Assisted, methods; Models, Theoretical; Pattern Recognition,
	Automated, methods; Retinal Vessels, anatomy /&/ histology},
  language = {eng},
  medline-pst = {ppublish},
  owner = {euHeart},
  pii = {789340589},
  pmid = {18183515},
  timestamp = {2009.10.14},
  url = {http://dx.doi.org/10.1080/03091900600700798}
}

@INPROCEEDINGS{YU05,
  author = {Yu, Tianli and Luo, Jiebo and Ahuja, Narendra},
  title = {Shape Regularized Active Contour using iterative global search and
	local optimization},
  booktitle = {2005 IEEE Computer Society Conference on Computer Vision and Pattern
	Recognition, CVPR 2005, Jun 20-25 2005},
  year = {2005},
  volume = {2},
  series = {Proceedings of the IEEE Computer Society Conference on Computer Vision
	and Pattern Recognition},
  pages = {655-662},
  address = {San Diego, CA, United States},
  publisher = {Institute of Electrical and Electronics Engineers Computer Society,
	Piscataway, NJ 08855-1331, United States},
  note = {TY - CONF U1 - 05389368537 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc. U2 - Combinatorial search
	Nonlinear shape models Shape energy Image energy},
  abstract = {Recently, nonlinear shape models have been shown to improve the robustness
	and flexibility of segmentation. In this paper, we propose Shape
	Regularized Active Contour (ShRAC) that incorporates existing nonlinear
	shape models into the classical active contour approach. ShRAC uses
	a discrete representation of the contour to allow efficient combinatorial
	search. The search for optimal contour is performed by a coarse-to-fine
	algorithm that iterates between combinatorial search and gradient-based
	local optimization. First, Multi-Solution Dynamic Programming (MSDP)
	is used to generate initial candidates by minimizing only the image
	energy. In the second step, a combination of image energy and shape
	energy determined by a given prior shape model is minimized for the
	initial candidates using a local optimization method and the best
	one is selected. To have diverse initial candidates, we employ a
	Clustered Solution Pruning procedure in the MSDP search space. Finally,
	Local Shape Regularization is used to feed shape constraints back
	into the new MSDP search space of the next iteration. Our search
	strategy combines the advantages of global combinatorial search and
	local optimization, and has shown excellent robustness to local minima
	caused by distracting suboptimal segmentations. Experimental results
	on segmentation of different anatomical structures using ShRAC are
	provided. &copy; 2005 IEEE.},
  keywords = {Optimization Nonlinear control systems Iterative methods Robustness
	(control systems) Dynamic programming Combinatorial mathematics Image
	segmentation},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{YU02,
  author = {Yu, Zeyun and Bajaj, Chandrajit},
  title = {Image segmentation using gradient vector diffusion and region merging},
  booktitle = {International Conference on Pattern Recognition},
  year = {2002},
  address = {Quebec},
  publisher = {IEEE},
  abstract = {Active Contour (or Snake) Model is recognized as one of the efficient
	tools for 2D/3D image segmentation. However, traditional snake models
	prove to be limited in several aspects. The present paper describes
	a set of diffusion equations applied to image gradient vectors, yielding
	a vector field over the image domain. The obtained vector field provides
	the Snake Model an external force as well as an automatic way to
	generate the initial contours. Finally a region merging technique
	is employed to further improve the segmentation results.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{ZHA06,
  author = {Zhang, Eugene and Mischaikow, Konstantin and Turk, Greg},
  title = {Vector field design on surfaces},
  journal = {ACM Transactions on Graphics},
  year = {2006},
  volume = { 25},
  pages = {1294 - 1326},
  number = { 4},
  note = {Vector field design;Nonphotorealistic rendering;Texture synthesis;},
  abstract = {Vector field design on surfaces is necessary for many graphics applications:
	example-based texture synthesis, nonphotorealistic rendering, and
	fluid simulation. For these applications, singularities contained
	in the input vector field often cause visual artifacts. In this article,
	we present a vector field design system that allows the user to create
	a wide variety of vector fields with control over vector field topology,
	such as the number and location of singularities. Our system combines
	basis vector fields to make an initial vector field that meets user
	specifications.The initial vector field often contains unwanted singularities.
	Such singularities cannot always be eliminated due to the Poincare-Hopf
	index theorem. To reduce the visual artifacts caused by these singularities,
	our system allows the user to move a singularity to a more favorable
	location or to cancel a pair of singularities. These operations offer
	topological guarantees for the vector field in that they only affect
	user-specified singularities. We develop efficient implementations
	of these operations based on Conley index theory. Our system also
	provides other editing operations so that the user may change the
	topological and geometric characteristics of the vector field.To
	create continuous vector fields on curved surfaces represented as
	meshes, we make use of the ideas of geodesic polar maps and parallel
	transport to interpolate vector values defined at the vertices of
	the mesh. We also use geodesic polar maps and parallel transport
	to create basis vector fields on surfaces that meet the user specifications.
	These techniques enable our vector field design system to work for
	both planar domains and curved surfaces.We demonstrate our vector
	field design system for several applications: example-based texture
	synthesis, painterly rendering of images, and pencil sketch illustrations
	of smooth surfaces. &copy; 2006 ACM.},
  copyright = {Compilation and indexing terms, Copyright 2007 Elsevier Inc. All rights
	reserved},
  issn = {0730-0301},
  key = {Computer graphics},
  keywords = {Computational geometry;Image quality;Mathematical models;Topology;Surfaces;Computer
	simulation;},
  language = {English},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{ZHA03,
  author = {Hao Zhang and Eugene Fiume},
  title = {Butterworth Filtering and Implicit Fairing of Irregular Meshes},
  booktitle = {Proc. of Pacific Conference on Computer Graphics and Applications},
  year = {2003},
  pages = {502--506},
  publisher = {IEEE Computer Society},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{ZHA08,
  author = {Hao Zhang and Oliver van Kaick and Ramsay Dyer},
  title = {Spectral Mesh Processing},
  journal = {Computer Graphics Forum},
  year = {2008},
  pages = {??--??},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{ZHA07,
  author = {Hao Zhang and Oliver van Kaick and Ramsay Dyer},
  title = {Spectral Methods for Mesh Processing and Analysis},
  booktitle = {Proc. of Eurographics State-of-the-art Report},
  year = {2007},
  pages = {1--22},
  abstract = {Spectral methods for mesh processing and analysis rely on the eigenvalues,
	eigenvectors, or eigenspace projections derived from appropriately
	defined mesh operators to carry out desired tasks. Early works in
	this area can be traced back to the seminal paper by Taubin in 1995,
	where spectral analysis of mesh geometry based on a combinatorial
	Laplacian aids our understanding of the low-pass filtering approach
	to mesh smoothing. Over the past ten years or so, the list of applications
	in the area of geometry processing which utilize the eigenstructures
	of a variety of mesh operators in different manners have been growing
	steadily. Many works presented so far draw parallels from developments
	in fields such as graph theory, computer vision, machine learning,
	graph drawing, numerical linear algebra, and high-performance computing.
	This state-of-the-art report aims to provide a comprehensive survey
	on the spectral approach, focusing on its power and versatility in
	solving geometry processing problems and attempting to bridge the
	gap between relevant research in computer graphics and other fields.
	Necessary theoretical background will be provided and existing works
	will be classified according to different criteria — the operators
	or eigenstructures employed, application domains, or the dimensionality
	of the spectral embeddings used — and described in adequate length.
	Finally, despite much empirical success, there still remain many
	open questions pertaining to the spectral approach, which we will
	discuss in the report as well.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{ZHO04,
  author = {Zhou, Ze-Ming and Heng, Pheng Ann and Xia, De-Shen},
  title = {Low contrast noisy cardiac MRI image segmentation based on Snake
	model},
  journal = {Xitong Fangzhen Xuebao / Journal of System Simulation},
  year = {2004},
  volume = {16},
  pages = {2493-2496},
  number = {11},
  note = {TY - JOUR U1 - 04538754566 Compilation and indexing terms, Copyright
	2005 Elsevier Engineering Information, Inc.},
  abstract = {A segmentation algorithm of low contrast noisy cardiac MRI based on
	Snake model is proposed. The Snake model can find the boundary of
	Region of Interest (ROI) by deforming the spline curve, but the segmentation
	result relies on the initial location of the curve and it is easy
	for the curve to converge to the local gradient maximum region or
	leak from the weak edges. Moreover, the model cannot segment the
	concave region accurately. The improved Snake model can search for
	the edge of ROI in the wider region by adding local area energy term.
	A fuzzy energy term is added to the model for dealing with weak edges,
	local gradient maximum region and artifacts in the MRI. The segmentation
	experiments demonstrate the effectiveness of the algorithm listed
	in the paper for the low contrast noisy cardiac MRI.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@ARTICLE{ZHU96,
  author = {Zhu, Song Chun and Yuille, Alan},
  title = {Region Competition: Unifying Snakes, Region Growing, and Bayes/MDL
	for Multi-band Image Segmentation},
  journal = {Transactions on Pattern Analysis and Machine Intelligence.},
  year = {1996},
  volume = {18},
  pages = {884-900},
  number = {9},
  abstract = {We present a novel statistical and variational approach to image segmentation
	based on a new algorithm named region competition. This algorithm
	is derived by minimizing a generalized Bayes/MDL criterion using
	the variational principle. The algorithm is guaranteed to converge
	to a local minimum and combines aspects of snakes/balloons and region
	growing. Indeed the classic snakes/balloons and region growing algorithms
	can be directly derived from our approach. We provide theoretical
	analysis of region competition including accuracy of boundary location,
	criteria for initial conditions, and the relationship to edge detection
	using filters. It is straightforward to generalize the algorithm
	to multiband segmentation and we demonstrate it on gray level images,
	color images and texture images. The novel color model allows us
	to eliminate intensity gradients and shadows, thereby obtaining segmentation
	based on the albedos of objects. It also helps detect highlight regions.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@BOOK{GRE96,
  title = {Interpolating and Approximating Scattered 3D-Data with Hierarchical
	Tensor Product B-Splines},
  publisher = {Vanderbilt University Press},
  year = {1996},
  editor = {Greiner, Gunther and Hormann, Kai},
  series = {Surface Fitting and Multiresolution Methods},
  edition = {A. Le Mehaute and C. Rabut and L. L. Schumaker},
  abstract = {In this note we describe surface reconstruction algorithms based on
	optimization techniques. The input are scattered 3D-data with specified
	topology. The surfaces constructed are tensor product B-splines.
	To achieve local detail and/or local fairness we make use of hierarchical
	tensor product B-splines. Interpolation as well as approximation
	problems are discussed. The problem of parameterizing a discrete
	point set which is crucial in this approach will be discussed in
	more detail.},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@INPROCEEDINGS{DES06.2,
  title = {Discrete differential geometry: an applied introduction},
  year = {2006},
  editor = {Eitan Grinspun},
  owner = {euHeart},
  proceeding = {ACM SIGGRAPH 2006 Courses},
  timestamp = {2009.02.24}
}

@BOOK{JOL00,
  title = {Les Syst\`emes de Vision},
  publisher = {Hermes Paris},
  year = {2000},
  editor = {Jolion, Jean-Michel},
  series = {Trait\'e IC2, Traitement},
  owner = {euHeart},
  timestamp = {2009.02.24}
}

@comment{jabref-meta: selector_publisher:}

@comment{jabref-meta: selector_author:}

@comment{jabref-meta: selector_journal:}

@comment{jabref-meta: selector_keywords:}

